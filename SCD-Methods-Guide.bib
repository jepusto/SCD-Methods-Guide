
@article{2014Council,
  title = {Council for {{Exceptional Children}}: {{Standards}} for {{Evidence-Based Practices}} in {{Special Education}}},
  shorttitle = {Council for {{Exceptional Children}}},
  year = {2014},
  month = jul,
  journal = {TEACHING Exceptional Children},
  volume = {46},
  number = {6},
  pages = {206--212},
  issn = {0040-0599, 2163-5684},
  doi = {10.1177/0040059914531389},
  langid = {english}
}

@article{appelbaum2018Journal,
  title = {Journal Article Reporting Standards for Quantitative Research in Psychology: {{The APA Publications}} and {{Communications Board}} Task Force Report.},
  shorttitle = {Journal Article Reporting Standards for Quantitative Research in Psychology},
  author = {Appelbaum, Mark and Cooper, Harris and Kline, Rex B. and {Mayo-Wilson}, Evan and Nezu, Arthur M. and Rao, Stephen M.},
  year = {2018},
  month = jan,
  journal = {American Psychologist},
  volume = {73},
  number = {1},
  pages = {3--25},
  issn = {1935-990X, 0003-066X},
  doi = {10.1037/amp0000191},
  langid = {english}
}

@article{barton2017TechnologyAided,
  title = {Technology-{{Aided Instruction}} and {{Intervention}} for {{Students With ASD}}: {{A Meta-Analysis Using Novel Methods}} of {{Estimating Effect Sizes}} for {{Single-Case Research}}},
  shorttitle = {Technology-{{Aided Instruction}} and {{Intervention}} for {{Students With ASD}}},
  author = {Barton, Erin E. and Pustejovsky, James E. and Maggin, Daniel M. and Reichow, Brian},
  year = {2017},
  month = nov,
  journal = {Remedial and Special Education},
  volume = {38},
  number = {6},
  pages = {371--386},
  issn = {0741-9325, 1538-4756},
  doi = {10.1177/0741932517729508},
  abstract = {The adoption of methods and strategies validated through rigorous, experimentally oriented research is a core professional value of special education. We conducted a systematic review and meta-analysis examining the experimental literature on Technology-Aided Instruction and Intervention (TAII) using research identified as part of the National Autism Professional Development Project. We applied novel between-case effect size methods to the TAII single-case research base. In addition, we used meta-analytic methodologies to examine the methodological quality of the research, calculate average effect sizes to quantify the level of evidence for TAII, and compare effect sizes across single-case and group-based experimental research. Results identified one category of TAII\textemdash computer-assisted instruction\textemdash as an evidence-based practice across both single-case and group studies. The remaining two categories of TAII\textemdash augmentative and alternative communication and virtual reality\textemdash were not identified as evidence-based using What Works Clearinghouse summary ratings.},
  langid = {english}
}

@article{Beretvas2008review,
  title = {A Review of Meta-Analyses of Single-Subject Experimental Designs: {{Methodological}} Issues and Practice},
  author = {Beretvas, S Natasha and Chung, Hyewon},
  year = {2008},
  journal = {Evidence-Based Communication Assessment and Intervention},
  volume = {2},
  number = {3},
  pages = {129--141},
  publisher = {{Psychology Press}},
  issn = {1748-9539},
  doi = {10.1080/17489530802446302},
  keywords = {analysis,designs,effect sizes,meta,methodology,multiple regression,single,subject experimental,subject experimental designs}
}

@incollection{Borenstein2009effect,
  title = {Effect Sizes for Continuous Data},
  booktitle = {The {{Handbook}} of {{Research Synthesis}} and {{Meta-Analysis}}},
  author = {Borenstein, Michael},
  editor = {Cooper, Harris M and Hedges, Larry V and Valentine, John C},
  year = {2009},
  pages = {221--236},
  publisher = {{Russell Sage Foundation}},
  address = {{New York, NY}}
}

@book{borenstein2009introduction,
  title = {Introduction to {{Meta-Analysis}}},
  author = {Borenstein, Michael and Hedges, Larry V and Higgins, Julian P T and Rothstein, Hannah R},
  year = {2009},
  month = mar,
  publisher = {{John Wiley \& Sons, Ltd}},
  address = {{Chichester, UK}},
  doi = {10.1002/9780470743386},
  isbn = {978-0-470-74338-6}
}

@article{bowman-perrott2016Promoting,
  title = {Promoting {{Positive Behavior Using}} the {{Good Behavior Game}}: {{A Meta-Analysis}} of {{Single-Case Research}}},
  shorttitle = {Promoting {{Positive Behavior Using}} the {{Good Behavior Game}}},
  author = {{Bowman-Perrott}, Lisa and Burke, Mack D. and Zaini, Samar and Zhang, Nan and Vannest, Kimberly},
  year = {2016},
  month = jul,
  journal = {Journal of Positive Behavior Interventions},
  volume = {18},
  number = {3},
  pages = {180--190},
  issn = {1098-3007, 1538-4772},
  doi = {10.1177/1098300715592355},
  abstract = {The Good Behavior Game (GBG) is a classroom management strategy that uses an interdependent group-oriented contingency to promote prosocial behavior and decrease problem behavior. This meta-analysis synthesized single-case research (SCR) on the GBG across 21 studies, representing 1,580 students in pre-kindergarten through Grade 12. The TauU effect size across 137 phase contrasts was .82 with a confidence interval 95\% CI = [0.78, 0.87], indicating a substantial reduction in problem behavior and an increase in prosocial behavior for participating students. Five potential moderators were examined: emotional and behavioral disorder (EBD) risk status, reinforcement frequency, target behaviors, GBG format, and grade level. Findings suggest that the GBG is most effective in reducing disruptive and off-task behaviors, and that students with or at risk for EBD benefit most from the intervention. Implications for research and practice are discussed.},
  langid = {english}
}

@article{case1992Improving,
  title = {Improving the {{Mathematical Problem-Solving Skills}} of {{Students}} with {{Learning Disabilities}}: {{Self-Regulated Strategy Development}}},
  shorttitle = {Improving the {{Mathematical Problem-Solving Skills}} of {{Students}} with {{Learning Disabilities}}},
  author = {Case, Lisa Pericola and Harris, Karen R. and Graham, Steve},
  year = {1992},
  month = apr,
  journal = {The Journal of Special Education},
  volume = {26},
  number = {1},
  pages = {1--19},
  issn = {0022-4669, 1538-4764},
  doi = {10.1177/002246699202600101},
  abstract = {Four students with learning disabilities, whose primary difficulty in solving simple word problems involved performing the wrong operation, were taught a strategy for comprehending the problem and devising an appropriate solution. Students learned to apply the strategy first to addition word problems, then to subtraction word problems. Upon completion of instruction, students' overall performance on mixed sets of addition and subtraction word problems improved, and they were much less likely to perform the wrong operation. Although the effects of instruction generalized to a different setting, maintenance of strategy effects was mixed, perhaps due to administration of the maintenance probes during summer vacation.},
  langid = {english}
}

@article{Center1985methodology,
  title = {A Methodology for the Quantitative Synthesis of Intra-Subject Design Research},
  author = {Center, B A and Skiba, R J and Casey, A},
  year = {1985},
  journal = {The Journal of Special Education},
  volume = {19},
  number = {4},
  pages = {387},
  publisher = {{SAGE Publications}}
}

@book{cooper2010Research,
  title = {Research {{Synthesis}} and {{Meta-Analysis}}},
  author = {Cooper, Harris M},
  year = {2010},
  edition = {Fourth},
  publisher = {{SAGE Publications}},
  address = {{Thousand Oaks, CA}}
}

@article{datchuk2016Writing,
  title = {Writing {{Simple Sentences}} and {{Descriptive Paragraphs}}: {{Effects}} of an {{Intervention}} on {{Adolescents}} with {{Writing Difficulties}}},
  shorttitle = {Writing {{Simple Sentences}} and {{Descriptive Paragraphs}}},
  author = {Datchuk, Shawn M.},
  year = {2016},
  month = jun,
  journal = {Journal of Behavioral Education},
  volume = {25},
  number = {2},
  pages = {166--188},
  issn = {1053-0819, 1573-3513},
  doi = {10.1007/s10864-015-9236-x},
  langid = {english}
}

@article{datchuk2020Level,
  title = {Level and {{Trend}} of {{Writing Sequences}}: {{A Review}} and {{Meta-Analysis}} of {{Writing Interventions}} for {{Students With Disabilities}}},
  shorttitle = {Level and {{Trend}} of {{Writing Sequences}}},
  author = {Datchuk, Shawn M. and Wagner, Kyle and Hier, Bridget O.},
  year = {2020},
  month = jan,
  journal = {Exceptional Children},
  volume = {86},
  number = {2},
  pages = {174--192},
  issn = {0014-4029, 2163-5560},
  doi = {10.1177/0014402919873311},
  abstract = {We examined effects of intervention on the level and trend of text-writing sequences of students with disabilities and writing difficulties, in addition to potential moderating effects related to student demographics (i.e., disability status, age, gender, and race) and writing task (i.e., sentence, essay, and narrative). We reviewed 18 single-case experimental design studies with a total of 96 students and subsequently meta-analyzed 15 of these studies with a total of 79 students using mixed-effects linear regression and an information-theoretic ranking of competing models. Results indicate that writing interventions, including direct instruction and self-regulated strategy development, produced gradual improvement in the trend of correct writing sequences per minute. Older students produced higher levels of writing sequences, but younger students showed steeper trends during intervention. Furthermore, students had higher levels of writing fluency on sentence-writing tasks than on discourse-writing tasks (narratives and essays).},
  langid = {english}
}

@article{delemere2018ParentImplemented,
  title = {Parent-{{Implemented Bedtime Fading}} and {{Positive Routines}} for {{Children}} with {{Autism Spectrum Disorders}}},
  author = {Delemere, Emma and Dounavi, Katerina},
  year = {2018},
  month = apr,
  journal = {Journal of Autism and Developmental Disorders},
  volume = {48},
  number = {4},
  pages = {1002--1019},
  issn = {1573-3432},
  doi = {10.1007/s10803-017-3398-4},
  abstract = {Sleep disorders affect a large portion of those with autism spectrum disorder. Behavioural interventions have been found to increase appropriate sleep behaviours. This study sought to examine the efficacy of two stimulus control interventions (bedtime fading and positive routines) on total sleep duration, sleep onset latency and frequency and duration of night wakings for children with autism using two multiple baseline designs. Secondary dependent variables, namely, educational opportunities, challenging behaviours, parent acceptance and social validity were also analysed. Results suggest some efficacy for both interventions. Increased total sleep duration and decreased sleep onset latency were achieved with bedtime fading. Positive routines showed mixed results with decreased sleep onset latency and increased total sleep duration for two of three participants.},
  langid = {english}
}

@article{Gingerich1984meta,
  title = {Meta-Analysis of Applied Time-Series Data},
  author = {Gingerich, W. J.},
  year = {1984},
  journal = {The Journal of Applied Behavioral Science},
  volume = {20},
  number = {1},
  pages = {71--79},
  publisher = {{Sage Publications}},
  issn = {0021-8863},
  doi = {10.1177/002188638402000113}
}

@article{gunning2003Psychological,
  title = {Psychological Treatment of Reported Sleep Disorder in Adults with Intellectual Disability Using a Multiple Baseline Design},
  author = {Gunning, M. J. and Espie, C. A.},
  year = {2003},
  journal = {Journal of Intellectual Disability Research},
  volume = {47},
  number = {3},
  pages = {191--202},
  issn = {1365-2788},
  doi = {10.1046/j.1365-2788.2003.00461.x},
  abstract = {Background The literature on sleep disturbance in adults with intellectual disability (ID) is sparse. Although prevalence rates for sleep disorders appear similar to those of non-disabled populations, previous treatment studies have largely been comprised of uncontrolled cases. Therefore, the present study adopted a single-case experimental methodology to evaluate behavioural sleep intervention. Methods A screening questionnaire was posted to 384 adults with ID and the sleep pattern of respondents with possible sleep disorders was further assessed using a structured diagnostic schedule. From the sleep-disordered subgroup, 12 participants were selected for a 4-week behavioural sleep intervention that was evaluated using randomly allocated, multiple-baseline, across-subjects designs and within-subject interrupted time series analyses (ITSAs). Results A total of 155 adults with ID (83 females and 72 males; mean age = 32 years, SD = 16.5 years), or their carers, completed the questionnaire (return rate = 40\%). The application of sleep diagnostic criteria revealed that 17\% had clinically significant difficulty getting to sleep and 11\% had difficulty remaining asleep. Nine out of the 12 participants recruited for the intervention completed all the experimental phases, thus providing three sets of three multiple-baseline designs. Visual inspection of within- and between-subject effects suggested beneficial treatment-specific effects across a range of target variables. The ITSA confirmed significant effects (P {$<$} 0.05) or trends (P {$<$} 0.10) for six out of the nine participants. Conclusions Behavioural sleep management may improve sleep pattern or sleep-related functioning in the majority of adults with ID who have significant sleep problems. The single-case methodology is helpful in addressing the heterogeneity of individual presentation, although clinical trial methodology is required to confirm these findings on a larger scale.},
  langid = {english},
  keywords = {behavioural sleep management,insomnia,learning disability,mental retardation},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1046/j.1365-2788.2003.00461.x}
}

@article{hebert2018Writing,
  title = {Writing Informational Text Using Provided Information and Text Structures: An Intervention for Upper Elementary Struggling Writers},
  shorttitle = {Writing Informational Text Using Provided Information and Text Structures},
  author = {Hebert, Michael and Bohaty, Janet J. and Nelson, J. Ron and Roehling, Julia V.},
  year = {2018},
  month = nov,
  journal = {Reading and Writing},
  volume = {31},
  number = {9},
  pages = {2165--2190},
  issn = {0922-4777, 1573-0905},
  doi = {10.1007/s11145-018-9841-x},
  langid = {english}
}

@article{Hedges1981distribution,
  title = {Distribution Theory for {{Glass}}'s Estimator of Effect Size and Related Estimators},
  author = {Hedges, Larry V},
  year = {1981},
  journal = {Journal of Educational Statistics},
  volume = {6},
  number = {2},
  pages = {107--128},
  publisher = {{Sage Publications}},
  issn = {1076-9986},
  keywords = {mean difdetence,measutement etro4,meta-analyzsi,research aynthe6sis,standaadized,weighting of}
}

@book{hedges1985statistical,
  title = {Statistical {{Methods}} for {{Meta-Analysis}}},
  author = {Hedges, Larry V and Olkin, Ingram},
  year = {1985},
  publisher = {{Academic Press}},
  address = {{Orlando, FL}}
}

@article{Hedges2007effect,
  title = {Effect Sizes in Cluster-Randomized Designs},
  author = {Hedges, Larry V},
  year = {2007},
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {32},
  number = {4},
  pages = {341--370},
  issn = {1076-9986},
  doi = {10.3102/1076998606298043}
}

@article{Hedges2012ABk,
  title = {A Standardized Mean Difference Effect Size for Single Case Designs},
  author = {Hedges, Larry V and Pustejovsky, James E and Shadish, William R},
  year = {2012},
  journal = {Research Synthesis Methods},
  volume = {3},
  pages = {224--239},
  issn = {17592879},
  doi = {10.1002/jrsm.1052},
  copyright = {All rights reserved},
  keywords = {autocorrelation,broader category of repeated,distinguished by the fact,effect size,hierarchical linear model,individual and,measure an outcome over,measures,single case designs,single case designs are,special type of the,such designs are a,that they assign different,time,treatments to the same}
}

@article{Hedges2012MB,
  title = {A Standardized Mean Difference Effect Size for Multiple Baseline Designs across Individuals},
  author = {Hedges, Larry V and Pustejovsky, James E and Shadish, William R},
  year = {2013},
  month = aug,
  journal = {Research Synthesis Methods},
  institution = {{Northwestern University}},
  location = {Evanston, IL},
  issn = {17592879},
  doi = {10.1002/jrsm.1086},
  copyright = {All rights reserved},
  keywords = {as a tool for,class of research methods,different conditions to the,effect size,evaluating interventions,hierarchical linear model,involving deliberate assignment of,multiple baseline designs,of one or more,outcomes over time,same individual and measurement,single-case design,single-case designs are a,these}
}

@article{higgins2009reevaluation,
  title = {A Re-Evaluation of Random-Effects Meta-Analysis},
  author = {Higgins, Julian P. T. and Thompson, Simon G. and Spiegelhalter, David J.},
  year = {2009},
  month = jan,
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume = {172},
  number = {1},
  pages = {137--159},
  issn = {09641998, 1467985X},
  doi = {10.1111/j.1467-985X.2008.00552.x},
  abstract = {Meta-analysis in the presence of unexplained heterogeneity is frequently undertaken by using a random-effects model, in which the effects underlying different studies are assumed to be drawn from a normal distribution. Here we discuss the justification and interpretation of such models, by addressing in turn the aims of estimation, prediction and hypothesis testing. A particular issue that we consider is the distinction between inference on the mean of the random-effects distribution and inference on the whole distribution. We suggest that random-effects meta-analyses as currently conducted often fail to provide the key results, and we investigate the extent to which distribution-free, classical and Bayesian approaches can provide satisfactory methods. We conclude that the Bayesian approach has the advantage of naturally allowing for full uncertainty, especially for prediction. However, it is not without problems, including computational intensity and sensitivity to a priori judgements. We propose a simple prediction interval for classical meta-analysis and offer extensions to standard practice of Bayesian meta-analysis, making use of an example of studies of `set shifting' ability in people with eating disorders.},
  langid = {english}
}

@incollection{horner2014Visual,
  title = {Visual Analysis of Single-Case Intervention Research: {{Conceptual}} and Methodological Issues},
  shorttitle = {Visual Analysis of Single-Case Intervention Research},
  booktitle = {Single-Case Intervention Research: {{Methodological}} and Statistical Advances},
  author = {Horner, Robert H. and Swoboda, Christopher M.},
  editor = {Kratochwill, Thomas R. and Levin, Joel R.},
  year = {2014},
  series = {School Psychology Series},
  pages = {91--125},
  publisher = {{American Psychological Association}},
  address = {{Washington, DC, US}},
  doi = {10.1037/14376-004},
  abstract = {The purposes of this chapter are to (a) review the traditional applications of and arguments for visual analysis in single-case intervention research, (b) review research on visual analysis, and (c) recommend options for supplementing visual analysis of data with recently proposed formal statistical procedures. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  isbn = {978-1-4338-1751-9},
  keywords = {Analysis,Experimental Design,Experimentation,Intervention,Statistical Data,Statistics}
}

@article{hutchinson1993Effects,
  title = {Effects of {{Cognitive Strategy Instruction}} on {{Algebra Problem Solving}} of {{Adolescents}} with {{Learning Disabilities}}},
  author = {Hutchinson, Nancy L.},
  year = {1993},
  month = feb,
  journal = {Learning Disability Quarterly},
  volume = {16},
  number = {1},
  pages = {34--63},
  publisher = {{SAGE Publications Inc}},
  issn = {0731-9487},
  doi = {10.2307/1511158},
  abstract = {This study investigated the effects of a two-phase cognitive strategy on algebra problem solving of adolescents with learning disabilities. The strategy was designed to enable students to represent and solve three types of word problems. The study used a modified multiple baseline with 11 replications as well as a two-group design. Conditions of the multiple-baseline design included baseline, instruction to mastery, transfer, and maintenance. Visual analysis of the single-subject data showed the strategy to be an effective intervention for this sample of students with deficits in algebra problem solving, but with criterial knowledge of basic operations and one-step problems. Statistical analyses of the two-group data showed that the instructed students had significantly higher posttest scores than the comparison group. Overall, the instructed students demonstrated improved performance on algebra word problems. Maintenance and transfer of the strategy were evident. This study has implications for teaching complex problem solving to adolescents with learning disabilities in secondary schools.},
  langid = {english}
}

@article{jamshidi2018Methodological,
  title = {Methodological Quality of Meta-Analyses of Single-Case Experimental Studies},
  author = {Jamshidi, Laleh and Heyvaert, Mieke and Declercq, Lies and {Fern{\'a}ndez-Castilla}, Bel{\'e}n and Ferron, John M. and Moeyaert, Mariola and Beretvas, S. Natasha and Onghena, Patrick and {Van den Noortgate}, Wim},
  year = {2018},
  month = aug,
  journal = {Research in Developmental Disabilities},
  volume = {79},
  pages = {97--115},
  issn = {08914222},
  doi = {10.1016/j.ridd.2017.12.016},
  abstract = {Background: Methodological rigor is a fundamental factor in the validity and credibility of the results of a meta-analysis. Aim: Following an increasing interest in single-case experimental design (SCED) meta-analyses, the current study investigates the methodological quality of SCED meta-analyses. Methods and procedures: We assessed the methodological quality of 178 SCED meta-analyses published between 1985 and 2015 through the modified Revised-Assessment of Multiple Systematic Reviews (R-AMSTAR) checklist. Outcomes and results: The main finding of the current review is that the methodological quality of the SCED meta-analyses has increased over time, but is still low according to the R-AMSTAR checklist. A remarkable percentage of the studies (93.80\% of the included SCED meta-analyses) did not even reach the midpoint score (22, on a scale of 0\textendash 44). The mean and median methodological quality scores were 15.57 and 16, respectively. Relatively high scores were observed for ``providing the characteristics of the included studies'' and ``doing comprehensive literature search''. The key areas of deficiency were ``reporting an assessment of the likelihood of publication bias'' and ``using the methods appropriately to combine the findings of studies''. Conclusions and implications: Although the results of the current review reveal that the methodological quality of the SCED meta-analyses has increased over time, still more efforts are needed to improve their methodological quality.},
  langid = {english}
}

@techreport{Kratochwill2010single,
  title = {Single-{{Case Designs Technical Documentation}}, {{Version}} 1.0 ({{Pilot}})},
  author = {Kratochwill, Thomas R and Hitchcock, Jill and Horner, Robert H and Levin, Joel R and Odom, Samuel L and Rindskopf, David M and Shadish, William R},
  year = {2010},
  volume = {0},
  number = {June},
  institution = {{What Works Clearinghouse}},
  keywords = {SCD,Single-Case Design,What Works Clearinghouse}
}

@article{kratochwill2013SingleCase,
  title = {Single-{{Case Intervention Research Design Standards}}},
  author = {Kratochwill, Thomas R. and Hitchcock, John H. and Horner, Robert H. and Levin, Joel R. and Odom, Samuel L. and Rindskopf, David M. and Shadish, William R.},
  year = {2013},
  month = jan,
  journal = {Remedial and Special Education},
  volume = {34},
  number = {1},
  pages = {26--38},
  issn = {0741-9325, 1538-4756},
  doi = {10.1177/0741932512452794},
  abstract = {In an effort to responsibly incorporate evidence based on single-case designs (SCDs) into the What Works Clearinghouse (WWC) evidence base, the WWC assembled a panel of individuals with expertise in quantitative methods and SCD methodology to draft SCD standards. In this article, the panel provides an overview of the SCD standards recommended by the panel (henceforth referred to as the Standards) and adopted in Version 1.0 of the WWC's official pilot standards. The Standards are sequentially applied to research studies that incorporate SCDs. The design standards focus on the methodological soundness of SCDs, whereby reviewers assign the categories of Meets Standards, Meets Standards With Reservations, and Does Not Meet Standards to each study. Evidence criteria focus on the credibility of the reported evidence, whereby the outcome measures that meet the design standards (with or without reservations) are examined by reviewers trained in visual analysis and categorized as demonstrating Strong Evidence, Moderate Evidence, or No Evidence. An illustration of an actual research application of the Standards is provided. Issues that the panel did not address are presented as priorities for future consideration. Implications for research and the evidence-based practice movement in psychology and education are discussed. The WWC's Version 1.0 SCD standards are currently being piloted in systematic reviews conducted by the WWC. This document reflects the initial standards recommended by the authors as well as the underlying rationale for those standards. It should be noted that the WWC may revise the Version 1.0 standards based on the results of the pilot; future versions of the WWC standards can be found at http://www.whatworks.ed.gov.},
  langid = {english}
}

@article{kratochwill2021Singlecase,
  title = {Single-Case Design Standards: {{An}} Update and Proposed Upgrades},
  shorttitle = {Single-Case Design Standards},
  author = {Kratochwill, Thomas R. and Horner, Robert H. and Levin, Joel R. and Machalicek, Wendy and Ferron, John and Johnson, Austin},
  year = {2021},
  month = dec,
  journal = {Journal of School Psychology},
  volume = {89},
  pages = {91--105},
  issn = {00224405},
  doi = {10.1016/j.jsp.2021.10.006},
  langid = {english}
}

@article{lambert2006effects,
  title = {Effects of Response Cards on Disruptive Behavior and Academic Responding during Math Lessons by Fourth-Grade Urban Students},
  author = {Lambert, Michael Charles and Cartledge, Gwendolyn and Heward, William L and Lo, Ya-yu},
  year = {2006},
  journal = {Journal of Positive Behavior Interventions},
  volume = {8},
  number = {2},
  pages = {88},
  publisher = {{SAGE Publications}}
}

@book{lipsey2001practical,
  title = {Practical {{Meta-Analysis}}},
  author = {Lipsey, Mark W and Wilson, David B},
  year = {2001},
  publisher = {{Sage Publications, Inc}},
  address = {{Thousand Oaks, CA}}
}

@article{maggin2011Quantitative,
  title = {A {{Quantitative Synthesis}} of {{Methodology}} in the {{Meta-Analysis}} of {{Single-Subject Research}} for {{Students}} with {{Disabilities}}: 1985\textendash 2009},
  shorttitle = {A {{Quantitative Synthesis}} of {{Methodology}} in the {{Meta-Analysis}} of {{Single-Subject Research}} for {{Students}} with {{Disabilities}}},
  author = {Maggin, Daniel M. and O'Keeffe, Breda V. and Johnson, Austin H.},
  year = {2011},
  month = apr,
  journal = {Exceptionality},
  volume = {19},
  number = {2},
  pages = {109--135},
  issn = {0936-2835, 1532-7035},
  doi = {10.1080/09362835.2011.565725},
  langid = {english}
}

@article{Maggin2017meta-analysis,
  title = {A Meta-Analysis of School-Based Group Contingency Interventions for Students with Challenging Behavior: {{An}} Update},
  author = {Maggin, Daniel M and Pustejovsky, James E and Johnson, A.H. Austin H},
  year = {2017},
  month = nov,
  journal = {Remedial and Special Education},
  volume = {38},
  number = {6},
  pages = {353--370},
  issn = {0741-9325},
  doi = {10.1177/0741932517716900},
  abstract = {\textcopyright{} 2017, \textcopyright{} Hammill Institute on Disabilities 2017. Group contingencies are recognized as a potent intervention for addressing challenging student behavior in the classroom, with research reviews supporting the use of this intervention platform going back more than four decades. Over this time period, the field of education has increasingly emphasized the role of research evidence for informing practice, as reflected in the increased use of systematic reviews and meta-analyses. In the current article, we continue this trend by applying recently developed between-case effect size measures and transparent visual analysis procedures to synthesize an up-to-date set of group contingency studies that used single-case designs. Results corroborated recent systematic reviews by indicating that group contingencies are generally effective\textemdash particularly for addressing challenging behavior in general education classrooms. However, our review highlights the need for more research on students with disabilities and the need to collect and report information about participants' functional level.},
  copyright = {All rights reserved},
  keywords = {behavior,evidence-based practice,management,meta-analysis,research methodology,single-subject}
}

@article{maggin2021Commentary,
  title = {Commentary on the {{{\emph{What Works Clearinghouse Standards}}}}{\emph{ and }}{{{\emph{Procedures Handbook}}}} (v. 4.1) for the {{Review}} of {{Single-Case Research}}},
  author = {Maggin, Daniel M. and Barton, Erin and Reichow, Brian and Lane, Kathleen and Shogren, Karrie A.},
  year = {2021},
  month = oct,
  journal = {Remedial and Special Education},
  pages = {074193252110513},
  issn = {0741-9325, 1538-4756},
  doi = {10.1177/07419325211051317},
  abstract = {The What Works Clearinghouse (WWC) provides school personnel with information on the amount and quality of evidence for educational programs, policies, and interventions. Over a decade ago, the WWC expanded their review procedures to include single-case research methods. Originally included as pilot standards, the recent updates elevated single-case research to a more prominent role for informing the development of evidence reviews and practice guides. While we applaud the removal of the pilot designation, our review of the updated procedures revealed concerns that, in our estimation, systematically favor studies based on nonexperimental criteria and inadequately address many issues and challenges with single-case research methods while overlooking other important concerns. As such, we are concerned that the current procedures will reduce the quality of information drawn from single-case research and disseminated to school personnel. In the following commentary, we describe these concerns and provide solutions-based recommendations for strengthening the standards and review process.},
  langid = {english}
}

@article{mancil2006Functional,
  title = {Functional {{Communication Training}} in the {{Natural Environment}}: {{A Pilot Investigation}} with a {{Young Child}} with {{Autism Spectrum Disorder}}},
  shorttitle = {Functional {{Communication Training}} in the {{Natural Environment}}},
  author = {Mancil, G. Richmond and Conroy, Maureen A. and Nakao, Taketo and Alter, Peter J.},
  year = {2006},
  journal = {Education and Treatment of Children},
  volume = {29},
  number = {4},
  pages = {615--633},
  publisher = {{West Virginia University Press}},
  issn = {0748-8491},
  abstract = {A child with Autism Spectrum Disorder (ASD) and a history of aberrant behaviors participated in this study with his mother. The primary purpose of the current study was to determine the effectiveness and efficiency of FCT on decreasing problem behaviors, increasing communication mands, and increasing spontaneous communication with a child with ASD in his home environment. Further, the number and diversity of spontaneous verbalizations were anecdotally recorded. The results of the multiple baseline study across mands demonstrated a dramatic decrease in aberrant behavior while increasing the number of mands and the latency to respond. In addition, the participant's number and diversity of words dramatically increased.}
}

@article{mason2016Video,
  title = {Video {{Self-Modeling}} for {{Individuals}} with {{Disabilities}}: {{A Best-Evidence}}, {{Single Case Meta-Analysis}}},
  shorttitle = {Video {{Self-Modeling}} for {{Individuals}} with {{Disabilities}}},
  author = {Mason, Rose A. and Davis, Heather S. and Ayres, Kevin M. and Davis, John L. and Mason, Benjamin A.},
  year = {2016},
  month = aug,
  journal = {Journal of Developmental and Physical Disabilities},
  volume = {28},
  number = {4},
  pages = {623--642},
  issn = {1573-3580},
  doi = {10.1007/s10882-016-9484-2},
  abstract = {Video-based modeling capitalizes on technology to increase the efficiency of modeling interventions for individuals with disabilities. Video self-modeling is a specific form of video-based modeling that utilizes the learner as the model to provide an opportunity for the learner to view him or herself as competent. This meta-analysis investigated the efficacy of video-self modeling using articles that met quality criteria with a focus on potential moderators of effect including participant characteristics, targeted outcomes, and implementation components. Results indicated strong effects for preschool and elementary aged participants with autism spectrum disorders, particularly when social-communicative and behavioral outcomes were addressed. In regards to implementation variables, significantly larger effect sizes were obtained when positive self-review was implemented as compared to feed forward. Additionally, video self-modeling implemented alone yielded stronger effects than video self-modeling implemented with programmed reinforcement or as part of a package. Gaps in the evidence are identified including limited evidence for the use of video self-modeling with older participants and participants with disabilities other than autism. Implications and suggestions for future research are discussed.},
  langid = {english}
}

@article{montgomery2004relative,
  title = {The Relative Efficacy of Two Brief Treatments for Sleep Problems in Young Learning Disabled (Mentally Retarded) Children: A Randomised Controlled Trial},
  shorttitle = {The Relative Efficacy of Two Brief Treatments for Sleep Problems in Young Learning Disabled (Mentally Retarded) Children},
  author = {Montgomery, P},
  year = {2004},
  month = feb,
  journal = {Archives of Disease in Childhood},
  volume = {89},
  number = {2},
  pages = {125--130},
  issn = {0003-9888, 1468-2044},
  doi = {10.1136/adc.2002.017202},
  langid = {english}
}

@article{odom2018Betweencase,
  title = {Between-Case Standardized Effect Size Analysis of Single Case Designs: {{Examination}} of the Two Methods},
  shorttitle = {Between-Case Standardized Effect Size Analysis of Single Case Designs},
  author = {Odom, Samuel L. and Barton, Erin E. and Reichow, Brian and Swaminathan, Hariharan and Pustejovsky, James E.},
  year = {2018},
  month = aug,
  journal = {Research in Developmental Disabilities},
  volume = {79},
  pages = {88--96},
  issn = {08914222},
  doi = {10.1016/j.ridd.2018.05.009},
  langid = {english}
}

@article{ota2002Task,
  title = {Task Engagement and Mathematics Performance in Children with Attention-Deficit Hyperactivity Disorder: {{Effects}} of Supplemental Computer Instruction},
  shorttitle = {Task Engagement and Mathematics Performance in Children with Attention-Deficit Hyperactivity Disorder},
  author = {Ota, Kenji R. and DuPaul, George J.},
  year = {2002},
  journal = {School Psychology Quarterly},
  volume = {17},
  number = {3},
  pages = {242--257},
  publisher = {{Guilford Publications}},
  address = {{US}},
  issn = {1939-1560},
  doi = {10.1521/scpq.17.3.242.20881},
  abstract = {Examined the effects of using software with a game format (as a supplement to teacher instruction) to improve math performance of 3 male 4th- to 6th-grade students with attention-deficit hyperactivity disorder. Following baseline (observation under normal classroom conditions), the math software was introduced sequentially using a multiple baseline design across participants. Observational data were collected during the baseline and experimental conditions along with a set of curriculum-based math probes, which were used throughout the study. The hypothesis that math software with a game format would improve the academic performance and increase attention of all participants was partially supported. Implications for practice and further research are discussed. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {Attention Deficit Disorder with Hyperactivity,Computer Games,Elementary School Students,Mathematics Achievement,Time On Task}
}

@article{parker2012Application,
  title = {An {{Application}} of {{Brief Experimental Analysis}} with {{Early Writing}}},
  author = {Parker, David C. and Dickey, Bradley N. and Burns, Matthew K. and McMaster, Kristen L.},
  year = {2012},
  month = dec,
  journal = {Journal of Behavioral Education},
  volume = {21},
  number = {4},
  pages = {329--349},
  issn = {1573-3513},
  doi = {10.1007/s10864-012-9151-3},
  abstract = {Students' poor performance on national assessments of writing suggests that educators need effective approaches to assess and intervene with writing problems. Brief experimental analysis (BEA) has supporting evidence for identifying interventions in reading, but little research has investigated BEA with writing. Early writing is an especially important period for students, and the current study sought to extend BEA research in early writing. Results showed that BEAs for 3 first grade students identified promising writing interventions, and extended analyses showed improved performance for each student following implementation of the interventions. Implications for future research in direct assessment and intervention are discussed.},
  langid = {english}
}

@article{peltier2020Effects,
  title = {Effects of {{Schema-Based Instruction}} on {{Immediate}}, {{Generalized}}, and {{Combined Structured Word Problems}}},
  author = {Peltier, Corey and Sinclair, Tracy E. and Pulos, Joshua M. and Suk, Andrea},
  year = {2020},
  month = aug,
  journal = {The Journal of Special Education},
  volume = {54},
  number = {2},
  pages = {101--112},
  publisher = {{SAGE Publications Inc}},
  issn = {0022-4669},
  doi = {10.1177/0022466919883397},
  abstract = {Instruction targeting the underlying math problem structure is identified as an evidence-based practice for students with a specific learning disability (SLD). Furthermore, schema-based instruction is identified as a potentially evidence-based practice for students with a SLD. This study extended prior work by (a) using a teacher as the implementer, (b) evaluating the efficacy of an adaptable intervention, and (c) evaluating student performance on generalized and combined schema structure problems. The participants included 12 fourth- and fifth-grade students with a disability and receiving supplemental mathematics instruction in a resource room setting. The intervention package consisted of a problem-solving mnemonic and schema-based instruction for mathematics. A multiple-probe design across participant groups was used to establish a functional relation. Students improved performance on word problems representing simple, generalized, and combined schema structures. The aggregated Tau-U effect size (ES) for this study was 95\% (CI90 [83\%, 100\%]) and the aggregated between-case standardized mean difference (BC-SMD) was 3.05 (CI95 [2.54, 3.60]).},
  langid = {english},
  keywords = {mathematics,problem solving,schema-based instruction}
}

@article{Pustejovsky2014design,
  title = {Design-Comparable Effect Sizes in Multiple Baseline Designs: {{A}} General Modeling Framework},
  author = {Pustejovsky, James E and Hedges, Larry V and Shadish, William R},
  year = {2014},
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {39},
  number = {5},
  pages = {368--393},
  issn = {1076-9986},
  doi = {10.3102/1076998614547577},
  abstract = {\textcopyright{} 2014 AERA. In single-case research, the multiple baseline design is a widely used approach for evaluating the effects of interventions on individuals. Multiple baseline designs involve repeated measurement of outcomes over time and the controlled introduction of a treatment at different times for different individuals. This article outlines a general framework for defining effect sizes in multiple baseline designs that are directly comparable to the standardized mean difference from a between-subjects randomized experiment. The target, design-comparable effect size parameter can be estimated using restricted maximum likelihood together with a small sample correction analogous to Hedges's g. The approach is demonstrated using hierarchical linear models that include baseline time trends and treatment-by-time interactions. A simulation compares the performance of the proposed estimator to that of an alternative, and an application illustrates the model-fitting process.},
  copyright = {All rights reserved},
  keywords = {Effect size,Hierarchical linear model,Single-case research}
}

@incollection{pustejovsky2017Research,
  title = {Research {{Synthesis}} and {{Meta-Analysis}} of {{Single-Case Designs}}},
  booktitle = {Handbook of {{Special Education}}},
  author = {Pustejovsky, James E and Ferron, John},
  year = {2017},
  edition = {2nd Edition},
  pages = {63},
  publisher = {{Routledge}},
  address = {{New York, NY}},
  copyright = {All rights reserved},
  langid = {english}
}

@book{pustejovsky2018SingleCaseES,
  title = {{{SingleCaseES}}: {{A Calculator}} for {{Single-Case Effect Sizes}}},
  author = {Pustejovsky, James E. and Swan, Daniel M.},
  year = {2018},
  copyright = {All rights reserved}
}

@article{pustejovsky2018Using,
  title = {Using Response Ratios for Meta-Analyzing Single-Case Designs with Behavioral Outcomes},
  author = {Pustejovsky, James E.},
  year = {2018},
  month = jun,
  journal = {Journal of School Psychology},
  volume = {68},
  pages = {99--112},
  issn = {00224405},
  doi = {10.1016/j.jsp.2018.02.003},
  langid = {english}
}

@misc{pustejovsky2021scdhlm,
  title = {Scdhlm: {{A}} Web-Based Calculator for between-Case Standardized Mean Differences},
  author = {Pustejovsky, James E. and Chen, Man and Hamilton, Bethany},
  year = {2021},
  copyright = {All rights reserved}
}

@article{reichow2018Development,
  title = {Development and Applications of the Single-Case Design Risk of Bias Tool for Evaluating Single-Case Design Research Study Reports},
  author = {Reichow, Brian and Barton, Erin E. and Maggin, Daniel M.},
  year = {2018},
  month = aug,
  journal = {Research in Developmental Disabilities},
  volume = {79},
  pages = {53--64},
  issn = {08914222},
  doi = {10.1016/j.ridd.2018.05.008},
  abstract = {Systematic reviews and meta-analyses can be a useful method for synthesizing evidence across multiple studies to draw conclusions about a research base. An important aspect of rigorous systematic reviews is an assessment of the study methods and potential biases impacting results or interpretations and conclusions of the primary studies. Single-case design (SCD) research has been a primary mechanism for identifying evidence-based practices across disciplines, but primarily in behavioral fields and special education. While the Cochrane Risk of Bias tool has been adapted for use in reviews of non-randomized studies, there is currently no guidance for evaluating SCD research. Hence, we developed a single case design risk of bias tool (SCD RoB) based on current conceptualizations of biases that might affect the validity of claims from single-case design research. We used the Cochrane risk of bias criteria and contemporary single-case design quality indicators and design standards to guide development. We describe the SCD RoB tool and two early applications of its use to demonstrate its application and provide initial validation. We also provide an overview of future areas of research using the SCD RoB tool in an effort to advance the science of single-case design research methods.},
  langid = {english}
}

@article{rodgers2021Effects,
  title = {Effects of a {{Text-Writing Fluency Intervention}} for {{Postsecondary Students}} with {{Intellectual}} and {{Developmental Disabilities}}},
  author = {Rodgers, Derek B. and Datchuk, S. M. and Rila, A. L.},
  year = {2021},
  month = aug,
  journal = {Exceptionality},
  volume = {29},
  number = {4},
  pages = {310--325},
  issn = {0936-2835, 1532-7035},
  doi = {10.1080/09362835.2020.1850451},
  langid = {english}
}

@misc{rohatgi2015Webplotdigitizer,
  title = {Webplotdigitizer},
  shorttitle = {Webplotdigitizer},
  author = {Rohatgi, Ankit},
  year = {2015},
  month = oct,
  doi = {10.5281/zenodo.32375},
  abstract = {Release Notes: New Features: Initial implementation of grid removal. Log scale support for polar diagrams. Bug fixes and code clean-up: Eliminate dependence on numeric.js for a lighter and more predictable code base. Improve rendering of data points. Refactor measurement code to allow for path length and area measurement tools in the future.},
  howpublished = {Zenodo}
}

@article{Shadish2007methods,
  title = {Methods for Evidence-Based Practice: {{Quantitative}} Synthesis of Single-Subject Designs},
  author = {Shadish, William R and Rindskopf, David M},
  year = {2007},
  journal = {New Directions for Evaluation},
  volume = {113},
  number = {113},
  pages = {95--109},
  publisher = {{Wiley Online Library}},
  issn = {10976736},
  doi = {10.1002/ev.217}
}

@article{Shadish2013d,
  title = {Analysis and Meta-Analysis of Single-Case Designs with a Standardized Mean Difference Statistic: {{A}} Primer and Applications},
  author = {Shadish, William R and Hedges, Larry V and Pustejovsky, James E},
  year = {2014},
  month = dec,
  journal = {Journal of School Psychology},
  volume = {52},
  number = {2},
  pages = {123--147},
  publisher = {{Elsevier B.V.}},
  issn = {0022-4405},
  doi = {10.1016/j.jsp.2013.11.005},
  copyright = {All rights reserved},
  keywords = {Analysis,d-Statistic,Meta-analysis,Single case designs}
}

@article{shadish2015metaanalytic,
  title = {The Meta-Analytic Big Bang},
  author = {Shadish, William R. and Lecy, Jesse D.},
  year = {2015},
  journal = {Research Synthesis Methods},
  volume = {6},
  number = {3},
  pages = {246--264},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1132},
  abstract = {This article looks at the impact of meta-analysis and then explores why meta-analysis was developed at the time and by the scholars it did in the social sciences in the 1970s. For the first problem, impact, it examines the impact of meta-analysis using citation network analysis. The impact is seen in the sciences, arts and humanities, and on such contemporaneous developments as multilevel modeling, medical statistics, qualitative methods, program evaluation, and single-case design. Using a constrained snowball sample of citations, we highlight key articles that are either most highly cited or most central to the systematic review network. Then, the article examines why meta-analysis came to be in the 1970s in the social sciences through the work of Gene Glass, Robert Rosenthal, and Frank Schmidt, each of whom developed similar theories of meta-analysis at about the same time. The article ends by explaining how Simonton's chance configuration theory and Campbell's evolutionary epistemology can illuminate why meta-analysis occurred with these scholars when it did and not in medical sciences. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1132}
}

@techreport{shadish2015Role,
  title = {The {{Role}} of {{Between-Case Effect Size}} in {{Conducting}}, {{Interpreting}}, and {{Summarizing Single-Case Research}}},
  author = {Shadish, William R and Hedges, Larry V and Horner, Robert H and Odom, Samuel L},
  year = {2015},
  month = dec,
  number = {NCER 2015-002},
  pages = {109},
  address = {{Washington, DC}},
  institution = {{National Center for Education Research, Institute of Education Sciences, U.S. Department of Education}},
  langid = {english}
}

@article{stotz2008Effects,
  title = {Effects of {{Self-graphing}} on {{Written Expression}} of {{Fourth Grade Students}} with {{High-Incidence Disabilities}}},
  author = {Stotz, Kate E. and Itoi, Madoka and Konrad, Moira and {Alber-Morgan}, Sheila R.},
  year = {2008},
  month = jun,
  journal = {Journal of Behavioral Education},
  volume = {17},
  number = {2},
  pages = {172--186},
  issn = {1573-3513},
  doi = {10.1007/s10864-007-9055-9},
  abstract = {The purpose of this study was to determine the effects of self-graphing on the writing of 3 fourth grade students with high-incidence disabilities. Measures of written expression included total number of words written and number of correct word sequences. During intervention, students self-graphed their total number of words written in response to a timed story starter. A functional relationship was found between the self-graphing intervention and the total words written and number of correct word sequences. Implications for future research and practice are discussed.},
  langid = {english}
}

@article{Swaminathan2014effect,
  title = {An Effect Size Measure and {{Bayesian}} Analysis of Single-Case Designs},
  author = {Swaminathan, Hariharan and Rogers, H. Jane and Horner, Robert H},
  year = {2014},
  journal = {Journal of School Psychology},
  publisher = {{Society for the Study of School Psychology}},
  issn = {00224405},
  doi = {10.1016/j.jsp.2013.12.002},
  keywords = {Autocorrelation,Bayesian analysis,standardized effect size}
}

@article{tate2016SingleCase,
  title = {The {{Single-Case Reporting Guideline In BEhavioural Interventions}} ({{SCRIBE}}) 2016: {{Explanation}} and {{Elaboration}}},
  author = {Tate, Robyn L and Perdices, Michael and Rosenkoetter, Ulrike and Togher, Leanne and McDonald, Skye and Shadish, William and Horner, Robert and Kratochwill, Thomas and Barlow, David H and Kazdin, Alan and Sampson, Margaret and Shamseer, Larissa and Vohra, Sunita},
  year = {2016},
  pages = {22},
  abstract = {There is substantial evidence that research studies reported in the scientific literature do not provide adequate information so that readers know exactly what was done and what was found. This problem has been addressed by the development of reporting guidelines which tell authors what should be reported and how it should be described. Many reporting guidelines are now available for different types of research designs. There is no such guideline for one type of research design commonly used in the behavioral sciences, the single-case experimental design (SCED). The present study addressed this gap. This report describes the Single-Case Reporting guideline In BEhavioural interventions (SCRIBE) 2016, which is a set of 26 items that authors need to address when writing about SCED research for publication in a scientific journal. Each item is described, a rationale for its inclusion is provided, and examples of adequate reporting taken from the literature are quoted. It is recommended that the SCRIBE 2016 is used by authors preparing manuscripts describing SCED research for publication, as well as journal reviewers and editors who are evaluating such manuscripts.},
  langid = {english}
}

@article{taylor2022Promoting,
  title = {Promoting {{Knowledge Accumulation About Intervention Effects}}: {{Exploring Strategies}} for {{Standardizing Statistical Approaches}} and {{Effect Size Reporting}}},
  shorttitle = {Promoting {{Knowledge Accumulation About Intervention Effects}}},
  author = {Taylor, Joseph A. and Pigott, Terri and Williams, Ryan},
  year = {2022},
  month = jan,
  journal = {Educational Researcher},
  volume = {51},
  number = {1},
  pages = {72--80},
  issn = {0013-189X, 1935-102X},
  doi = {10.3102/0013189X211051319},
  abstract = {Toward the goal of more rapid knowledge accumulation via better meta-analyses, this article explores statistical approaches intended to increase the precision and comparability of effect sizes from education research. The featured estimate of the proposed approach is a standardized mean difference effect size whose numerator is a mean difference that has been adjusted for baseline differences in the outcome measure, at a minimum, and whose denominator is the total variance. The article describes the utility and efficiency of covariate adjustment through baseline measures and the need to standardize effects on a total variance that accounts for variation at multiple levels. As computation of the total variance can be complex in multilevel studies, a shiny application is provided to assist with computation of the total variance and subsequent effect size. Examples are provided for how to interpret and input the required calculator inputs.},
  langid = {english}
}

@article{VandenNoortgate2008multilevel,
  title = {A Multilevel Meta-Analysis of Single-Subject Experimental Design Studies},
  author = {{Van den Noortgate}, Wim and Onghena, Patrick},
  year = {2008},
  journal = {Evidence-Based Communication Assessment and Intervention},
  volume = {2},
  number = {3},
  pages = {142--151},
  issn = {1748-9539},
  doi = {10.1080/17489530802505362},
  keywords = {meta-analysis,multilevel model,single-case,single-subject}
}

@techreport{whatworksclearinghouse2020What,
  title = {What {{Works Clearinghouse Standards Handbook}}},
  author = {{What Works Clearinghouse}},
  year = {2020},
  number = {Version 4.1},
  address = {{Washington, DC}},
  institution = {{U.S. Department of Education, Institute of Education Sciences, National Center for Education Evaluation and Regional Assistance.}}
}

@article{White1987some,
  title = {Some Comments Concerning "{{The}} Quantitative Synthesis of Single-Subject Research"},
  author = {White, Owen R},
  year = {1987},
  journal = {Remedial and Special Education},
  volume = {8},
  number = {2},
  pages = {34--39},
  issn = {0741-9325},
  doi = {10.1177/074193258700800207}
}

@article{wood2018Does,
  title = {Does {{Use}} of {{Text-to-Speech}} and {{Related Read-Aloud Tools Improve Reading Comprehension}} for {{Students With Reading Disabilities}}? {{A Meta-Analysis}}},
  shorttitle = {Does {{Use}} of {{Text-to-Speech}} and {{Related Read-Aloud Tools Improve Reading Comprehension}} for {{Students With Reading Disabilities}}?},
  author = {Wood, Sarah G. and Moxley, Jerad H. and Tighe, Elizabeth L. and Wagner, Richard K.},
  year = {2018},
  month = jan,
  journal = {Journal of Learning Disabilities},
  volume = {51},
  number = {1},
  pages = {73--84},
  publisher = {{SAGE Publications Inc}},
  issn = {0022-2194},
  doi = {10.1177/0022219416688170},
  abstract = {Text-to-speech and related read-aloud tools are being widely implemented in an attempt to assist students' reading comprehension skills. Read-aloud software, including text-to-speech, is used to translate written text into spoken text, enabling one to listen to written text while reading along. It is not clear how effective text-to-speech is at improving reading comprehension. This study addresses this gap in the research by conducting a meta-analysis on the effects of text-to-speech technology and related read-aloud tools on reading comprehension for students with reading difficulties. Random effects models yielded an average weighted effect size of (d\textasciimacron d\textasciimacron{$<$}math display="inline" id="math1-0022219416688170" overflow="scroll" altimg="eq-00001.gif"{$><$}mover accent="true"{$><$}mi{$>$}d{$<$}/mi{$><$}mo{$>$}\textasciimacron{$<$}/mo{$><$}/mover{$><$}/math{$>$} = .35, with a 95\% confidence interval of .14 to .56, p {$<$} .01). Moderator effects of study design were found to explain some of the variance. Taken together, this suggests that text-to-speech technologies may assist students with reading comprehension. However, more studies are needed to further explore the moderating variables of text-to-speech and read-aloud tools' effectiveness for improving reading comprehension. Implications and recommendations for future research are discussed.},
  langid = {english},
  keywords = {meta-analysis,reading comprehension,reading disabilities,technology,text-to-speech}
}

@article{zimmerman2018Singlecase,
  title = {Single-Case Synthesis Tools {{I}}: {{Comparing}} Tools to Evaluate {{SCD}} Quality and Rigor},
  shorttitle = {Single-Case Synthesis Tools {{I}}},
  author = {Zimmerman, Kathleen N. and Ledford, Jennifer R. and Severini, Katherine E. and Pustejovsky, James E. and Barton, Erin E. and Lloyd, Blair P.},
  year = {2018},
  month = aug,
  journal = {Research in Developmental Disabilities},
  volume = {79},
  pages = {19--32},
  issn = {08914222},
  doi = {10.1016/j.ridd.2018.02.003},
  abstract = {Tools for evaluating the quality and rigor of single case research designs (SCD) are often used when conducting SCD syntheses. Preferred components include evaluations of design features related to the internal validity of SCD to obtain quality and/or rigor ratings. Three tools for evaluating the quality and rigor of SCD (Council for Exceptional Children, What Works Clearinghouse, and Single-Case Analysis and Design Framework) were compared to determine if conclusions regarding the effectiveness of antecedent sensory-based interventions for young children changed based on choice of quality evaluation tool. Evaluation of SCD quality differed across tools, suggesting selection of quality evaluation tools impacts evaluation findings. Suggestions for selecting an appropriate quality and rigor assessment tool are provided and across-tool conclusions are drawn regarding the quality and rigor of studies. Finally, authors provide guidance for using quality evaluations in conjunction with outcome analyses when conducting syntheses of interventions evaluated in the context of SCD.},
  copyright = {All rights reserved},
  langid = {english}
}


