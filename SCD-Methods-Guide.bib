@article{2014Council,
  title = {Council for {{Exceptional Children}}: {{Standards}} for {{Evidence-Based Practices}} in {{Special Education}}},
  shorttitle = {Council for {{Exceptional Children}}},
  year = {2014},
  month = jul,
  journal = {TEACHING Exceptional Children},
  volume = {46},
  number = {6},
  pages = {206--212},
  issn = {0040-0599, 2163-5684},
  doi = {10.1177/0040059914531389},
  urldate = {2022-02-28},
  langid = {english}
}

@article{Allison_Gorman_1994, 
  title={“Make things as simple as possible, but no simpler.” A rejoinder to Scruggs and Mastropieri},
  volume={32}, 
  ISSN={0005-7967}, 
  DOI={10.1016/0005-7967(94)90170-8}, 
  abstractNote={Scruggs and Mastropieri (Behaviour Research and Therapy, 32, 879–883, 1994) take issue with criticisms of their PND (Percent of Nonoverlapping Data) statistic that we offered in our recent article (Allison & German, Behaviour Research and Therapy, 31, 621–631, 1993), which advocated a regressionbased method for obtaining effect sizes in single-subject studies. They contend that their PND approach has several advantages over our approach because: (1) they believe that, unlike ours, it can take advantage of the small number of observations that are typically available in single-case studies; (2) it is simple to compute; (3) it frees researchers from traditional regression assumptions of normality, homogeneity of variance, and independence of observations and residuals; and (4) it correlates with visual judgements made by experts. As we shall argue, these claims are built upon very questionable assumptions and they are very difficult to substantiate. In addition, we show that the expected value of the PND is so strongly related to sample size as to be rendered meaningless.}, 
  number={8}, 
  journal={Behaviour Research and Therapy}, 
  author={Allison, David B. and Gorman, Bernard S.}, 
  year={1994}, 
  month=nov, 
  pages={885–890} 
}

@article{appelbaum2018Journal,
  title = {Journal Article Reporting Standards for Quantitative Research in Psychology: {{The APA Publications}} and {{Communications Board}} Task Force Report.},
  shorttitle = {Journal Article Reporting Standards for Quantitative Research in Psychology},
  author = {Appelbaum, Mark and Cooper, Harris and Kline, Rex B. and {Mayo-Wilson}, Evan and Nezu, Arthur M. and Rao, Stephen M.},
  year = {2018},
  month = jan,
  journal = {American Psychologist},
  volume = {73},
  number = {1},
  pages = {3--25},
  issn = {1935-990X, 0003-066X},
  doi = {10.1037/amp0000191},
  urldate = {2022-02-26},
  langid = {english}
}

@article{baek_Ferron_2020, 
  title={Modeling heterogeneity of the level-1 error covariance matrix in multilevel models for single-case data}, 
  volume={16}, 
  ISSN={1614-2241}, 
  DOI={10.5964/meth.2817}, 
  abstractNote={Previous research applying multilevel models to single-case data has made a critical assumption that the level-1 error covariance matrix is constant across all participants. However, the level-1 error covariance matrix may differ across participants and ignoring these differences can have an impact on estimation and inferences. Despite the importance of this issue, the effects of modeling between-case variation in the level-1 error structure had not yet been systematically studied. The purpose of this simulation study was to identify the consequences of modeling and not modeling between-case variation in the level-1 error covariance matrices in single-case studies, using Bayesian estimation. The results of this study found that variance estimation was more sensitive to the method used to model the level-1 error structure than fixed effect estimation, with fixed effects only being impacted in the most extreme heterogeneity conditions. Implications for applied singlecase researchers and methodologists are discussed.}, 
  number={2}, 
  journal={Methodology}, 
  author={Baek, Eunkyeng and Ferron, John J. M.}, 
  year={2020}, 
  month=jun, 
  pages={166–185}, 
  language={en} 
}

@article{baek2016using,
  title={Using visual analysis to evaluate and refine multilevel models of single-case studies},
  author={Baek, Eun Kyeng and Petit-Bois, Merlande and Van den Noortgate, Wim and Beretvas, S Natasha and Ferron, John M},
  journal={The Journal of Special Education},
  volume={50},
  number={1},
  pages={18--26},
  year={2016},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{barton2017TechnologyAided,
  title = {Technology-{{Aided Instruction}} and {{Intervention}} for {{Students With ASD}}: {{A Meta-Analysis Using Novel Methods}} of {{Estimating Effect Sizes}} for {{Single-Case Research}}},
  shorttitle = {Technology-{{Aided Instruction}} and {{Intervention}} for {{Students With ASD}}},
  author = {Barton, Erin E. and Pustejovsky, James E. and Maggin, Daniel M. and Reichow, Brian},
  year = {2017},
  month = nov,
  journal = {Remedial and Special Education},
  volume = {38},
  number = {6},
  pages = {371--386},
  issn = {0741-9325, 1538-4756},
  doi = {10.1177/0741932517729508},
  urldate = {2022-02-26},
  abstract = {The adoption of methods and strategies validated through rigorous, experimentally oriented research is a core professional value of special education. We conducted a systematic review and meta-analysis examining the experimental literature on Technology-Aided Instruction and Intervention (TAII) using research identified as part of the National Autism Professional Development Project. We applied novel between-case effect size methods to the TAII single-case research base. In addition, we used meta-analytic methodologies to examine the methodological quality of the research, calculate average effect sizes to quantify the level of evidence for TAII, and compare effect sizes across single-case and group-based experimental research. Results identified one category of TAII\textemdash computer-assisted instruction\textemdash as an evidence-based practice across both single-case and group studies. The remaining two categories of TAII\textemdash augmentative and alternative communication and virtual reality\textemdash were not identified as evidence-based using What Works Clearinghouse summary ratings.},
  langid = {english}
}

@inbook{Becker_1996, 
 address={Baltimore, MD, US}, 
 title={The generalizability of empirical research results}, 
 ISBN={978-0-8018-5301-2}, 
 abstractNote={propose a systematic theory of the generalizability of research results, drawing on formal quantitative methods for research synthesis and on the theory of generalizability of measurements / the proposed theory provides a model for describing the characteristics or facets of studies that may have an impact on generalizability, and suggests a statistical method for evaluating the importance of those facets / specifically, variation in the parameters of interest is estimated and partitioned into quantities that can be attributed to differences in operationalizations of the constructs or variables of interest  the information gained can be valuable in the interpretation of existing research and in planning future studies / focus on the question of how to assess the generalizability of findings concerning relationships among variables (constructs) expressed as correlation coefficients / however, the analyses the author proposes also can be applied when other parameters (such as treatment effects) are of interest (PsycInfo Database Record (c) 2022 APA, all rights reserved)}, 
 booktitle={Intellectual talent:  Psychometric and social issues}, 
 publisher={Johns Hopkins University Press}, 
 author={Becker, Betsy Jane}, 
 year={1996}, 
 pages={362–383} 
}


@article{Beretvas2008review,
  title = {A Review of Meta-Analyses of Single-Subject Experimental Designs: {{Methodological}} Issues and Practice},
  author = {Beretvas, S Natasha and Chung, Hyewon},
  year = {2008},
  journal = {Evidence-Based Communication Assessment and Intervention},
  volume = {2},
  number = {3},
  pages = {129--141},
  publisher = {{Psychology Press}},
  issn = {1748-9539},
  doi = {10.1080/17489530802446302},
  keywords = {analysis,designs,effect sizes,meta,methodology,multiple regression,single,subject experimental,subject experimental designs}
}

@incollection{Borenstein2009effect,
  title = {Effect Sizes for Continuous Data},
  booktitle = {The {{Handbook}} of {{Research Synthesis}} and {{Meta-Analysis}}},
  author = {Borenstein, Michael},
  editor = {Cooper, Harris M and Hedges, Larry V and Valentine, John C},
  year = {2009},
  pages = {221--235},
  publisher = {{Russell Sage Foundation}},
  address = {{New York, NY}}
}

@incollection{Borenstein2019effect,
  title = {Effect Sizes for Continuous Data},
  booktitle = {The {{Handbook}} of {{Research Synthesis}} and {{Meta-Analysis}}},
  author = {Borenstein, Michael},
  editor = {Cooper, Harris M and Hedges, Larry V and Valentine, John C},
  year = {2019},
  publisher = {{Russell Sage Foundation}},
  address = {{New York, NY}}
}

@book{borenstein2021introduction,
  title = {Introduction to {{Meta-Analysis}}},
  author = {Borenstein, Michael and Hedges, Larry V and Higgins, Julian P T and Rothstein, Hannah R},
  year = {2021},
  publisher = {{John Wiley \& Sons, Ltd}},
  address = {{Chichester, UK}},
  isbn = {978-0-470-05724-7}
}


@article{bowman-perrott2016Promoting,
  title = {Promoting {{Positive Behavior Using}} the {{Good Behavior Game}}: {{A Meta-Analysis}} of {{Single-Case Research}}},
  shorttitle = {Promoting {{Positive Behavior Using}} the {{Good Behavior Game}}},
  author = {{Bowman-Perrott}, Lisa and Burke, Mack D. and Zaini, Samar and Zhang, Nan and Vannest, Kimberly},
  year = {2016},
  month = jul,
  journal = {Journal of Positive Behavior Interventions},
  volume = {18},
  number = {3},
  pages = {180--190},
  issn = {1098-3007, 1538-4772},
  doi = {10.1177/1098300715592355},
  urldate = {2022-02-26},
  abstract = {The Good Behavior Game (GBG) is a classroom management strategy that uses an interdependent group-oriented contingency to promote prosocial behavior and decrease problem behavior. This meta-analysis synthesized single-case research (SCR) on the GBG across 21 studies, representing 1,580 students in pre-kindergarten through Grade 12. The TauU effect size across 137 phase contrasts was .82 with a confidence interval 95\% CI = [0.78, 0.87], indicating a substantial reduction in problem behavior and an increase in prosocial behavior for participating students. Five potential moderators were examined: emotional and behavioral disorder (EBD) risk status, reinforcement frequency, target behaviors, GBG format, and grade level. Findings suggest that the GBG is most effective in reducing disruptive and off-task behaviors, and that students with or at risk for EBD benefit most from the intervention. Implications for research and practice are discussed.},
  langid = {english}
}

@article{Brossart_Vannest_Davis_Patience_2014, 
  title={Incorporating nonoverlap indices with visual analysis for quantifying intervention effectiveness in single-case experimental designs}, 
  volume={24}, 
  ISSN={1464-0694}, 
  DOI={10.1080/09602011.2013.868361}, 
  abstractNote={The field of neuropsychological rehabilitation frequently employs single case experimental designs (SCED) in research, but few if any, of the published studies use the effect sizes recommended by the American Psychological Association. Among the available methods for analysing single case designs, this paper focuses on nonoverlap methods. This paper provides examples and suggestions for integrating visual and statistical analysis, pointing out where contradictions may occur and how to be a critical consumer.}, 
  number={3–4}, 
  journal={Neuropsychological Rehabilitation}, 
  author={Brossart, Daniel F. and Vannest, Kimberly J. and Davis, John L. and Patience, Marc A.}, 
  year={2014}, 
  pages={464–491}, 
  language={eng} 
}

@incollection{Busk_Serlin_1992, 
  address={Hillsdale, NJ, US}, 
  title={Meta-analysis for single-case research}, 
  ISBN={978-0-8058-0515-4}, 
  abstractNote={previously proposed meta-analytic techniques for single-case research designs / guidelines for defining an effect-size measure / an effect-size measure for single-case research / suggested procedures for meta-analysis in single-case research (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  booktitle={Single-case research design and analysis:  New directions for psychology and education}, 
  publisher={Lawrence Erlbaum Associates, Inc}, 
  author={Busk, Patricia L. and Serlin, Ronald C.}, 
  editor = {Kratochwill, Thomas R. and Levin, Joel R.},
  year={1992}, 
  pages={187–212} 
}

@article{Byiers2014, 
  title={Functional communication training in rett syndrome: a preliminary study}, 
  volume={119}, 
  ISSN={1944-7515}, 
  DOI={10.1352/1944-7558-119.4.340}, 
  abstractNote={Rett syndrome (RTT) is associated with a range of serious neurodevelopmental consequences including severe communicative impairments. Currently, no evidence-based communication interventions exist for the population ( Sigafoos et al., 2009 ). The purpose of the current study was to examine the effectiveness of functional assessment (FA) and functional communication training (FCT) methods for teaching 3 individuals (ages 15-47 years) with classic RTT novel communicative behaviors. Using single-case experimental designs, functional reinforcers were identified (FA) and each participant quickly learned to activate a voice-output switch to obtain a reinforcer (FCT). These results suggest that individuals with classic RTT can learn novel communicative responses, which has important implications for future intervention research.}, 
  number={4}, 
  journal={American Journal on Intellectual and Developmental Disabilities}, 
  author={Byiers, Breanne J. and Dimian, Adele and Symons, Frank J.}, 
  year={2014}, 
  month=jul, 
  pages={340–350}, 
  language={eng} 
}


@article{case1992Improving,
  title = {Improving the {{Mathematical Problem-Solving Skills}} of {{Students}} with {{Learning Disabilities}}: {{Self-Regulated Strategy Development}}},
  shorttitle = {Improving the {{Mathematical Problem-Solving Skills}} of {{Students}} with {{Learning Disabilities}}},
  author = {Case, Lisa Pericola and Harris, Karen R. and Graham, Steve},
  year = {1992},
  month = apr,
  journal = {The Journal of Special Education},
  volume = {26},
  number = {1},
  pages = {1--19},
  issn = {0022-4669, 1538-4764},
  doi = {10.1177/002246699202600101},
  urldate = {2022-02-26},
  abstract = {Four students with learning disabilities, whose primary difficulty in solving simple word problems involved performing the wrong operation, were taught a strategy for comprehending the problem and devising an appropriate solution. Students learned to apply the strategy first to addition word problems, then to subtraction word problems. Upon completion of instruction, students' overall performance on mixed sets of addition and subtraction word problems improved, and they were much less likely to perform the wrong operation. Although the effects of instruction generalized to a different setting, maintenance of strategy effects was mixed, perhaps due to administration of the maintenance probes during summer vacation.},
  langid = {english}
}

@article{Casey1978, 
  title={Development of communicative behavior in autistic children: a parent program using manual signs}, 
  volume={8}, 
  ISSN={0021-9185}, 
  DOI={10.1007/BF01550277}, 
  abstractNote={Mothers of four 6- and 7-year-old autistic classmates were taught to use manual signs for verbalizations to aid development of appropriate communicative behavior and to deter undesirable behavior in their children. The experimental treatment was initiated in a daily mother-child laboratory session using a multiple-baseline design across subjects. The preschool classroom program remained unchanged. Data were recorded for each child daily over a 5-week period, in the classroom and in a mother-child session, on four types of communicative behavior and four types of inappropriate behavior. Communicative behaviors increased and inappropriate behaviors decreased in relation to baseline conditions for each child. The manual sign program facilitated generalization of communicative behavior to the child’s total environment. Desirable behaviors were maintained and had improved in the classroom 3 months after initiation of the program.}, 
  number={1}, 
  journal={Journal of Autism and Childhood Schizophrenia}, 
  author={Casey, L. O.}, 
  year={1978}, 
  month=mar, 
  pages={45–59}, 
  language={eng} 
}

@article{Center1985methodology,
  title = {A Methodology for the Quantitative Synthesis of Intra-Subject Design Research},
  author = {Center, B A and Skiba, R J and Casey, A},
  year = {1985},
  journal = {The Journal of Special Education},
  volume = {19},
  number = {4},
  pages = {387},
  publisher = {{SAGE Publications}}
}

@article{Chen_Pustejovsky_2022, 
  title={Multilevel meta-analysis of single-case experimental designs using robust variance estimation},
  ISSN={1082-989X}, 
  archiveLocation={2022-78012-001}, 
  url={https://ezproxy.library.wisc.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&AuthType=ip,uid&db=pdh&AN=2022-78012-001&site=ehost-live&scope=site}, 
  DOI={10.1037/met0000510.supp}, 
  abstractNote={Single-case experimental designs (SCEDs) are used to study the effects of interventions on the behavior of individual cases, by making comparisons between repeated measurements of an outcome under different conditions. In research areas where SCEDs are prevalent, there is a need for methods to synthesize results across multiple studies. One approach to synthesis uses a multilevel meta-analysis (MLMA) model to describe the distribution of effect sizes across studies and across cases within studies. However, MLMA relies on having accurate sampling variances of effect size estimates for each case, which may not be possible due to auto-correlation in the raw data series. One possible solution is to combine MLMA with robust variance estimation (RVE), which provides valid assessments of uncertainty even if the sampling variances of effect size estimates are inaccurate. Another possible solution is to forgo MLMA and use simpler, ordinary least squares (OLS) methods with RVE. This study evaluates the performance of effect size estimators and methods of synthesizing SCEDs in the presence of auto-correlation, for several different effect size metrics, via a Monte Carlo simulation designed to emulate the features of real data series. Results demonstrate that the MLMA model with RVE performs properly in terms of bias, accuracy, and confidence interval coverage for estimating overall average log response ratios. The OLS estimator corrected with RVE performs the best in estimating overall average Tau effect sizes. None of the available methods perform adequately for meta-analysis of within-case standardized mean differences. (PsycInfo Database Record (c) 2022 APA, all rights reserved)}, 
  journal={Psychological Methods}, 
  publisher={American Psychological Association}, 
  author={Chen, Man and Pustejovsky, James E.}, 
  year={2022}, 
  month=jul 
}

@article{Chen_Chen_Yang_Chiang_Hsieh_Cheng_Ding_Wu_Peng_2023, 
  title={Examining the normality assumption of a design-comparable effect size in single-case designs},
  ISSN={1554-3528}, 
  url={https://doi.org/10.3758/s13428-022-02035-8}, 
  DOI={10.3758/s13428-022-02035-8}, 
  abstractNote={What Works Clearinghouse (WWC, 2022) recommends a design-comparable effect size (D-CES; i.e., gAB) to gauge an intervention in single-case experimental design (SCED) studies, or to synthesize findings in meta-analysis. So far, no research has examined gAB’s performance under non-normal distributions. This study expanded Pustejovsky et al. (2014) to investigate the impact of data distributions, number of cases (m), number of measurements (N), within-case reliability or intra-class correlation (ρ), ratio of variance components (λ), and autocorrelation (ϕ) on gAB in multiple-baseline (MB) design. The performance of gAB was assessed by relative bias (RB), relative bias of variance (RBV), MSE, and coverage rate of 95% CIs (CR). Findings revealed that gAB was unbiased even under non-normal distributions. gAB’s variance was generally overestimated, and its 95% CI was over-covered, especially when distributions were normal or nearly normal combined with small m and N. Large imprecision of gAB occurred when m was small and ρ was large. According to the ANOVA results, data distributions contributed to approximately 49% of variance in RB and 25% of variance in both RBV and CR. m and ρ each contributed to 34% of variance in MSE. We recommend gAB for MB studies and meta-analysis with N≥16 and when either (1) data distributions are normal or nearly normal, m=6, and ρ=0.6 or 0.8, or (2) data distributions are mildly or moderately non-normal, m≥4, and ρ=0.2, 0.4, or 0.6. The paper concludes with a discussion of gAB’s applicability and design-comparability, and sound reporting practices of ES indices.}, 
  journal={Behavior Research Methods}, 
  author={Chen, Li-Ting and Chen, Yi-Kai and Yang, Tong-Rong and Chiang, Yu-Shan and Hsieh, Cheng-Yu and Cheng, Che and Ding, Qi-Wen and Wu, Po-Ju and Peng, Chao-Ying Joanne}, 
  year={2023}, 
  month=jan, 
  language={en} 
}

@article{CollierMeek2017, 
  address={US}, 
  title={Toward feasible implementation support: E-mailed prompts to promote teachers’ treatment integrity}, 
  volume={46}, 
  ISSN={2372-966X}, 
  DOI={10.17105/SPR-2017-0028.V46-4}, 
  abstractNote={Although high levels of intervention implementation are more likely to lead to improved student outcomes, educators struggle to maintain high implementation levels over time. School psychologists might provide research-supported, consequence-oriented supports (e.g., performance feedback) to promote educators’ implementation, yet these are reactive and potentially time intensive. This study evaluated whether a proactive, antecedent- oriented support (i.e., daily, preprogrammed e-mailed prompts) could effectively promote educators’ implementation. Findings indicate that for 3 of 4 teachers who participated in this multiple baseline single case design study, implementation of the class-wide behavior intervention improved upon receiving e-mailed prompts. In addition, increases in praise, decreases in corrective statements, and corresponding improvements in student outcomes were noted. This initial study suggests that prompts may be a feasible and effective Tier 1 implementation support that can be incorporated by school psychologists to support educators responsible for delivering interventions in the classroom. Additional implications for future research and school-based practice are discussed. (PsycInfo Database Record (c) 2020 APA, all rights reserved)}, 
  number={4}, 
  journal={School Psychology Review}, 
  publisher={National Assn of School Psychologists}, 
  author={Collier-Meek, Melissa A. and Fallon, Lindsay M. and DeFouw, Emily R.}, 
  year={2017}, 
  pages={379–394} 
}

@article{cook2014council,
  title={Council for Exceptional Children: Standards for evidence-based practices in special education},
  author={Cook, Bryan and Buysse, Virginia and Klingner, Janette and Landrum, Tim and McWilliam, Robin and Tankersley, Melody and Test, Dave},
  journal={Teaching Exceptional Children},
  volume={46},
  number={6},
  pages={206},
  year={2014},
  publisher={SAGE PUBLICATIONS, INC.}
}

@book{cooper2010Research,
  title = {Research {{Synthesis}} and {{Meta-Analysis}}},
  author = {Cooper, Harris M},
  year = {2010},
  edition = {4th},
  publisher = {{SAGE Publications}},
  address = {{Thousand Oaks, CA}}
}

@book{cooper2019handbook,
  title={The handbook of research synthesis and meta-analysis},
  author={Cooper, Harris and Hedges, Larry V and Valentine, Jeffrey C},
  year={2019},
  publisher={Russell Sage Foundation}
}

@article{datchuk2016Writing,
  title = {Writing {{Simple Sentences}} and {{Descriptive Paragraphs}}: {{Effects}} of an {{Intervention}} on {{Adolescents}} with {{Writing Difficulties}}},
  shorttitle = {Writing {{Simple Sentences}} and {{Descriptive Paragraphs}}},
  author = {Datchuk, Shawn M.},
  year = {2016},
  month = jun,
  journal = {Journal of Behavioral Education},
  volume = {25},
  number = {2},
  pages = {166--188},
  issn = {1053-0819, 1573-3513},
  doi = {10.1007/s10864-015-9236-x},
  urldate = {2022-02-26},
  langid = {english}
}

@article{datchuk2020Level,
  title = {Level and {{Trend}} of {{Writing Sequences}}: {{A Review}} and {{Meta-Analysis}} of {{Writing Interventions}} for {{Students With Disabilities}}},
  shorttitle = {Level and {{Trend}} of {{Writing Sequences}}},
  author = {Datchuk, Shawn M. and Wagner, Kyle and Hier, Bridget O.},
  year = {2020},
  month = jan,
  journal = {Exceptional Children},
  volume = {86},
  number = {2},
  pages = {174--192},
  issn = {0014-4029, 2163-5560},
  doi = {10.1177/0014402919873311},
  urldate = {2022-02-26},
  abstract = {We examined effects of intervention on the level and trend of text-writing sequences of students with disabilities and writing difficulties, in addition to potential moderating effects related to student demographics (i.e., disability status, age, gender, and race) and writing task (i.e., sentence, essay, and narrative). We reviewed 18 single-case experimental design studies with a total of 96 students and subsequently meta-analyzed 15 of these studies with a total of 79 students using mixed-effects linear regression and an information-theoretic ranking of competing models. Results indicate that writing interventions, including direct instruction and self-regulated strategy development, produced gradual improvement in the trend of correct writing sequences per minute. Older students produced higher levels of writing sequences, but younger students showed steeper trends during intervention. Furthermore, students had higher levels of writing fluency on sentence-writing tasks than on discourse-writing tasks (narratives and essays).},
  langid = {english}
}

@article{Declercq2020, 
  title={MultiSCED: A tool for (meta-)analyzing single-case experimental data with multilevel modeling},
  volume={52}, 
  ISSN={1554-3528}, 
  DOI={10.3758/s13428-019-01216-2}, 
  abstractNote={The MultiSCED web application has been developed to assist applied researchers in behavioral sciences to apply multilevel modeling to quantitatively summarize single-case experimental design (SCED) studies through a user-friendly point-andclick interface embedded within R. In this paper, we offer a brief introduction to the application, explaining how to define and estimate the relevant multilevel models and how to interpret the results numerically and graphically. The use of the application is illustrated through a re-analysis of an existing meta-analytic dataset. By guiding applied researchers through MultiSCED, we aim to make use of the multilevel modeling technique for combining SCED data across cases and across studies more comprehensible and accessible.}, 
  number={1}, 
  journal={Behavior Research Methods}, 
  author={Declercq, Lies and Cools, Wilfried and Beretvas, S. Natasha and Moeyaert, Mariola and Ferron, John M. and Van den Noortgate, Wim}, 
  year={2020}, 
  month=feb, 
  pages={177–192}, 
language={en} }

@article{Declercq2019, 
  title={Analysis of single-case experimental count data using the linear mixed effects model: A simulation study}, 
  volume={51}, 
  ISSN={1554-3528}, 
  DOI={10.3758/s13428-018-1091-y}, 
  abstractNote={When (meta-)analyzing single-case experimental design (SCED) studies by means of hierarchical or multilevel modeling, applied researchers almost exclusively rely on the linear mixed model (LMM). This type of model assumes that the residuals are normally distributed. However, very often SCED studies consider outcomes of a discrete rather than a continuous nature, like counts, percentages or rates. In those cases the normality assumption does not hold. The LMM can be extended into a generalized linear mixed model (GLMM), which can account for the discrete nature of SCED count data. In this simulation study, we look at the effects of misspecifying an LMM for SCED count data simulated according to a GLMM. We compare the performance of a misspecified LMM and of a GLMM in terms of goodness of fit, fixed effect parameter recovery, type I error rate, and power. Because the LMM and the GLMM do not estimate identical fixed effects, we provide a transformation to compare the fixed effect parameter recovery. The results show that, compared to the GLMM, the LMM has worse performance in terms of goodness of fit and power. Performance in terms of fixed effect parameter recovery is equally good for both models, and in terms of type I error rate the LMM performs better than the GLMM. Finally, we provide some guidelines for applied researchers about aspects to consider when using an LMM for analyzing SCED count data.}, 
  number={6}, 
  journal={Behavior Research Methods}, 
  author={Declercq, Lies and Jamshidi, Laleh and Fernández-Castilla, Belén and Beretvas, S. Natasha and Moeyaert, Mariola and Ferron, John M. and Van den Noortgate, Wim}, 
  year={2019}, 
  month=dec, 
  pages={2477–2497}, 
  language={en} 
}

@article{delemere2018ParentImplemented,
  title = {Parent-{{Implemented Bedtime Fading}} and {{Positive Routines}} for {{Children}} with {{Autism Spectrum Disorders}}},
  author = {Delemere, Emma and Dounavi, Katerina},
  year = {2018},
  month = apr,
  journal = {Journal of Autism and Developmental Disorders},
  volume = {48},
  number = {4},
  pages = {1002--1019},
  issn = {1573-3432},
  doi = {10.1007/s10803-017-3398-4},
  urldate = {2022-02-26},
  abstract = {Sleep disorders affect a large portion of those with autism spectrum disorder. Behavioural interventions have been found to increase appropriate sleep behaviours. This study sought to examine the efficacy of two stimulus control interventions (bedtime fading and positive routines) on total sleep duration, sleep onset latency and frequency and duration of night wakings for children with autism using two multiple baseline designs. Secondary dependent variables, namely, educational opportunities, challenging behaviours, parent acceptance and social validity were also analysed. Results suggest some efficacy for both interventions. Increased total sleep duration and decreased sleep onset latency were achieved with bedtime fading. Positive routines showed mixed results with decreased sleep onset latency and increased total sleep duration for two of three participants.},
  langid = {english}
}

@article{Dowdy2020, 
  title={Evaluation of publication bias in response interruption and redirection: A meta‐analysis},
  volume={53}, 
  ISSN={0021-8855, 1938-3703}, 
  DOI={10.1002/jaba.724}, 
  abstractNote={Publication bias is the disproportionate representation of studies with large effects and statistically signiﬁcant ﬁndings in the published research literature. If publication bias occurs in singlecase research design studies on applied behavior-analytic (ABA) interventions, it can result in inﬂated estimates of ABA intervention effects. We conducted an empirical evaluation of publication bias on an evidence-based ABA intervention for children diagnosed with autism spectrum disorder, response interruption and redirection (RIRD). We determined effect size estimates for published and unpublished studies using 3 metrics, percentage of nonoverlapping data (PND), Hedges’ g, and log response ratios (LRR). Omnibus effect size estimates across all 3 metrics were positive, supporting that RIRD is an effective treatment for reducing problem behavior maintained by nonsocial consequences. We observed larger PND for published compared to unpublished studies, small and nonsigniﬁcant differences in LRR for published compared to unpublished studies, and signiﬁcant differences in Hedges’ g for published compared to unpublished studies, with published studies showing slightly larger effect. We found little, if any, difference in methodological quality between published and unpublished studies. While RIRD appears to be an effective intervention for challenging behavior maintained by nonsocial consequences, our results reﬂect some degree of publication bias present in the RIRD research literature.}, 
  number={4}, 
  journal={Journal of Applied Behavior Analysis}, 
  author={Dowdy, Art and Tincani, Matt and Schneider, W. Joel}, 
  year={2020}, 
  month=sep, 
  pages={2151–2171}, 
  language={en} 
}

@article{Dowdy2022, 
  title={Meta-Analytic Methods to Detect Publication Bias in Behavior Science Research}, 
  volume={45}, 
  ISSN={2520-8977}, 
  DOI={10.1007/s40614-021-00303-0}, 
  abstractNote={Publication bias is an issue of great concern across a range of scientific fields. Although less documented in the behavior science fields, there is a need to explore viable methods for evaluating publication bias, in particular for studies based on single-case experimental design logic. Although publication bias is often detected by examining differences between meta-analytic effect sizes for published and grey studies, difficulties identifying the extent of grey studies within a particular research corpus present several challenges. We describe in this article several meta-analytic techniques for examining publication bias when published and grey literature are available as well as alternative meta-analytic techniques when grey literature is inaccessible. Although the majority of these methods have primarily been applied to meta-analyses of group design studies, our aim is to provide preliminary guidance for behavior scientists who might use or adapt these techniques for evaluating publication bias. We provide sample data sets and R scripts to follow along with the statistical analysis in hope that an increased understanding of publication bias and respective techniques will help researchers understand the extent to which it is a problem in behavior science research.}, 
  number={1}, 
  journal={Perspectives on Behavior Science}, 
  author={Dowdy, Art and Hantula, Donald A. and Travers, Jason C. and Tincani, Matt}, 
  year={2022}, 
  month=mar, 
  pages={37–52}, 
  language={en} 
}

@article{Ferron_2002, 
  title={Reconsidering the use of the general linear model with single-case data}, 
  volume={34}, 
  ISSN={1532-5970}, 
  DOI={10.3758/BF03195459}, 
  abstractNote={Using a low point estimate of autocorrelation to justify analyzing single-case data with the general linear model (GLM) is questioned. Monte Carlo methods are used to examine the degree to which bias in the estimate of autocorrelation depends on the complexity of the linear model used to describe the data. A method is then illustrated for determining the range of autocorrelation parameters that could reasonably have led to the observed autocorrelation. The argument for using a GLM analysis can be strengthened when the GLM analysis functions appropriately across the range of plausible autocorrelations. For situations in which the GLM analysis does not function appropriately across this range, a method is provided for adjusting the confidence intervals to ensure adequate coverage probabilities for specified levels of autocorrelation.}, 
  number={3}, 
  journal={Behavior Research Methods, Instruments, & Computers}, 
  author={Ferron, John}, 
  year={2002}, 
  month=aug, 
  pages={324–331}, 
  language={en} 
}
 
@article{Ferron_Goldstein_Olszewski_Rohrer_2020, 
  title={Indexing effects in single-case experimental designs by estimating the percent of goal obtained}, 
  volume={14}, 
  ISSN={1748-9539}, DOI={10.1080/17489539.2020.1732024}, 
  abstractNote={Single-case experimental designs are often used to examine the efficacy and effectiveness of interventions that are designed to change behaviors so that they are less problematic and more similar to the behavior of typically developing peers. By defining a goal level for the behavior, we were able to develop an effect size based on the percent of goal obtained (PoGO). We discuss methods for determining the goal and methods of estimating PoGO, including those for when there are and are not trends. Applicability is demonstrated using data from single-case experimental design studies of speech production, early literacy, and social skill interventions in the early childhood literature and by comparing PoGO to graphic displays and other summary measures, including Tau-U, NAP, the standardized mean difference, and the log-response ratio. We consider how PoGO could be used in systematic reviews, provide recommendations, and discuss areas in need of research.}, 
  number={1-2}, 
  journal={Evidence-Based Communication Assessment and Intervention}, 
  publisher={Taylor & Francis}, 
  author={Ferron, John and Goldstein, Howard and Olszewski, Arnold and Rohrer, Lodi}, 
  year={2020}, 
  month=apr, 
  pages={6–27} 
}
 
@article{Ferron_Bell_Hess_Rendina-Gobioff_Hibbard_2009, 
  title={Making treatment effect inferences from multiple-baseline data: The utility of multilevel modeling approaches}, 
  volume={41}, 
  ISSN={1554-351X, 1554-3528}, 
  DOI={10.3758/BRM.41.2.372}, 
  number={2}, 
  journal={Behavior Research Methods}, 
  author={Ferron, John M. and Bell, Bethany A. and Hess, Melinda R. and Rendina-Gobioff, Gianna and Hibbard, Susan T.}, 
  year={2009}, 
  month=may, 
  pages={372–384}, 
  language={en} 
}

@article{Gage2020, 
  title={Using Positive Behavioral Interventions and Supports to Reduce School Suspensions}, 
  volume={29}, 
  ISSN={1074-2956}, 
  DOI={10.1177/1074295620950611}, 
  abstractNote={The Positive Behavioral Interventions and Supports (PBIS) framework is currently implemented in more than 25,000 schools across the globe. Its implementation has demonstrated positive impacts on a number of behavioral and academic outcomes. A growing evidence base has found that PBIS has been particularly effective at reducing both in- and out-of-school suspensions. This article describes concerns with the use of suspensions, defines PBIS, and outlines how schools can implement and use PBIS to reduce suspensions.}, 
  note={ERIC Number: EJ1273750}, 
  number={3}, 
  journal={Beyond Behavior}, 
  publisher={SAGE Publications and Hammill Institute on Disabilities}, 
  author={Gage, Nicholas A. and Beahm, Lydia and Kaplan, Rachel and MacSuga-Gage, Ashley S. and Lee, Ahhyun}, 
  year={2020}, 
  month=dec, 
  pages={132–140}, 
  language={en} 
}

@article{Gage2017, 
  title={Publication Bias in Special Education Meta-Analyses}, 
  volume={83}, 
  ISSN={0014-4029}, 
  DOI={10.1177/0014402917691016}, 
  abstractNote={Publication bias involves the disproportionate representation of studies with large and significant effects in the published research. Among other problems, publication bias results in inflated omnibus effect sizes in meta-analyses, giving the impression that interventions have stronger effects than they actually do. Although evidence suggests that publication bias exists in other fields, research has not examined the issue in special education. In this study, we examined the inclusion of gray literature, testing for publication bias, the extent to which publication bias exists, the relation of including gray literature to the presence of publication bias, and differences in effect size magnitude for gray literature and published studies among 109 meta-analyses published in special education journals. We found the following: (a) 42% of meta-analyses included gray literature, (b) 33% examined publication bias, (c) meta-analyses not including gray literature were more likely to reflect publication bias, and (d) published studies had larger effect sizes than gray literature. We discuss implications and recommendations for research and practice.}, 
  number={4}, 
  journal={Exceptional Children}, 
  publisher={SAGE Publications Inc}, 
  author={Gage, Nicholas A. and Cook, Bryan G. and Reichow, Brian}, 
  year={2017}, 
  month=jul, 
  pages={428–445}, 
  language={en} 
}

@article{Gage2018, 
  title={Professional development to increase teacher behavior-specific praise: A single-case design replication}, 
  volume={55}, 
  rights={© 2018 Wiley Periodicals, Inc.}, 
  ISSN={1520-6807}, 
  DOI={10.1002/pits.22106}, 
  abstractNote={Effective classroom instruction is contingent upon successful classroom management. Unfortunately, not all teachers successfully manage classroom behavior and need in-service professional development. In this study, we replicated a targeted professional development approach that included a brief one-on-one training session and emailed visual performance feedback to increase novice teachers’ use of behavior-specific praise, an evidence-based classroom management skill. Dependent variables collected through direct observation included teachers’ behavior-specific praise along with average student engagement and disruptions. Four elementary teachers participated and, based on a multiple-baseline single-case design, we found a functional relationship between the targeted professional development and teachers’ increased use of behavior-specific praise. However, because of variability and one teacher’s limited response, effect sizes were small for behavior-specific praise and little change was observed in student behaviors. These findings warrant further research to determine which classroom management skills affect student behaviors overall, as well as continued evaluation of this professional development model and using school-based coaches.}, 
  number={3}, 
  journal={Psychology in the Schools}, 
  author={Gage, Nicholas A. and Grasley-Boy, Nicolette M. and MacSuga-Gage, Ashley S.}, 
  year={2018}, 
  pages={264–277}, 
  language={en} 
}

@article{Ganz_et_al_2023, 
  title={Participant characteristics predicting communication outcomes in AAC implementation for individuals with ASD and IDD: a systematic review and meta-analysis}, 
  volume={39}, 
  ISSN={0743-4618}, 
  DOI={10.1080/07434618.2022.2116355}, 
  abstractNote={This meta-analysis examined communication outcomes in single-case design studies of augmentative and alternative communication (AAC) interventions and their relationship to participant characteristics. Variables addressed included chronological age, pre-intervention communication mode, productive repertoire, and pre-intervention imitation skills. Investigators identified 114 single-case design studies that implemented AAC interventions with school-aged individuals with autism spectrum disorder and/or intellectual disability. Two complementary effect size indices, Tau(AB) and the log response ratio, were applied to synthesize findings. Both indices showed positive effects on average, but also exhibited a high degree of heterogeneity. Moderator analyses detected few differences in effectiveness when comparing across diagnoses, age, the number and type of communication modes, participant’s productive repertoires, and imitation skills to intervention. A PRISMA-compliant abstract is available: https://bit.ly/30BzbLv.}, number={1}, 
  journal={Augmentative and Alternative Communication}, 
  publisher={Taylor & Francis}, 
  author={Ganz, J. B. and Pustejovsky, James E. and Reichle, Joe and Vannest, Kimberly J. and Foster, Margaret and Pierson, Lauren M. and Wattanawongwan, Sanikan and Bernal, Armando J. and Chen, Man and Haas, April N. and Liao, Ching-Yi and Sallese, Mary Rose and Skov, Rachel and Smith, S. D.}, 
  year={2023}, 
  month=jan, 
  pages={7–22} 
}

@article{Gevarter_Horan_2019, 
  title={A Behavioral Intervention Package to Increase Vocalizations of Individuals with Autism During Speech-Generating Device Intervention}, 
  volume={28}, 
  ISSN={1573-3513}, 
  DOI={10.1007/s10864-018-9300-4}, 
  abstractNote={This study examined a behavioral intervention package to promote the use of target vocalizations alongside speech-generating device (SGD) mands. Six minimally verbal children with autism spectrum disorder participated, including three with no prior SGD experience. During baseline, SGD responses resulted in access to a preferred item and there was no reinforcement for vocalizations. In Phase I of intervention, responses that included target vocalizations were differentially reinforced with a highly preferred target item and, following a delay, responses without vocalizations produced an easy distractor trial and access to a lesser-preferred item. Three participants increased vocalizations with these procedures alone. For two of these participants, closer approximations or full words were modeled and differentially reinforced during a secondary intervention phase, resulting in an increase in closer matches for one of them. The third did not require this intervention, but a second target was introduced successfully. Although the remaining three participants responded minimally to Phase I, vocalizations increased to high levels for two of these three participants after a vocal model was added and faded during Phase II. Independent SGD maintained throughout all phases of the study for all participants, and participants generalized the use of vocalization responses when the SGD was not present.}, 
  number={1}, 
  journal={Journal of Behavioral Education}, 
  author={Gevarter, Cindy and Horan, Keri}, 
  year={2019}, 
  month=mar, 
  pages={141–167}, 
  language={en} 
}

@article{Greenwood_Matyas_1990, 
  address={US}, 
  title={Problems with the application of interrupted time series analysis for brief single-subject data}, 
  volume={12}, 
  ISSN={0191-5401}, 
  abstractNote={J. M. Gottman (1981) proposed a simplified method for interrupted time-series analysis suitable for brief, autocorrelated data, and E. A. Williams and Gottman (1982) implemented it in a computer package. On the basis of a simulation study, Gottman concluded that the method controlled α in a broad range of conditions and was not affected by overfitting. A partial replication and extension of Gottman’s simulation showed that his program and test statistics are flawed. However, with modified test statistics, Gottman’s program did control α in the presence of serial dependence and overfitted models provided n was large. In brief time series (n≤20 points per phase), serial dependence and overfitting resulted in substantially inflated α. Gottman’s approach does not provide a solution for statistical analysis of brief time series. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}, 
  number={3}, 
  journal={Behavioral Assessment}, 
  publisher={Pergamon Press, Inc.}, 
  author={Greenwood, Kenneth M. and Matyas, Thomas A.}, 
  year={1990}, 
  pages={355–370} 
}

@article{Gingerich1984meta,
  title = {Meta-Analysis of Applied Time-Series Data},
  author = {Gingerich, W. J.},
  year = {1984},
  journal = {The Journal of Applied Behavioral Science},
  volume = {20},
  number = {1},
  pages = {71--79},
  publisher = {{Sage Publications}},
  issn = {0021-8863},
  doi = {10.1177/002188638402000113}
}

@article{gunning2003Psychological,
  title = {Psychological Treatment of Reported Sleep Disorder in Adults with Intellectual Disability Using a Multiple Baseline Design},
  author = {Gunning, M. J. and Espie, C. A.},
  year = {2003},
  journal = {Journal of Intellectual Disability Research},
  volume = {47},
  number = {3},
  pages = {191--202},
  issn = {1365-2788},
  doi = {10.1046/j.1365-2788.2003.00461.x},
  urldate = {2022-02-26},
  abstract = {Background The literature on sleep disturbance in adults with intellectual disability (ID) is sparse. Although prevalence rates for sleep disorders appear similar to those of non-disabled populations, previous treatment studies have largely been comprised of uncontrolled cases. Therefore, the present study adopted a single-case experimental methodology to evaluate behavioural sleep intervention. Methods A screening questionnaire was posted to 384 adults with ID and the sleep pattern of respondents with possible sleep disorders was further assessed using a structured diagnostic schedule. From the sleep-disordered subgroup, 12 participants were selected for a 4-week behavioural sleep intervention that was evaluated using randomly allocated, multiple-baseline, across-subjects designs and within-subject interrupted time series analyses (ITSAs). Results A total of 155 adults with ID (83 females and 72 males; mean age = 32 years, SD = 16.5 years), or their carers, completed the questionnaire (return rate = 40\%). The application of sleep diagnostic criteria revealed that 17\% had clinically significant difficulty getting to sleep and 11\% had difficulty remaining asleep. Nine out of the 12 participants recruited for the intervention completed all the experimental phases, thus providing three sets of three multiple-baseline designs. Visual inspection of within- and between-subject effects suggested beneficial treatment-specific effects across a range of target variables. The ITSA confirmed significant effects (P {$<$} 0.05) or trends (P {$<$} 0.10) for six out of the nine participants. Conclusions Behavioural sleep management may improve sleep pattern or sleep-related functioning in the majority of adults with ID who have significant sleep problems. The single-case methodology is helpful in addressing the heterogeneity of individual presentation, although clinical trial methodology is required to confirm these findings on a larger scale.},
  langid = {english},
  keywords = {behavioural sleep management,insomnia,learning disability,mental retardation}
}

@article{hebert2018Writing,
  title = {Writing Informational Text Using Provided Information and Text Structures: An Intervention for Upper Elementary Struggling Writers},
  shorttitle = {Writing Informational Text Using Provided Information and Text Structures},
  author = {Hebert, Michael and Bohaty, Janet J. and Nelson, J. Ron and Roehling, Julia V.},
  year = {2018},
  month = nov,
  journal = {Reading and Writing},
  volume = {31},
  number = {9},
  pages = {2165--2190},
  issn = {0922-4777, 1573-0905},
  doi = {10.1007/s11145-018-9841-x},
  urldate = {2022-02-26},
  langid = {english}
}

@article{Hedges1981distribution,
  title = {Distribution Theory for {{Glass}}'s Estimator of Effect Size and Related Estimators},
  author = {Hedges, Larry V},
  year = {1981},
  journal = {Journal of Educational Statistics},
  volume = {6},
  number = {2},
  pages = {107--128},
  publisher = {{Sage Publications}},
  issn = {1076-9986},
  keywords = {mean difdetence,measutement etro4,meta-analyzsi,research aynthe6sis,standaadized,weighting of}
}

@book{hedges1985statistical,
  title = {Statistical {{Methods}} for {{Meta-Analysis}}},
  author = {Hedges, Larry V and Olkin, Ingram},
  year = {1985},
  publisher = {{Academic Press}},
  address = {{Orlando, FL}}
}

@article{Hedges2007effect,
  title = {Effect Sizes in Cluster-Randomized Designs},
  author = {Hedges, Larry V},
  year = {2007},
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {32},
  number = {4},
  pages = {341--370},
  issn = {1076-9986},
  doi = {10.3102/1076998606298043}
}

@article{Hedges2012ABk,
  title = {A Standardized Mean Difference Effect Size for Single Case Designs},
  author = {Hedges, Larry V and Pustejovsky, James E and Shadish, William R},
  year = {2012},
  journal = {Research Synthesis Methods},
  volume = {3},
  pages = {224--239},
  issn = {17592879},
  doi = {10.1002/jrsm.1052},
  copyright = {All rights reserved},
  keywords = {autocorrelation,broader category of repeated,distinguished by the fact,effect size,hierarchical linear model,individual and,measure an outcome over,measures,single case designs,single case designs are,special type of the,such designs are a,that they assign different,time,treatments to the same}
}

@article{Hedges2012MB,
  title = {A Standardized Mean Difference Effect Size for Multiple Baseline Designs across Individuals},
  author = {Hedges, Larry V and Pustejovsky, James E and Shadish, William R},
  year = {2013},
  month = aug,
  journal = {Research Synthesis Methods},
  institution = {{Northwestern University}},
  location = {Evanston, IL},
  issn = {17592879},
  doi = {10.1002/jrsm.1086},
  copyright = {All rights reserved},
  keywords = {as a tool for,class of research methods,different conditions to the,effect size,evaluating interventions,hierarchical linear model,involving deliberate assignment of,multiple baseline designs,of one or more,outcomes over time,same individual and measurement,single-case design,single-case designs are a,these}
}

@article{Hedges_Vevea_1998, 
  address={US}, 
  title={Fixed- and random-effects models in meta-analysis}, 
  volume={3}, 
  ISSN={1939-1463}, 
  DOI={10.1037/1082-989X.3.4.486}, 
  abstractNote={There are 2 families of statistical procedures in meta-analysis: fixed- and random-effects procedures. They were developed for somewhat different inference goals: making inferences about the effect parameters in the studies that have been observed versus making inferences about the distribution of effect parameters in a population of studies from a random sample of studies. The authors evaluate the performance of confidence intervals and hypothesis tests when each type of statistical procedure is used for each type of inference and confirm that each procedure is best for making the kind of inference for which it was designed. Conditionally random-effects procedures (a hybrid type) are shown to have properties in between those of fixed- and random-effects procedures. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}, 
  number={4}, 
  journal={Psychological Methods}, 
  publisher={American Psychological Association}, 
  author={Hedges, Larry V. and Vevea, Jack L.}, 
  year={1998}, 
  pages={486–504} 
}

@article{Hembry_Bunuan_Beretvas_Ferron_VandenNoortgate_2015, 
  title={Estimation of a Nonlinear Intervention Phase Trajectory for Multiple-Baseline Design Data}, 
  volume={83}, 
  ISSN={0022-0973, 1940-0683}, 
  DOI={10.1080/00220973.2014.907231}, 
  abstractNote={A multilevel logistic model for estimating a nonlinear trajectory in a multiple-baseline design is introduced. The model is applied to data from a real multiple-baseline design study to demonstrate interpretation of relevant parameters. A simple change-in-levels ( Levels) model and a model involving a quadratic function (Quadratic) for the nonlinear intervention phase data were also estimated. In addition, a simulation study was conducted to assess Markov chain Monte Carlo estimation of the logistic model and compare its trajectory recovery with use of the Levels and Quadratic models. While most of the logistic model’s parameter values were recovered well, trajectory recovery was very reasonable using the simpler Quadratic model. Results are discussed along with recommendations for practitioners and directions for future research.}, 
  number={4}, 
  journal={The Journal of Experimental Education}, 
  author={Hembry, Ian and Bunuan, Rommel and Beretvas, S. Natasha and Ferron, John M. and Van den Noortgate, Wim}, 
  year={2015}, 
  month=oct, 
  pages={514–546}, 
  language={en} 
}

@incollection{hershberger1999meta,
  title={Meta-analysis of single-case data},
  author={Hershberger, SL and Wallace, DD and Green, SB and Marquis, JG},
  booktitle={Statistical strategies for small sample research},
  editor={Hoyle, R. H.},
  pages={107--132},
  year={1999},
  publisher={Sage Newbury Park, CA}
}

@article{higgins2009reevaluation,
  title = {A Re-Evaluation of Random-Effects Meta-Analysis},
  author = {Higgins, Julian P. T. and Thompson, Simon G. and Spiegelhalter, David J.},
  year = {2009},
  month = jan,
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume = {172},
  number = {1},
  pages = {137--159},
  issn = {09641998, 1467985X},
  doi = {10.1111/j.1467-985X.2008.00552.x},
  urldate = {2022-02-25},
  abstract = {Meta-analysis in the presence of unexplained heterogeneity is frequently undertaken by using a random-effects model, in which the effects underlying different studies are assumed to be drawn from a normal distribution. Here we discuss the justification and interpretation of such models, by addressing in turn the aims of estimation, prediction and hypothesis testing. A particular issue that we consider is the distinction between inference on the mean of the random-effects distribution and inference on the whole distribution. We suggest that random-effects meta-analyses as currently conducted often fail to provide the key results, and we investigate the extent to which distribution-free, classical and Bayesian approaches can provide satisfactory methods. We conclude that the Bayesian approach has the advantage of naturally allowing for full uncertainty, especially for prediction. However, it is not without problems, including computational intensity and sensitivity to a priori judgements. We propose a simple prediction interval for classical meta-analysis and offer extensions to standard practice of Bayesian meta-analysis, making use of an example of studies of `set shifting' ability in people with eating disorders.},
  langid = {english}
}

@incollection{horner2014Visual,
  title = {Visual Analysis of Single-Case Intervention Research: {{Conceptual}} and Methodological Issues},
  shorttitle = {Visual Analysis of Single-Case Intervention Research},
  booktitle = {Single-Case Intervention Research: {{Methodological}} and Statistical Advances},
  author = {Horner, Robert H. and Swoboda, Christopher M.},
  editor = {Kratochwill, Thomas R. and Levin, Joel R.},
  year = {2014},
  series = {School Psychology Series},
  pages = {91--125},
  publisher = {{American Psychological Association}},
  address = {{Washington, DC, US}},
  doi = {10.1037/14376-004},
  abstract = {The purposes of this chapter are to (a) review the traditional applications of and arguments for visual analysis in single-case intervention research, (b) review research on visual analysis, and (c) recommend options for supplementing visual analysis of data with recently proposed formal statistical procedures. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  isbn = {978-1-4338-1751-9},
  keywords = {Analysis,Experimental Design,Experimentation,Intervention,Statistical Data,Statistics}
}

@article{hutchinson1993Effects,
  title = {Effects of {{Cognitive Strategy Instruction}} on {{Algebra Problem Solving}} of {{Adolescents}} with {{Learning Disabilities}}},
  author = {Hutchinson, Nancy L.},
  year = {1993},
  month = feb,
  journal = {Learning Disability Quarterly},
  volume = {16},
  number = {1},
  pages = {34--63},
  publisher = {{SAGE Publications Inc}},
  issn = {0731-9487},
  doi = {10.2307/1511158},
  urldate = {2022-02-26},
  abstract = {This study investigated the effects of a two-phase cognitive strategy on algebra problem solving of adolescents with learning disabilities. The strategy was designed to enable students to represent and solve three types of word problems. The study used a modified multiple baseline with 11 replications as well as a two-group design. Conditions of the multiple-baseline design included baseline, instruction to mastery, transfer, and maintenance. Visual analysis of the single-subject data showed the strategy to be an effective intervention for this sample of students with deficits in algebra problem solving, but with criterial knowledge of basic operations and one-step problems. Statistical analyses of the two-group data showed that the instructed students had significantly higher posttest scores than the comparison group. Overall, the instructed students demonstrated improved performance on algebra word problems. Maintenance and transfer of the strategy were evident. This study has implications for teaching complex problem solving to adolescents with learning disabilities in secondary schools.},
  langid = {english}
}

@article{jamshidi2018Methodological,
  title = {Methodological Quality of Meta-Analyses of Single-Case Experimental Studies},
  author = {Jamshidi, Laleh and Heyvaert, Mieke and Declercq, Lies and {Fern{\'a}ndez-Castilla}, Bel{\'e}n and Ferron, John M. and Moeyaert, Mariola and Beretvas, S. Natasha and Onghena, Patrick and {Van den Noortgate}, Wim},
  year = {2018},
  month = aug,
  journal = {Research in Developmental Disabilities},
  volume = {79},
  pages = {97--115},
  issn = {08914222},
  doi = {10.1016/j.ridd.2017.12.016},
  urldate = {2019-01-11},
  abstract = {Background: Methodological rigor is a fundamental factor in the validity and credibility of the results of a meta-analysis. Aim: Following an increasing interest in single-case experimental design (SCED) meta-analyses, the current study investigates the methodological quality of SCED meta-analyses. Methods and procedures: We assessed the methodological quality of 178 SCED meta-analyses published between 1985 and 2015 through the modified Revised-Assessment of Multiple Systematic Reviews (R-AMSTAR) checklist. Outcomes and results: The main finding of the current review is that the methodological quality of the SCED meta-analyses has increased over time, but is still low according to the R-AMSTAR checklist. A remarkable percentage of the studies (93.80\% of the included SCED meta-analyses) did not even reach the midpoint score (22, on a scale of 0\textendash 44). The mean and median methodological quality scores were 15.57 and 16, respectively. Relatively high scores were observed for ``providing the characteristics of the included studies'' and ``doing comprehensive literature search''. The key areas of deficiency were ``reporting an assessment of the likelihood of publication bias'' and ``using the methods appropriately to combine the findings of studies''. Conclusions and implications: Although the results of the current review reveal that the methodological quality of the SCED meta-analyses has increased over time, still more efforts are needed to improve their methodological quality.},
  langid = {english}
}

@article{Joo_Ferron_2019, 
  title={Application of the Within- and Between-Series Estimators to Non-normal Multiple-Baseline Data: Maximum Likelihood and Bayesian Approaches}, 
  volume={54}, 
  ISSN={1532-7906}, 
  DOI={10.1080/00273171.2018.1564877}, 
  abstractNote={In single-case research, multiple-baseline (MB) design provides the opportunity to estimate the treatment effect based on not only within-series comparisons of treatment phase to baseline phase observations, but also time-specific between-series comparisons of observations from those that have started treatment to those that are still in the baseline. For analyzing MB studies, two types of linear mixed modeling methods have been proposed: the within- and between-series models. In principle, those models were developed based on normality assumptions, however, normality may not always be found in practical settings. Therefore, this study aimed to investigate the robustness of the within- and between-series models when data were non-normal. A Monte Carlo study was conducted with four statistical approaches. The approaches were defined by the crossing of two analytic decisions: (a) whether to use a within- or between-series estimate of effect and (b) whether to use restricted maximum likelihood or Markov chain Monte Carlo estimations. The results showed the treatment effect estimates of the four approaches had minimal bias, that within-series estimates were more precise than between-series estimates, and that confidence interval coverage was frequently acceptable, but varied across conditions and methods of estimation. Applications and implications were discussed based on the findings.}, 
  number={5}, 
  journal={Multivariate Behavioral Research}, 
  author={Joo, Seang-Hwane and Ferron, John M.}, 
  year={2019}, 
  pages={666–689}, 
  language={eng} 
}

@article{Joo_et_al_2019, 
  title={Approaches for Specifying the Level-1 Error Structure When Synthesizing Single-Case Data}, 
  volume={87}, 
  ISSN={0022-0973, 1940-0683}, 
  DOI={10.1080/00220973.2017.1409181}, 
  abstractNote={Multilevel modeling has been utilized for combining single-case experimental design (SCED) data assuming simple level-1 error structures. The purpose of this study is to compare various multilevel analysis approaches for handling potential complexity in the level-1 error structure within SCED data, including approaches assuming simple and complex error structures (heterogeneous, autocorrelation, and both) and those using ﬁt indices to select between alternative error structures. A Monte Carlo study was conducted to empirically validate the suggested multilevel modeling approaches. Results indicate that each approach leads to ﬁxed effect estimates with little to no bias and that inferences for ﬁxed effects were frequently accurate, particularly when a simple homogeneous level-1 error structure or a ﬁrstorder autoregressive structure was assumed and the inferences were based on the Kenward-Roger method. Practical implications and recommendations are discussed.}, 
  number={1}, 
  journal={The Journal of Experimental Education}, 
  author={Joo, Seang-Hwane and Ferron, John M. and Moeyaert, Mariola and Beretvas, S. Natasha and Van den Noortgate, Wim}, year={2019}, 
  month=jan, 
  pages={55-74}, 
  language={en} 
}

@article{knochel2021culturally,
  title={Culturally focused classroom staff training to increase praise for students with autism spectrum disorder in Ghana},
  author={Knochel, Ashley Elizabeth and Blair, Kwang-Sun Cho and Sofarelli, Rachel},
  journal={Journal of Positive Behavior Interventions},
  volume={23},
  number={2},
  pages={106--117},
  year={2021},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@incollection{konstantopoulos2019statistically,
  title={Statistically analyzing effect sizes: Fixed-and random-effects models},
  author={Konstantopoulos, SPYROS and Hedges, LARRY V},
  booktitle={The handbook of research synthesis and meta-analysis},
  editor = {Cooper, Harris, Hedges, Larry V., and Valentine, Jeffrey C.},
  edition = {3rd Edition},
  pages={246--280},
  year={2019},
  publisher={Russell Sage Foundation}
}

@techreport{Kratochwill2010single,
  title = {Single-{{Case Designs Technical Documentation}}, {{Version}} 1.0 ({{Pilot}})},
  author = {Kratochwill, Thomas R and Hitchcock, Jill and Horner, Robert H and Levin, Joel R and Odom, Samuel L and Rindskopf, David M and Shadish, William R},
  year = {2010},
  volume = {0},
  number = {June},
  institution = {{What Works Clearinghouse}},
  keywords = {SCD,Single-Case Design,What Works Clearinghouse}
}

@article{kratochwill2013SingleCase,
  title = {Single-{{Case Intervention Research Design Standards}}},
  author = {Kratochwill, Thomas R. and Hitchcock, John H. and Horner, Robert H. and Levin, Joel R. and Odom, Samuel L. and Rindskopf, David M. and Shadish, William R.},
  year = {2013},
  month = jan,
  journal = {Remedial and Special Education},
  volume = {34},
  number = {1},
  pages = {26--38},
  issn = {0741-9325, 1538-4756},
  doi = {10.1177/0741932512452794},
  urldate = {2018-11-16},
  abstract = {In an effort to responsibly incorporate evidence based on single-case designs (SCDs) into the What Works Clearinghouse (WWC) evidence base, the WWC assembled a panel of individuals with expertise in quantitative methods and SCD methodology to draft SCD standards. In this article, the panel provides an overview of the SCD standards recommended by the panel (henceforth referred to as the Standards) and adopted in Version 1.0 of the WWC's official pilot standards. The Standards are sequentially applied to research studies that incorporate SCDs. The design standards focus on the methodological soundness of SCDs, whereby reviewers assign the categories of Meets Standards, Meets Standards With Reservations, and Does Not Meet Standards to each study. Evidence criteria focus on the credibility of the reported evidence, whereby the outcome measures that meet the design standards (with or without reservations) are examined by reviewers trained in visual analysis and categorized as demonstrating Strong Evidence, Moderate Evidence, or No Evidence. An illustration of an actual research application of the Standards is provided. Issues that the panel did not address are presented as priorities for future consideration. Implications for research and the evidence-based practice movement in psychology and education are discussed. The WWC's Version 1.0 SCD standards are currently being piloted in systematic reviews conducted by the WWC. This document reflects the initial standards recommended by the authors as well as the underlying rationale for those standards. It should be noted that the WWC may revise the Version 1.0 standards based on the results of the pilot; future versions of the WWC standards can be found at http://www.whatworks.ed.gov.},
  langid = {english}
}

@article{kratochwill2021Singlecase,
  title = {Single-Case Design Standards: {{An}} Update and Proposed Upgrades},
  shorttitle = {Single-Case Design Standards},
  author = {Kratochwill, Thomas R. and Horner, Robert H. and Levin, Joel R. and Machalicek, Wendy and Ferron, John and Johnson, Austin},
  year = {2021},
  month = dec,
  journal = {Journal of School Psychology},
  volume = {89},
  pages = {91--105},
  issn = {00224405},
  doi = {10.1016/j.jsp.2021.10.006},
  urldate = {2022-02-26},
  langid = {english}
}

@incollection{Kratochwill2014Visual,
  title={Visual analysis of single-case intervention research: Conceptual and methodological issues},
  author={Kratochwill, Thomas R., Levin, Joel R., Horner, Robert H., and Swoboda, Christopher M.},
  booktitle={Single-case intervention research: Methodological and statistical advances},
  editor = {Kratochwill, Thomas R. and Levin, Joel R.},
  pages={91--125},
  year={2014},
  publisher={American Psychological Association},
  doi = {https://doi.org/10.1037/14376-004}
}

@article{lambert2006effects,
  title = {Effects of Response Cards on Disruptive Behavior and Academic Responding during Math Lessons by Fourth-Grade Urban Students},
  author = {Lambert, Michael Charles and Cartledge, Gwendolyn and Heward, William L and Lo, Ya-yu},
  year = {2006},
  journal = {Journal of Positive Behavior Interventions},
  volume = {8},
  number = {2},
  pages = {88},
  publisher = {{SAGE Publications}}
}

@phdthesis{lewandowski2011effects,
  title={The effects of a modified version of self-regulated strategy development and self-talk internalization strategy on writing and self-talk for elementary students with attentional difficulties},
  author={Lewandowski, Sara C},
  year={2011},
  url = {https://d.lib.msu.edu/etd/28}
}

@book{lipsey2001practical,
  title = {Practical {{Meta-Analysis}}},
  author = {Lipsey, Mark W and Wilson, David B},
  year = {2001},
  publisher = {{Sage Publications, Inc}},
  address = {{Thousand Oaks, CA}}
}

@article{Li_Luo_Baek_Thompson_Lam_2023, 
  address={US}, 
  title={Multilevel modeling in single-case studies with count and proportion data: A demonstration and evaluation}, 
  ISSN={1939-1463}, 
  DOI={10.1037/met0000607}, 
  abstractNote={The outcomes in single-case experimental designs (SCEDs) are often counts or proportions. In our study, we provided a colloquial illustration for a new class of generalized linear mixed models (GLMMs) to fit count and proportion data from SCEDs. We also addressed important aspects in the GLMM framework including overdispersion, estimation methods, statistical inferences, model selection methods by detecting overdispersion, and interpretations of regression coefficients. We then demonstrated the GLMMs with two empirical examples with count and proportion outcomes in SCEDs. In addition, we conducted simulation studies to examine the performance of GLMMs in terms of biases and coverage rates for the immediate treatment effect and treatment effect on the trend. We also examined the empirical Type I error rates of statistical tests. Finally, we provided recommendations about how to make sound statistical decisions to use GLMMs based on the findings from simulation studies. Our hope is that this article will provide SCED researchers with the basic information necessary to conduct appropriate statistical analysis of count and proportion data in their own research and outline the future agenda for methodologists to explore the full potential of GLMMs to analyze or meta-analyze SCED data. (PsycInfo Database Record (c) 2023 APA, all rights reserved)}, 
  journal={Psychological Methods}, 
  publisher={American Psychological Association}, 
  author={Li, Haoran and Luo, Wen and Baek, Eunkyeng and Thompson, Christopher G. and Lam, Kwok Hap}, 
  year={2023}, 
  pages={No Pagination Specified-No Pagination Specified} 
}

@article{maggin2011Quantitative,
  title = {A {{Quantitative Synthesis}} of {{Methodology}} in the {{Meta-Analysis}} of {{Single-Subject Research}} for {{Students}} with {{Disabilities}}: 1985\textendash 2009},
  shorttitle = {A {{Quantitative Synthesis}} of {{Methodology}} in the {{Meta-Analysis}} of {{Single-Subject Research}} for {{Students}} with {{Disabilities}}},
  author = {Maggin, Daniel M. and O'Keeffe, Breda V. and Johnson, Austin H.},
  year = {2011},
  month = apr,
  journal = {Exceptionality},
  volume = {19},
  number = {2},
  pages = {109--135},
  issn = {0936-2835, 1532-7035},
  doi = {10.1080/09362835.2011.565725},
  urldate = {2018-11-16},
  langid = {english}
}

@article{Maggin2017meta-analysis,
  title = {A Meta-Analysis of School-Based Group Contingency Interventions for Students with Challenging Behavior: {{An}} Update},
  author = {Maggin, Daniel M and Pustejovsky, James E and Johnson, A.H. Austin H},
  year = {2017},
  month = nov,
  journal = {Remedial and Special Education},
  volume = {38},
  number = {6},
  pages = {353--370},
  issn = {0741-9325},
  doi = {10.1177/0741932517716900},
  abstract = {\textcopyright{} 2017, \textcopyright{} Hammill Institute on Disabilities 2017. Group contingencies are recognized as a potent intervention for addressing challenging student behavior in the classroom, with research reviews supporting the use of this intervention platform going back more than four decades. Over this time period, the field of education has increasingly emphasized the role of research evidence for informing practice, as reflected in the increased use of systematic reviews and meta-analyses. In the current article, we continue this trend by applying recently developed between-case effect size measures and transparent visual analysis procedures to synthesize an up-to-date set of group contingency studies that used single-case designs. Results corroborated recent systematic reviews by indicating that group contingencies are generally effective\textemdash particularly for addressing challenging behavior in general education classrooms. However, our review highlights the need for more research on students with disabilities and the need to collect and report information about participants' functional level.},
  copyright = {All rights reserved},
  keywords = {behavior,evidence-based practice,management,meta-analysis,research methodology,single-subject}
}

@article{maggin2021Commentary,
  title = {Commentary on the {{{\emph{What Works Clearinghouse Standards}}}}{\emph{ and }}{{{\emph{Procedures Handbook}}}} (v. 4.1) for the {{Review}} of {{Single-Case Research}}},
  author = {Maggin, Daniel M. and Barton, Erin and Reichow, Brian and Lane, Kathleen and Shogren, Karrie A.},
  year = {2021},
  month = oct,
  journal = {Remedial and Special Education},
  pages = {074193252110513},
  issn = {0741-9325, 1538-4756},
  doi = {10.1177/07419325211051317},
  urldate = {2022-02-28},
  abstract = {The What Works Clearinghouse (WWC) provides school personnel with information on the amount and quality of evidence for educational programs, policies, and interventions. Over a decade ago, the WWC expanded their review procedures to include single-case research methods. Originally included as pilot standards, the recent updates elevated single-case research to a more prominent role for informing the development of evidence reviews and practice guides. While we applaud the removal of the pilot designation, our review of the updated procedures revealed concerns that, in our estimation, systematically favor studies based on nonexperimental criteria and inadequately address many issues and challenges with single-case research methods while overlooking other important concerns. As such, we are concerned that the current procedures will reduce the quality of information drawn from single-case research and disseminated to school personnel. In the following commentary, we describe these concerns and provide solutions-based recommendations for strengthening the standards and review process.},
  langid = {english}
}

@article{Maggin_Swaminathan_Rogers_OKeeffe_Sugai_Horner_2011, 
  title={A generalized least squares regression approach for computing effect sizes in single-case research: Application examples}, 
  volume={49}, 
  ISSN={00224405}, 
  DOI={10.1016/j.jsp.2011.03.004}, abstractNote={A new method for deriving effect sizes from single-case designs is proposed. The strategy is applicable to small-sample time-series data with autoregressive errors. The method uses Generalized Least Squares (GLS) to model the autocorrelation of the data and estimate regression parameters to produce an effect size that represents the magnitude of treatment effect from baseline to treatment phases in standard deviation units. In this paper, the method is applied to two published examples using common single case designs (i.e., withdrawal and multiple-baseline). The results from these studies are described, and the method is compared to ten desirable criteria for single-case effect sizes. Based on the results of this application, we conclude with observations about the use of GLS as a support to visual analysis, provide recommendations for future research, and describe implications for practice.}, 
  number={3}, 
  journal={Journal of School Psychology}, 
  author={Maggin, Daniel M. and Swaminathan, Hariharan and Rogers, Helen J. and O’Keeffe, Breda V. and Sugai, George and Horner, Robert H.}, 
  year={2011}, 
  month=jun, 
  pages={301–321}, 
  language={en} 
}

@article{Manolov_Solanas_Sierra_2019, 
  title={Extrapolating baseline trend in single-case data: Problems and tentative solutions}, 
  volume={51}, 
  ISSN={1554-3528}, 
  DOI={10.3758/s13428-018-1165-x}, 
  abstractNote={Single-case data often contain trends. Accordingly, to account for baseline trend, several data-analytical techniques extrapolate it into the subsequent intervention phase. Such extrapolation led to forecasts that were smaller than the minimal possible value in 40% of the studies published in 2015 that we reviewed. To avoid impossible predicted values, we propose extrapolating a damping trend, when necessary. Furthermore, we propose a criterion for determining whether extrapolation is warranted and, if so, how far out it is justified to extrapolate a baseline trend. This criterion is based on the baseline phase length and the goodness of fit of the trend line to the data. These proposals were implemented in a modified version of an analytical technique called Mean phase difference. We used both real and generated data to illustrate how unjustified extrapolations may lead to inappropriate quantifications of effect, whereas our proposals help avoid these issues. The new techniques are implemented in a user-friendly website via the Shiny application, offering both graphical and numerical information. Finally, we point to an alternative not requiring either trend line fitting or extrapolation.}, 
  number={6}, 
  journal={Behavior Research Methods}, 
  author={Manolov, Rumen and Solanas, Antonio and Sierra, Vicenta}, 
  year={2019}, 
  month=dec, 
  pages={2847–2869}, 
  language={en} 
}

@article{Manolov_Moeyaert_Fingerhut_2022, 
  title={A Priori Justification for Effect Measures in Single-Case Experimental Designs}, 
  volume={45}, 
  ISSN={2520-8977}, 
  DOI={10.1007/s40614-021-00282-2}, 
  abstractNote={Due to the complex nature of single-case experimental design data, numerous effect measures are available to quantify and evaluate the effectiveness of an intervention. An inappropriate choice of the effect measure can result in a misrepresentation of the intervention effectiveness and this can have far-reaching implications for theory, practice, and policymaking. As guidelines for reporting appropriate justification for selecting an effect measure are missing, the first aim is to identify the relevant dimensions for effect measure selection and justification prior to data gathering. The second aim is to use these dimensions to construct a user-friendly flowchart or decision tree guiding applied researchers in this process. The use of the flowchart is illustrated in the context of a preregistered protocol. This is the first study that attempts to propose reporting guidelines to justify the effect measure choice, before collecting the data, to avoid selective reporting of the largest quantifications of an effect. A proper justification, less prone to confirmation bias, and transparent and explicit reporting can enhance the credibility of the single-case design study findings.}, 
  number={1}, 
  journal={Perspectives on Behavior Science}, 
  author={Manolov, Rumen and Moeyaert, Mariola and Fingerhut, Joelle E.}, 
  year={2022}, 
  month=mar, 
  pages={153–186}, 
  language={en} 
}

@article{Mason_Davis_Ayres_Davis_Mason_2016, 
  address={Germany}, 
  title={Video self-modeling for individuals with disabilities: A best-evidence, single case meta-analysis},
  volume={28}, 
  ISSN={1573-3580}, 
  DOI={10.1007/s10882-016-9484-2}, 
  abstractNote={Video-based modeling capitalizes on technology to increase the efficiency of modeling interventions for individuals with disabilities. Video self-modeling is a specific form of video-based modeling that utilizes the learner as the model to provide an opportunity for the learner to view him or herself as competent. This meta-analysis investigated the efficacy of video-self modeling using articles that met quality criteria with a focus on potential moderators of effect including participant characteristics, targeted outcomes, and implementation components. Results indicated strong effects for preschool and elementary aged participants with autism spectrum disorders, particularly when social-communicative and behavioral outcomes were addressed. In regards to implementation variables, significantly larger effect sizes were obtained when positive self-review was implemented as compared to feed forward. Additionally, video self-modeling implemented alone yielded stronger effects than video self-modeling implemented with programmed reinforcement or as part of a package. Gaps in the evidence are identified including limited evidence for the use of video self-modeling with older participants and participants with disabilities other than autism. Implications and suggestions for future research are discussed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}, 
  number={4}, 
  journal={Journal of Developmental and Physical Disabilities}, 
  publisher={Springer}, 
  author={Mason, Rose A. and Davis, Heather S. and Ayres, Kevin M. and Davis, John L. and Mason, Benjamin A.}, 
  year={2016}, 
  pages={623–642} 
}

@incollection{matyas1997Serial,
  title = {Serial dependency in single case time series},
  booktitle = {Design and analysis of single-case research},
  author = {Matyas, Thomas A and Greenwood, Kenneth M},
  year = {1997},
  pages = {215--243},
  publisher = {Erlbaum},
  copyright = {All rights reserved},
  langid = {english}
}

@article{mancil2006Functional,
  title = {Functional {{Communication Training}} in the {{Natural Environment}}: {{A Pilot Investigation}} with a {{Young Child}} with {{Autism Spectrum Disorder}}},
  shorttitle = {Functional {{Communication Training}} in the {{Natural Environment}}},
  author = {Mancil, G. Richmond and Conroy, Maureen A. and Nakao, Taketo and Alter, Peter J.},
  year = {2006},
  journal = {Education and Treatment of Children},
  volume = {29},
  number = {4},
  eprint = {42900556},
  eprinttype = {jstor},
  pages = {615--633},
  publisher = {{West Virginia University Press}},
  issn = {0748-8491},
  urldate = {2022-02-28},
  abstract = {A child with Autism Spectrum Disorder (ASD) and a history of aberrant behaviors participated in this study with his mother. The primary purpose of the current study was to determine the effectiveness and efficiency of FCT on decreasing problem behaviors, increasing communication mands, and increasing spontaneous communication with a child with ASD in his home environment. Further, the number and diversity of spontaneous verbalizations were anecdotally recorded. The results of the multiple baseline study across mands demonstrated a dramatic decrease in aberrant behavior while increasing the number of mands and the latency to respond. In addition, the participant's number and diversity of words dramatically increased.}
}

@article{mason2016Video,
  title = {Video {{Self-Modeling}} for {{Individuals}} with {{Disabilities}}: {{A Best-Evidence}}, {{Single Case Meta-Analysis}}},
  shorttitle = {Video {{Self-Modeling}} for {{Individuals}} with {{Disabilities}}},
  author = {Mason, Rose A. and Davis, Heather S. and Ayres, Kevin M. and Davis, John L. and Mason, Benjamin A.},
  year = {2016},
  month = aug,
  journal = {Journal of Developmental and Physical Disabilities},
  volume = {28},
  number = {4},
  pages = {623--642},
  issn = {1573-3580},
  doi = {10.1007/s10882-016-9484-2},
  urldate = {2022-02-28},
  abstract = {Video-based modeling capitalizes on technology to increase the efficiency of modeling interventions for individuals with disabilities. Video self-modeling is a specific form of video-based modeling that utilizes the learner as the model to provide an opportunity for the learner to view him or herself as competent. This meta-analysis investigated the efficacy of video-self modeling using articles that met quality criteria with a focus on potential moderators of effect including participant characteristics, targeted outcomes, and implementation components. Results indicated strong effects for preschool and elementary aged participants with autism spectrum disorders, particularly when social-communicative and behavioral outcomes were addressed. In regards to implementation variables, significantly larger effect sizes were obtained when positive self-review was implemented as compared to feed forward. Additionally, video self-modeling implemented alone yielded stronger effects than video self-modeling implemented with programmed reinforcement or as part of a package. Gaps in the evidence are identified including limited evidence for the use of video self-modeling with older participants and participants with disabilities other than autism. Implications and suggestions for future research are discussed.},
  langid = {english}
}

@article{Moeyaert_Ferron_Beretvas_VandenNoortgate_2014, 
  title={From a single-level analysis to a multilevel analysis of single-case experimental designs}, volume={52}, 
  ISSN={00224405}, 
  DOI={10.1016/j.jsp.2013.11.003}, 
  abstractNote={Multilevel modeling provides one approach to synthesizing single-case experimental design data. In this study, we present the multilevel model (the two-level and the three-level models) for summarizing single-case results over cases, over studies, or both. In addition to the basic multilevel models, we elaborate on several plausible alternative models. We apply the proposed models to real datasets and investigate to what extent the estimated treatment effect is dependent on the modeling specifications and the underlying assumptions. By considering a range of plausible models and assumptions, researchers can determine the degree to which the effect estimates and conclusions are sensitive to the specific assumptions made. If the same conclusions are reached across a range of plausible assumptions, confidence in the conclusions can be enhanced. We advise researchers not to focus on one model but conduct multiple plausible multilevel analyses and investigate whether the results depend on the modeling options.}, 
  number={2}, 
  journal={Journal of School Psychology}, 
  author={Moeyaert, Mariola and Ferron, John M. and Beretvas, S. Natasha and Van den Noortgate, Wim}, 
  year={2014}, 
  month=apr, 
  pages={191–211}, 
  language={en} 
}
 
@article{Moeyaert_Ugille_Ferron_Beretvas_VandenNoortgate_2014, 
  title={Three-Level Analysis of Single-Case Experimental Data: Empirical Validation}, 
  volume={82}, 
  ISSN={0022-0973, 1940-0683}, 
  DOI={10.1080/00220973.2012.745470}, 
  number={1}, 
  journal={The Journal of Experimental Education}, 
  author={Moeyaert, Mariola and Ugille, Maaike and Ferron, John M. and Beretvas, S. Natasha and den Noortgate, Wim Van}, 
  year={2014}, 
  month=jan, 
  pages={1–21}, 
  language={en} 
}
 
@article{Moeyaert_Ugille_Ferron_Beretvas_VandenNoortgate_2013, 
  title={The Three-Level Synthesis of Standardized Single-Subject Experimental Data: A Monte Carlo Simulation Study}, 
  volume={48}, 
  ISSN={0027-3171, 1532-7906}, 
  DOI={10.1080/00273171.2013.816621}, 
  number={5}, 
  journal={Multivariate Behavioral Research}, 
  author={Moeyaert, Mariola and Ugille, Maaike and Ferron, John M. and Beretvas, S. Natasha and Van den Noortgate, Wim}, 
  year={2013}, 
  month=sep, 
  pages={719–748}, 
  language={en} 
}

@article{Moeyaert_Ugille_Ferron_Beretvas_VanDenNoortgate_2016, 
  title={The Misspecification of the Covariance Structures in Multilevel Models for Single-Case Data: A Monte Carlo Simulation Study}, 
  volume={84}, 
  ISSN={0022-0973, 1940-0683}, 
  DOI={10.1080/00220973.2015.1065216}, number={3}, 
  journal={The Journal of Experimental Education}, 
  author={Moeyaert, Mariola and Ugille, Maaike and Ferron, John M. and Beretvas, S. Natasha and Van Den Noortgate, Wim}, 
  year={2016}, 
  month=jul, 
  pages={473–509}, 
  language={en} 
}

@article{Moeyaert_Yang_Xue_2023, 
  title={Individual Participant Data Meta-Analysis Including Moderators: Empirical Validation}, 
  ISSN={0022-0973, 1940-0683}, 
  DOI={10.1080/00220973.2023.2208062}, 
  journal={The Journal of Experimental Education}, 
  author={Moeyaert, Mariola and Yang, Panpan and Xue, Yukang}, 
  year={2023}, 
  month=may, 
  pages={1–18}, 
  language={en} 
}

@article{montgomery2004relative,
  title = {The Relative Efficacy of Two Brief Treatments for Sleep Problems in Young Learning Disabled (Mentally Retarded) Children: A Randomised Controlled Trial},
  shorttitle = {The Relative Efficacy of Two Brief Treatments for Sleep Problems in Young Learning Disabled (Mentally Retarded) Children},
  author = {Montgomery, P},
  year = {2004},
  month = feb,
  journal = {Archives of Disease in Childhood},
  volume = {89},
  number = {2},
  pages = {125--130},
  issn = {0003-9888, 1468-2044},
  doi = {10.1136/adc.2002.017202},
  urldate = {2022-02-28},
  langid = {english}
}

@article{NatesanBatley_Hedges_2021, 
  title={Accurate models vs. accurate estimates: A simulation study of Bayesian single-case experimental designs}, 
  ISSN={1554-3528}, 
  url={https://doi.org/10.3758/s13428-020-01522-0}, 
  DOI={10.3758/s13428-020-01522-0}, 
  abstractNote={Although statistical practices to evaluate intervention effects in single-case experimental design (SCEDs) have gained prominence in recent times, models are yet to incorporate and investigate all their analytic complexities. Most of these statistical models incorporate slopes and autocorrelations, both of which contribute to trend in the data. The question that arises is whether in SCED data that show trend, there is indeterminacy between estimating slope and autocorrelation, because both contribute to trend, and the data have a limited number of observations. Using Monte Carlo simulation, we compared the performance of four Bayesian change-point models: (a) intercepts only (IO), (b) slopes but no autocorrelations (SI), (c) autocorrelations but no slopes (NS), and (d) both autocorrelations and slopes (SA). Weakly informative priors were used to remain agnostic about the parameters. Coverage rates showed that for the SA model, either the slope effect size or the autocorrelation credible interval almost always erroneously contained 0, and the type II errors were prohibitively large. Considering the 0-coverage and coverage rates of slope effect size, intercept effect size, mean relative bias, and second-phase intercept relative bias, the SI model outperformed all other models. Therefore, it is recommended that researchers favor the SI model over the other three models. Research studies that develop slope effect sizes for SCEDs should consider the performance of the statistic by taking into account coverage and 0-coverage rates. These helped uncover patterns that were not realized in other simulation studies. We underline the need for investigating the use of informative priors in SCEDs.}, 
  journal={Behavior Research Methods}, 
  author={Natesan Batley, Prathiba and Hedges, Larry Vernon}, 
  year={2021}, 
  month=feb, 
  language={en} 
}

@article{odom2018Betweencase,
  title = {Between-Case Standardized Effect Size Analysis of Single Case Designs: {{Examination}} of the Two Methods},
  shorttitle = {Between-Case Standardized Effect Size Analysis of Single Case Designs},
  author = {Odom, Samuel L. and Barton, Erin E. and Reichow, Brian and Swaminathan, Hariharan and Pustejovsky, James E.},
  year = {2018},
  month = aug,
  journal = {Research in Developmental Disabilities},
  volume = {79},
  pages = {88--96},
  issn = {08914222},
  doi = {10.1016/j.ridd.2018.05.009},
  urldate = {2022-02-28},
  langid = {english}
}

@article{ota2002Task,
  title = {Task Engagement and Mathematics Performance in Children with Attention-Deficit Hyperactivity Disorder: {{Effects}} of Supplemental Computer Instruction},
  shorttitle = {Task Engagement and Mathematics Performance in Children with Attention-Deficit Hyperactivity Disorder},
  author = {Ota, Kenji R. and DuPaul, George J.},
  year = {2002},
  journal = {School Psychology Quarterly},
  volume = {17},
  number = {3},
  pages = {242--257},
  publisher = {{Guilford Publications}},
  address = {{US}},
  issn = {1939-1560},
  doi = {10.1521/scpq.17.3.242.20881},
  abstract = {Examined the effects of using software with a game format (as a supplement to teacher instruction) to improve math performance of 3 male 4th- to 6th-grade students with attention-deficit hyperactivity disorder. Following baseline (observation under normal classroom conditions), the math software was introduced sequentially using a multiple baseline design across participants. Observational data were collected during the baseline and experimental conditions along with a set of curriculum-based math probes, which were used throughout the study. The hypothesis that math software with a game format would improve the academic performance and increase attention of all participants was partially supported. Implications for practice and further research are discussed. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {Attention Deficit Disorder with Hyperactivity,Computer Games,Elementary School Students,Mathematics Achievement,Time On Task}
}

@article{Owens_Ferron_2012, 
  title={Synthesizing single-case studies: A Monte Carlo examination of a three-level meta-analytic model}, 
  volume={44}, 
  ISSN={1554-3528}, 
  DOI={10.3758/s13428-011-0180-y}, 
  number={3}, 
  journal={Behavior Research Methods}, 
  author={Owens, Corina M. and Ferron, John M.}, 
  year={2012}, 
  month=sep, 
  pages={795–805}, 
  language={en} 
}

@article{parker2012Application,
  title = {An {{Application}} of {{Brief Experimental Analysis}} with {{Early Writing}}},
  author = {Parker, David C. and Dickey, Bradley N. and Burns, Matthew K. and McMaster, Kristen L.},
  year = {2012},
  month = dec,
  journal = {Journal of Behavioral Education},
  volume = {21},
  number = {4},
  pages = {329--349},
  issn = {1573-3513},
  doi = {10.1007/s10864-012-9151-3},
  urldate = {2022-02-28},
  abstract = {Students' poor performance on national assessments of writing suggests that educators need effective approaches to assess and intervene with writing problems. Brief experimental analysis (BEA) has supporting evidence for identifying interventions in reading, but little research has investigated BEA with writing. Early writing is an especially important period for students, and the current study sought to extend BEA research in early writing. Results showed that BEAs for 3 first grade students identified promising writing interventions, and extended analyses showed improved performance for each student following implementation of the interventions. Implications for future research in direct assessment and intervention are discussed.},
  langid = {english}
}

@article{Parker_Vannest_2009, 
  title={An Improved Effect Size for Single-Case Research: Nonoverlap of All Pairs}, 
  volume={40}, 
  ISSN={00057894}, 
  DOI={10.1016/j.beth.2008.10.006}, 
  number={4}, 
  journal={Behavior Therapy}, author={Parker, Richard I. and Vannest, Kimberly}, 
  year={2009}, 
  month=dec, 
  pages={357–367}, 
  language={en} 
}

@article{Parker_Vannest_Davis_Sauber_2011, 
  title={Combining Nonoverlap and Trend for Single-Case Research: Tau-U}, 
  volume={42}, 
  ISSN={0005-7894}, 
  DOI={10.1016/j.beth.2010.08.006}, 
  abstractNote={A new index for analysis of single-case research data was proposed, Tau-U, which combines nonoverlap between phases with trend from within the intervention phase. In addition, it provides the option of controlling undesirable Phase A trend. The derivation of Tau-U from Kendall’s Rank Correlation and the Mann-Whitney U test between groups is demonstrated. The equivalence of trend and nonoverlap is also shown, with supportive citations from field leaders. Tau-U calculations are demonstrated for simple AB and ABA designs. Tau-U is then field tested on a sample of 382 published data series. Controlling undesirable Phase A trend caused only a modest change from nonoverlap. The inclusion of Phase B trend yielded more modest results than simple nonoverlap. The Tau-U score distribution did not show the artificial ceiling shown by all other nonoverlap techniques. It performed reasonably well with autocorrelated data. Tau-U shows promise for single-case applications, but further study is desirable.}, 
  number={2}, 
  journal={Behavior Therapy}, author={Parker, Richard I. and Vannest, Kimberly J. and Davis, John L. and Sauber, Stephanie B.}, 
  year={2011}, 
  month=jun, 
  pages={284–299}, 
  language={en} 
}

@incollection{parker2014Nonoverlap,
  title={Non-overlap analysis for single-case research},
  author={Parker, R. L., Vannest, K. J., and Davis, J. L.},
  booktitle={Single-case intervention research: Methodological and statistical advances},
  editor = {Kratochwill, Thomas R. and Levin, Joel R.},
  pages={127--151},
  year={2014},
  publisher={American Psychological Association},
  doi = {https://doi.org/10.1037/14376-005}
}

@article{Patrona2022, 
  title={Effects of Explicit Vocabulary Interventions for Preschoolers: An Exploratory Application of the Percent of Goal Obtained Effect Size Metric}, 
  volume={65}, 
  ISSN={10924388}, 
  DOI={10.1044/2022_JSLHR-22-00217}, 
  abstractNote={Purpose: Systematic reviews of literature are routinely conducted to identify practices that are effective in addressing educational and clinical problems. One complication, however, is how best to combine data from both group experimental design (GED) studies and single-case experimental design (SCED) studies. Percent of Goal Obtained (PoGO) has been developed as a metric to express the size of the effect relative to the distance to a goal, which could have broad applicability. This study sought to augment this descriptive index with estimates of standard errors, which are needed to use PoGO as an effect size metric in meta-analyses of SCED and GED studies. This study investigated the application of PoGO and standard errors to both SCED and GED studies examining a common intervention approach used with a single population. Method: Sixteen articles investigating explicit vocabulary instruction applied to pre-K and kindergarten students were identified. PoGO and standard errors were calculated for variations of explicit vocabulary interventions. Evaluated interventions included six studies using exclusively an SCED, nine studies using a GED, and one that used both. Results: PoGO was calculated for each treatment condition when applicable (i.e., alternating treatments designs). Standard errors and confidence interval limits also were calculated. PoGO effect size values ranged from 14.4% to 93.6%. PoGO for single-case experiments was 49.2% with a standard error of 7.26, and for group experiments, it was 30.8% with a standard error of 3.71. Conclusion: Despite variation in the percentage of goal obtained across studies, the high degree of overlap in PoGO and standard errors between single-case and group experiments provides an indication that systematic reviews can apply this effect size metric to combine information obtained across experimental designs.}, 
  number={12}, 
  journal={Journal of Speech, Language & Hearing Research}, publisher={American Speech-Language-Hearing Association}, 
  author={Patrona, Ella and Ferron, John and Olszewski, Arnold and Kelley, Elizabeth and Goldstein, Howard},
  year={2022}, 
  month=dec, 
  pages={4821–4836} 
}

@article{peltier2020Effects,
  title = {Effects of {{Schema-Based Instruction}} on {{Immediate}}, {{Generalized}}, and {{Combined Structured Word Problems}}},
  author = {Peltier, Corey and Sinclair, Tracy E. and Pulos, Joshua M. and Suk, Andrea},
  year = {2020},
  month = aug,
  journal = {The Journal of Special Education},
  volume = {54},
  number = {2},
  pages = {101--112},
  publisher = {{SAGE Publications Inc}},
  issn = {0022-4669},
  doi = {10.1177/0022466919883397},
  urldate = {2022-02-28},
  abstract = {Instruction targeting the underlying math problem structure is identified as an evidence-based practice for students with a specific learning disability (SLD). Furthermore, schema-based instruction is identified as a potentially evidence-based practice for students with a SLD. This study extended prior work by (a) using a teacher as the implementer, (b) evaluating the efficacy of an adaptable intervention, and (c) evaluating student performance on generalized and combined schema structure problems. The participants included 12 fourth- and fifth-grade students with a disability and receiving supplemental mathematics instruction in a resource room setting. The intervention package consisted of a problem-solving mnemonic and schema-based instruction for mathematics. A multiple-probe design across participant groups was used to establish a functional relation. Students improved performance on word problems representing simple, generalized, and combined schema structures. The aggregated Tau-U effect size (ES) for this study was 95\% (CI90 [83\%, 100\%]) and the aggregated between-case standardized mean difference (BC-SMD) was 3.05 (CI95 [2.54, 3.60]).},
  langid = {english},
  keywords = {mathematics,problem solving,schema-based instruction}
}

@phdthesis{Petit-Bois_2014, 
  title={A Monte Carlo Study: The Consequences of the Misspecification of the Level-1 Error Structure}, url={https://scholarcommons.usf.edu/etd/5341}, 
  author={Petit-Bois, Merlande}, 
  year={2014}, 
  month=jan 
}

@article{Petit-Bois_Baek_VandenNoortgate_Beretvas_Ferron_2016, 
  title={The consequences of modeling autocorrelation when synthesizing single-case studies using a three-level model}, 
  volume={48}, 
  ISSN={1554-3528}, 
  DOI={10.3758/s13428-015-0612-1}, 
  abstractNote={Results from single-case studies are being synthesized using three-level models in which repeated observations are nested in participants, which in turn are nested in studies. We examined the performance of these models under conditions in which the errors associated with the repeated observations (the Level-1 errors) were assumed to be first-order autoregressive. Monte Carlo methods were used to examine conditions in which the first-order autoregressive assumption was accurate, conditions in which it represented an overspecification because the errors were actually independent, and conditions in which it represented a misspecification because the errors were generated on the basis of a moving-average model. Conditions also varied the series lengths, the numbers of participants per study, the numbers of studies per meta-analysis, the variances between the participants within studies, and the variances between studies. Fixed effects (e.g., the average treatment effect for the intervention and the average treatment effect for the trend) tended to be unbiased, and confidence intervals for the fixed effects tended to be accurate even when the error covariance model was overspecified or misspecified. The variance components, particularly at Levels 2 and 3, showed substantial bias.}, number={2}, journal={Behavior Research Methods}, author={Petit-Bois, Merlande and Baek, Eun Kyeng and Van den Noortgate, Wim and Beretvas, S. Natasha and Ferron, John M.}, 
  year={2016}, 
  month=jun, 
  pages={803–812}, 
  language={en} 
}

@article{Pustejovsky2014design,
  title = {Design-Comparable Effect Sizes in Multiple Baseline Designs: {{A}} General Modeling Framework},
  author = {Pustejovsky, James E and Hedges, Larry V and Shadish, William R},
  year = {2014},
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {39},
  number = {5},
  pages = {368--393},
  issn = {1076-9986},
  doi = {10.3102/1076998614547577},
  abstract = {\textcopyright{} 2014 AERA. In single-case research, the multiple baseline design is a widely used approach for evaluating the effects of interventions on individuals. Multiple baseline designs involve repeated measurement of outcomes over time and the controlled introduction of a treatment at different times for different individuals. This article outlines a general framework for defining effect sizes in multiple baseline designs that are directly comparable to the standardized mean difference from a between-subjects randomized experiment. The target, design-comparable effect size parameter can be estimated using restricted maximum likelihood together with a small sample correction analogous to Hedges's g. The approach is demonstrated using hierarchical linear models that include baseline time trends and treatment-by-time interactions. A simulation compares the performance of the proposed estimator to that of an alternative, and an application illustrates the model-fitting process.},
  copyright = {All rights reserved},
  keywords = {Effect size,Hierarchical linear model,Single-case research}
}

@incollection{pustejovsky2017Research,
  title = {Research {{Synthesis}} and {{Meta-Analysis}} of {{Single-Case Designs}}},
  booktitle = {Handbook of {{Special Education}}},
  author = {Pustejovsky, James E and Ferron, John},
  year = {2017},
  edition = {2nd Edition},
  pages = {63},
  publisher = {{Routledge}},
  address = {{New York, NY}},
  copyright = {All rights reserved},
  langid = {english}
}

@misc{pustejovsky2023SingleCaseES,
  title = {{{SingleCaseES}}: {{A Calculator}} for {{Single-Case Effect Sizes}}},
  author = {Pustejovsky, James E. and Chen, Man and and Swan, Daniel M.},
  year = {2023},
  copyright = {All rights reserved}
}

@article{pustejovsky2018Using,
  title = {Using Response Ratios for Meta-Analyzing Single-Case Designs with Behavioral Outcomes},
  author = {Pustejovsky, James E.},
  year = {2018},
  month = jun,
  journal = {Journal of School Psychology},
  volume = {68},
  pages = {99--112},
  issn = {00224405},
  doi = {10.1016/j.jsp.2018.02.003},
  urldate = {2022-02-28},
  langid = {english}
}

@misc{pustejovsky2021scdhlm,
  title = {Scdhlm: {{A}} Web-Based Calculator for between-Case Standardized Mean Differences},
  author = {Pustejovsky, James E. and Chen, Man and Hamilton, Bethany},
  year = {2021},
  copyright = {All rights reserved}
}

@misc{pustejovsky2023singlecasees,
  title = {Single-case effect size calculator},
  author = {Pustejovsky, James E. and Chen, Man and Swan, Daniel M.},
  year = {2023},
  copyright = {All rights reserved}
}

@article{Pustejovsky2015Measurement, 
  title={Measurement-comparable effect sizes for single-case studies of free-operant behavior.}, 
  volume={20}, 
  ISSN={1939-1463, 1082-989X}, 
  DOI={10.1037/met0000019}, 
  abstractNote={Single-case research comprises a set of designs and methods for evaluating the effects of interventions, practices, or programs on individual cases, through comparison of outcomes measured at different points in time. Although there has long been interest in meta-analytic techniques for synthesizing single-case research, there has been little scrutiny of whether proposed effect sizes remain on a directly comparable metric when outcomes are measured using different operational procedures. Much of single-case research focuses on behavioral outcomes in free-operant contexts, which may be measured using a variety of different direct observation procedures. This article describes a suite of effect sizes for quantifying changes in free-operant behavior, motivated by an alternating renewal process model that allows measurement comparability to be established in precise terms. These effect size metrics have the advantage of comporting with how direct observation data are actually collected and summarized. Effect size estimators are proposed that are applicable when the behavior being measured remains stable within a given treatment condition. The methods are illustrated by 2 examples, including a re-analysis of a systematic review of the effects of choice-making opportunities on problem behavior.}, 
  number={3}, 
  journal={Psychological Methods}, author={Pustejovsky, James E.}, 
  year={2015}, 
  pages={342–359}, 
  language={en} 
}

@article{Pustejovsky2019Procedural, 
  address={US}, 
  title={Procedural sensitivities of effect sizes for single-case designs with directly observed behavioral outcome measures}, 
  volume={24}, 
  ISSN={1939-1463(Electronic),1082-989X(Print)}, 
  DOI={10.1037/met0000179}, 
  abstractNote={A wide variety of effect size indices have been proposed for quantifying the magnitude of treatment effects in single-case designs. Commonly used measures include parametric indices such as the standardized mean difference as well as nonoverlap measures such as the percentage of nonoverlapping data, improvement rate difference, and nonoverlap of all pairs. Currently, little is known about the properties of these indices when applied to behavioral data collected by systematic direct observation, even though systematic direct observation is the most common method for outcome measurement in single-case research. This study uses Monte Carlo simulation to investigate the properties of several widely used single-case effect size measures when applied to systematic direct observation data. Results indicate that the magnitude of the nonoverlap measures and of the standardized mean difference can be strongly influenced by procedural details of the study’s design, which is a significant limitation to using these indices as effect sizes for meta-analysis of single-case designs. A less widely used parametric index, the log response ratio, has the advantage of being insensitive to sample size and observation session length, although its magnitude is influenced by the use of partial interval recording. (PsycINFO Database Record (c) 2019 APA, all rights reserved)}, 
  number={2}, 
  journal={Psychological Methods}, publisher={American Psychological Association}, 
  author={Pustejovsky, James E.}, 
  year={2019}, 
  pages={217–235} 
}

@article{PustejovskyTipton2021, 
  title={Meta-analysis with Robust Variance Estimation: Expanding the Range of Working Models}, 
  ISSN={1573-6695}, 
  url={https://doi.org/10.1007/s11121-021-01246-3}, 
  DOI={10.1007/s11121-021-01246-3}, 
  abstractNote={In prevention science and related fields, large meta-analyses are common, and these analyses often involve dependent effect size estimates. Robust variance estimation (RVE) methods provide a way to include all dependent effect sizes in a single meta-regression model, even when the exact form of the dependence is unknown. RVE uses a working model of the dependence structure, but the two currently available working models are limited to each describing a single type of dependence. Drawing on flexible tools from multilevel and multivariate meta-analysis, this paper describes an expanded range of working models, along with accompanying estimation methods, which offer potential benefits in terms of better capturing the types of data structures that occur in practice and, under some circumstances, improving the efficiency of meta-regression estimates. We describe how the methods can be implemented using existing software (the “metafor” and “clubSandwich” packages for R), illustrate the proposed approach in a meta-analysis of randomized trials on the effects of brief alcohol interventions for adolescents and young adults, and report findings from a simulation study evaluating the performance of the new methods.}, 
  journal={Prevention Science}, 
  author={Pustejovsky, James E. and Tipton, Elizabeth}, 
  year={2021}, 
  month=may, 
  language={en} 
}

@Manual{RTeam,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2023},
    url = {https://www.R-project.org/},
  }

@article{reichow2018Development,
  title = {Development and Applications of the Single-Case Design Risk of Bias Tool for Evaluating Single-Case Design Research Study Reports},
  author = {Reichow, Brian and Barton, Erin E. and Maggin, Daniel M.},
  year = {2018},
  month = aug,
  journal = {Research in Developmental Disabilities},
  volume = {79},
  pages = {53--64},
  issn = {08914222},
  doi = {10.1016/j.ridd.2018.05.008},
  urldate = {2019-01-11},
  abstract = {Systematic reviews and meta-analyses can be a useful method for synthesizing evidence across multiple studies to draw conclusions about a research base. An important aspect of rigorous systematic reviews is an assessment of the study methods and potential biases impacting results or interpretations and conclusions of the primary studies. Single-case design (SCD) research has been a primary mechanism for identifying evidence-based practices across disciplines, but primarily in behavioral fields and special education. While the Cochrane Risk of Bias tool has been adapted for use in reviews of non-randomized studies, there is currently no guidance for evaluating SCD research. Hence, we developed a single case design risk of bias tool (SCD RoB) based on current conceptualizations of biases that might affect the validity of claims from single-case design research. We used the Cochrane risk of bias criteria and contemporary single-case design quality indicators and design standards to guide development. We describe the SCD RoB tool and two early applications of its use to demonstrate its application and provide initial validation. We also provide an overview of future areas of research using the SCD RoB tool in an effort to advance the science of single-case design research methods.},
  langid = {english}
}

@article{Rice_Higgins_Lumley_2018, 
  title={A Re-Evaluation of Fixed Effect(s) Meta-Analysis}, 
  volume={181}, 
  ISSN={0964-1998}, 
  DOI={10.1111/rssa.12275}, 
  abstractNote={Meta-analysis is a common tool for synthesizing results of multiple studies. Among methods for performing meta-analysis, the approach known as ‘fixed effects’ or ‘inverse variance weighting’ is popular and widely used. A common interpretation of this method is that it assumes that the underlying effects in contributing studies are identical, and for this reason it is sometimes dismissed by practitioners. However, other interpretations of fixed effects analyses do not make this assumption, yet appear to be little known in the literature. We review these alternative interpretations, describing both their strengths and their limitations. We also describe how heterogeneity of the underlying effects can be addressed, with the same minimal assumptions, through either testing or meta-regression. Recommendations for the practice of meta-analysis are given; it is hoped that these will foster more direct connection of the questions that meta-analysts wish to answer with the statistical methods they choose.}, 
  number={1}, 
  journal={Journal of the Royal Statistical Society Series A: Statistics in Society}, 
  author={Rice, Kenneth and Higgins, Julian P. T. and Lumley, Thomas}, 
  year={2018}, 
  month=jan, 
  pages={205–227} 
}

@article{Rindskopf_2014, 
  title={Nonlinear Bayesian analysis for single case designs}, 
  volume={52}, 
  ISSN={1873-3506}, 
  DOI={10.1016/j.jsp.2013.12.003}, 
  abstractNote={Several authors have suggested the use of multilevel models for the analysis of data from single case designs. Multilevel models are a logical approach to analyzing such data, and deal well with the possible different time points and treatment phases for different subjects. However, they are limited in several ways that are addressed by Bayesian methods. For small samples Bayesian methods fully take into account uncertainty in random effects when estimating fixed effects; the computational methods now in use can fit complex models that represent accurately the behavior being modeled; groups of parameters can be more accurately estimated with shrinkage methods; prior information can be included; and interpretation is more straightforward. The computer programs for Bayesian analysis allow many (nonstandard) nonlinear models to be fit; an example using floor and ceiling effects is discussed here.}, 
  number={2}, 
  journal={Journal of School Psychology}, 
  author={Rindskopf, David}, 
  year={2014}, 
  month=apr, 
  pages={179–189}, 
  language={eng} 
}

@article{rodgers2021Effects,
  title = {Effects of a {{Text-Writing Fluency Intervention}} for {{Postsecondary Students}} with {{Intellectual}} and {{Developmental Disabilities}}},
  author = {Rodgers, Derek B. and Datchuk, S. M. and Rila, A. L.},
  year = {2021},
  month = aug,
  journal = {Exceptionality},
  volume = {29},
  number = {4},
  pages = {310--325},
  issn = {0936-2835, 1532-7035},
  doi = {10.1080/09362835.2020.1850451},
  urldate = {2022-02-28},
  langid = {english}
}

@misc{rohatgi2015Webplotdigitizer,
  title = {Webplotdigitizer},
  shorttitle = {Webplotdigitizer},
  author = {Rohatgi, Ankit},
  year = {2015},
  month = oct,
  doi = {10.5281/zenodo.32375},
  urldate = {2019-01-28},
  abstract = {Release Notes: New Features: Initial implementation of grid removal. Log scale support for polar diagrams. Bug fixes and code clean-up: Eliminate dependence on numeric.js for a lighter and more predictable code base. Improve rendering of data points. Refactor measurement code to allow for path length and area measurement tools in the future.},
  howpublished = {Zenodo}
}

@book{rothstein2005publication,
  title={Publication bias in meta-analysis: Prevention, Assessment, and Adjustments},
  author={Rothstein, Hannah R and Sutton, Alexander J and Borenstein, Michael},
  year={2005},
  publisher={West Sussex, England: John Wiley & Sons}
}

@article{Sallese_Vannest_2022, 
  title={Effects of a Manualized Teacher-Led Coaching Intervention on Paraprofessional Use of Behavior-Specific Praise}, 
  volume={43}, 
  ISSN={0741-9325}, 
  DOI={10.1177/07419325211017298}, 
  abstractNote={Paraprofessionals are an essential part of special education. School districts increasingly rely on paraprofessional support to meet students’ needs, but formal professional development opportunities vary. A lack of training in effective instructional strategies is potentially problematic for the efficacy of support staff. A multiple-baseline across participants single-case research design examined the effects of a manualized teacher-to-paraprofessional coaching intervention to increase the rate of behavior-specific praise by paraprofessionals. Participant dyads (paraprofessionals and special education teachers) taught in a rural public elementary school serving third- through fifth-grade students. The collaborative multicomponent training program included self-monitoring, performance feedback, goal setting, modeling, and action planning. Analyses encompassed primary author visual analysis, masked visual analysis by three independent raters, and nonparametric statistical analysis. The intervention resulted in increased use of behavior-specific praise across all four paraprofessionals and participants indicated good social validity. Discussions include implications for future research and practice.}, 
  number={1}, 
  journal={Remedial and Special Education}, 
  publisher={SAGE Publications Inc}, 
  author={Sallese, Mary Rose and Vannest, Kimberly J.}, 
  year={2022}, 
  month=feb, 
  pages={27–39}, 
  language={en} 
}

@article{Scotti_Evans_Meyer_Walker_1991, 
  title={A meta-analysis of intervention research with problem behavior: treatment validity and standards of practice}, 
  volume={96}, 
  ISSN={0895-8017}, 
  abstractNote={Published intervention research to remediate problem behavior provides a major source of empirical evidence regarding standards of practice and the relative effectiveness of intervention strategies. A meta-analysis of the developmental disabilities literature for the years 1976 through 1987 was performed. Two measures of intervention effectiveness were employed to evaluate the relations between standards of practice, intervention and participant characteristics, and the treatment validity of different levels of intervention for a range of excess behaviors. The results largely failed to support several widespread assumptions regarding precepts of clinical practice. Suggestions were made concerning clinical-experimental research and publication practices to ensure that future work will provide a more conclusive base.}, 
  number={3}, 
  journal={American journal of mental retardation: AJMR}, author={Scotti, J. R. and Evans, I. M. and Meyer, L. H. and Walker, P.}, 
  year={1991}, 
  month=nov, 
  pages={233–256}, 
  language={eng} 
}

@article{Scruggs_Mastropieri_Casto_1987, 
  title={The Quantitative Synthesis of Single-Subject Research: Methodology and Validation}, 
  volume={8}, 
  ISSN={0741-9325}, 
  DOI={10.1177/074193258700800206}, 
  abstractNote={This article describes procedures recently employed for the quantitative synthesis of single-subject research literature in special education. First, the need for objective, systematic review procedures is discussed. Second, previous approaches for quantitative evaluation of outcomes of single-case research designs are reviewed. Third, procedures employed by the present authors are outlined using examples from recent synthesis efforts. Finally, implications for future reviews of single-subject research are described.}, number={2}, journal={Remedial and Special Education},
  publisher={SAGE Publications Inc}, 
  author={Scruggs, Thomas E. and Mastropieri, Margo A. and Casto, Glendon},
  year={1987}, 
  month=mar, 
  pages={24–33} 
}

@article{Shadish2007methods,
  title = {Methods for Evidence-Based Practice: {{Quantitative}} Synthesis of Single-Subject Designs},
  author = {Shadish, William R and Rindskopf, David M},
  year = {2007},
  journal = {New Directions for Evaluation},
  volume = {113},
  number = {113},
  pages = {95--109},
  publisher = {{Wiley Online Library}},
  issn = {10976736},
  doi = {10.1002/ev.217}
}

@article{Shadish2013d,
  title = {Analysis and Meta-Analysis of Single-Case Designs with a Standardized Mean Difference Statistic: {{A}} Primer and Applications},
  author = {Shadish, William R and Hedges, Larry V and Pustejovsky, James E},
  year = {2014},
  month = dec,
  journal = {Journal of School Psychology},
  volume = {52},
  number = {2},
  pages = {123--147},
  publisher = {{Elsevier B.V.}},
  issn = {0022-4405},
  doi = {10.1016/j.jsp.2013.11.005},
  copyright = {All rights reserved},
  keywords = {Analysis,d-Statistic,Meta-analysis,Single case designs}
}

@article{shadish2015metaanalytic,
  title = {The Meta-Analytic Big Bang},
  author = {Shadish, William R. and Lecy, Jesse D.},
  year = {2015},
  journal = {Research Synthesis Methods},
  volume = {6},
  number = {3},
  pages = {246--264},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1132},
  urldate = {2022-02-16},
  abstract = {This article looks at the impact of meta-analysis and then explores why meta-analysis was developed at the time and by the scholars it did in the social sciences in the 1970s. For the first problem, impact, it examines the impact of meta-analysis using citation network analysis. The impact is seen in the sciences, arts and humanities, and on such contemporaneous developments as multilevel modeling, medical statistics, qualitative methods, program evaluation, and single-case design. Using a constrained snowball sample of citations, we highlight key articles that are either most highly cited or most central to the systematic review network. Then, the article examines why meta-analysis came to be in the 1970s in the social sciences through the work of Gene Glass, Robert Rosenthal, and Frank Schmidt, each of whom developed similar theories of meta-analysis at about the same time. The article ends by explaining how Simonton's chance configuration theory and Campbell's evolutionary epistemology can illuminate why meta-analysis occurred with these scholars when it did and not in medical sciences. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  langid = {english}
}

@techreport{shadish2015Role,
  title = {The {{Role}} of {{Between-Case Effect Size}} in {{Conducting}}, {{Interpreting}}, and {{Summarizing Single-Case Research}}},
  author = {Shadish, William R and Hedges, Larry V and Horner, Robert H and Odom, Samuel L},
  year = {2015},
  month = dec,
  number = {NCER 2015-002},
  pages = {109},
  address = {{Washington, DC}},
  institution = {{National Center for Education Research, Institute of Education Sciences, U.S. Department of Education}},
  langid = {english}
}

@article{Shadish_Kyse_Rindskopf_2013, 
  title={Analyzing data from single-case designs using multilevel models: New applications and some agenda items for future research.}, 
  volume={18}, 
  ISSN={1939-1463, 1082-989X}, 
  DOI={10.1037/a0032964}, 
  number={3}, 
  journal={Psychological Methods}, 
  author={Shadish, William R. and Kyse, Eden Nagler and Rindskopf, David M.}, 
  year={2013}, 
  pages={385--405}, 
  language={en} 
}

@article{Shadish_Sullivan_2011, 
  title={Characteristics of single-case designs used to assess intervention effects in 2008}, 
  volume={43}, 
  ISSN={1554-3528}, 
  DOI={10.3758/s13428-011-0111-y}, 
  abstractNote={This article reports the results of a study that located, digitized, and coded all 809 single-case designs appearing in 113 studies in the year 2008 in 21 journals in a variety of fields in psychology and education. Coded variables included the specific kind of design, number of cases per study, number of outcomes, data points and phases per case, and autocorrelations for each case. Although studies of the effects of interventions are a minority in these journals, within that category, single-case designs are used more frequently than randomized or nonrandomized experiments. The modal study uses a multiple-baseline design with 20 data points for each of three or four cases, where the aim of the intervention is to increase the frequency of a desired behavior; but these characteristics vary widely over studies. The average autocorrelation is near to but significantly different from zero; but autocorrelations are significantly heterogeneous. The results have implications for the contributions of single-case designs to evidence-based practice and suggest a number of future research directions.}, 
  number={4}, 
  journal={Behavior Research Methods}, 
  author={Shadish, William R. and Sullivan, Kristynn J.}, 
  year={2011}, 
  month=dec, 
  pages={971–980}, 
  language={en} 
}

@article{Shadish2016, 
  title={A survey of publication practices of single-case design researchers when treatments have small or large effects}, 
  volume={49}, 
  ISSN={1938-3703}, 
  DOI={10.1002/jaba.308}, abstractNote={The published literature often underrepresents studies that do not find evidence for a treatment effect; this is often called publication bias. Literature reviews that fail to include such studies may overestimate the size of an effect. Only a few studies have examined publication bias in single-case design (SCD) research, but those studies suggest that publication bias may occur. This study surveyed SCD researchers about publication preferences in response to simulated SCD results that show a range of small to large effects. Results suggest that SCD researchers are more likely to submit manuscripts that show large effects for publication and are more likely to recommend acceptance of manuscripts that show large effects when they act as a reviewer. A nontrivial minority of SCD researchers (4% to 15%) would drop 1 or 2 cases from the study if the effect size is small and then submit for publication. This article ends with a discussion of implications for publication practices in SCD research.}, 
  number={3}, 
  journal={Journal of Applied Behavior Analysis}, 
  author={Shadish, William R. and Zelinsky, Nicole A. M. and Vevea, Jack L. and Kratochwill, Thomas R.}, 
  year={2016}, 
  month=sep, 
  pages={656–673}, 
  language={eng} 
}

@article{Sham_Smith_2014, 
  title={Publication bias in studies of an applied behavior-analytic intervention: An initial analysis}, 
  volume={47}, 
  ISSN={1938-3703}, 
  DOI={10.1002/jaba.146}, 
  abstractNote={Publication bias arises when studies with favorable results are more likely to be reported than are studies with null findings. If this bias occurs in studies with single-subject experimental designs (SSEDs) on applied behavior-analytic (ABA) interventions, it could lead to exaggerated estimates of intervention effects. Therefore, we conducted an initial test of bias by comparing effect sizes, measured by percentage of nonoverlapping data (PND), in published SSED studies (n = 21) and unpublished dissertations (n = 10) on 1 well-established intervention for children with autism, pivotal response treatment (PRT). Although published and unpublished studies had similar methodologies, the mean PND in published studies was 22% higher than in unpublished studies, 95% confidence interval (4%, 38%). Even when unpublished studies are included, PRT appeared to be effective (PND M = 62%). Nevertheless, the disparity between published and unpublished studies suggests a need for further assessment of publication bias in the ABA literature.}, 
  number={3}, 
  journal={Journal of Applied Behavior Analysis}, 
  author={Sham, Elyssa and Smith, Tristram}, 
  year={2014}, 
  pages={663–678}, 
  language={fr} 
}

@article{stotz2008Effects,
  title = {Effects of {{Self-graphing}} on {{Written Expression}} of {{Fourth Grade Students}} with {{High-Incidence Disabilities}}},
  author = {Stotz, Kate E. and Itoi, Madoka and Konrad, Moira and {Alber-Morgan}, Sheila R.},
  year = {2008},
  month = jun,
  journal = {Journal of Behavioral Education},
  volume = {17},
  number = {2},
  pages = {172--186},
  issn = {1573-3513},
  doi = {10.1007/s10864-007-9055-9},
  urldate = {2022-02-28},
  abstract = {The purpose of this study was to determine the effects of self-graphing on the writing of 3 fourth grade students with high-incidence disabilities. Measures of written expression included total number of words written and number of correct word sequences. During intervention, students self-graphed their total number of words written in response to a timed story starter. A functional relationship was found between the self-graphing intervention and the total words written and number of correct word sequences. Implications for future research and practice are discussed.},
  langid = {english}
}

@article{StrasbergerFerreri2014, 
 title={The Effects of Peer Assisted Communication Application Training on the Communicative and Social Behaviors of Children with Autism}, 
 volume={26}, 
 ISSN={1573-3580}, 
 DOI={10.1007/s10882-013-9358-9}, 
 abstractNote={Non-verbal children with autism are candidates for augmentative and alternative communication (AAC). Augmentative and Alternative Communication is defined as systems that either supplement or replace existing communication when speech impairments are present, such as the case with non-verbal children with autism (Mirenda, Language Speech and Hearing Services in Schools 34(3), 203–216, 2003). One type of AAC device is a speech-generating device (SGD). The primary drawbacks of past SGDs, which are portable electronic devices that produce digitized speech were their expense and portability. Newer iPod-based VOCAs alleviate these concerns. This study sought to evaluate an iPod-based SGD and a peer mediated intervention to teach children with autism more sophisticated communication skills. Using a multiple baseline design, four children with autism were taught through peer assisted communication application (PACA) training how to mand using a two-step sequence and respond to the questions, “What do you want?” and “What is your name?” using a two-step sequence. Data were taken on the number of independent mands and independent responses. Results indicated that three of the four participants were able to acquire communicative skills. The implications are analyzed in regards to the effectiveness of peer assisted communication application training to teach sophisticated communication skills.}, 
 number={5}, 
 journal={Journal of Developmental and Physical Disabilities}, 
 author={Strasberger, Sean K. and Ferreri, Summer J.}, 
 year={2014}, 
 month=oct, 
 pages={513–526}, 
 language={en} 
}

@article{Swaminathan2014effect, 
 title={An effect size measure and Bayesian analysis of single-case designs}, 
 volume={52}, 
 ISSN={00224405}, 
 DOI={10.1016/j.jsp.2013.12.002}, 
 abstractNote={This article describes a linear modeling approach for the analysis of single-case designs (SCDs). Effect size measures in SCDs have been defined and studied for the situation where there is a level change without a time trend. However, when there are level and trend changes, effect size measures are either defined in terms of changes in R2 or defined separately for changes in slopes and intercept coefficients. We propose an alternate effect size measure that takes into account changes in slopes and intercepts in the presence of serial dependence and provides an integrated procedure for the analysis of SCDs through estimation and inference based directly on the effect size measure. A Bayesian procedure is described to analyze the data and draw inferences in SCDs. A multilevel model that is appropriate when several subjects are available is integrated into the Bayesian procedure to provide a standardized effect size measure comparable to effect size measures in a between-subjects design. The applicability of the Bayesian approach for the analysis of SCDs is demonstrated through an example.}, 
 number={2}, 
 journal={Journal of School Psychology}, 
 author={Swaminathan, Hariharan and Rogers, H. Jane and Horner, Robert H.}, 
 year={2014}, 
 month=apr, 
 pages={213--230}, 
 language={en} 
}

@article{Swan_Pustejovsky_2018, 
 title={A Gradual Effects Model for Single-Case Designs}, 
 volume={53}, 
 ISSN={0027-3171, 1532-7906}, 
 DOI={10.1080/00273171.2018.1466681}, 
 abstractNote={Single-case designs are a class of repeated measures experiments used to evaluate the effects of interventions for small or specialized populations, such as individuals with low-incidence disabilities. There has been growing interest in systematic reviews and syntheses of evidence from single-case designs, but there remains a need to further develop appropriate statistical models and effect sizes for data from the designs. We propose a novel model for single-case data that exhibit nonlinear time trends created by an intervention that produces gradual effects, which build up and dissipate over time. The model expresses a structural relationship between a pattern of treatment assignment and an outcome variable, making it appropriate for both treatment reversal and multiple baseline designs. It is formulated as a generalized linear model so that it can be applied to outcomes measured as frequency counts or proportions, both of which are commonly used in single-case research, while providing readily interpretable effect size estimates such as log response ratios or log odds ratios. We demonstrate the gradual effects model by applying it to data from a single-case study and examine the performance of proposed estimation methods in a Monte Carlo simulation of frequency count data.}, 
 number={4}, 
 journal={Multivariate Behavioral Research}, 
 author={Swan, Daniel M. and Pustejovsky, James E.}, 
 year={2018}, 
 month=jul, 
 pages={574--593}, 
 language={en} 
}

 @article{Tarlow_2017, 
  title={An Improved Rank Correlation Effect Size Statistic for Single-Case Designs: Baseline Corrected Tau}, 
  volume={41}, 
  ISSN={0145-4455}, 
  DOI={10.1177/0145445516676750}, 
  abstractNote={Measuring treatment effects when an individual?s pretreatment performance is improving poses a challenge for single-case experimental designs. It may be difficult to determine whether improvement is due to the treatment or due to the preexisting baseline trend. Tau-U is a popular single-case effect size statistic that purports to control for baseline trend. However, despite its strengths, Tau-U has substantial limitations: Its values are inflated and not bound between ?1 and +1, it cannot be visually graphed, and its relatively weak method of trend control leads to unacceptable levels of Type I error wherein ineffective treatments appear effective. An improved effect size statistic based on rank correlation and robust regression, Baseline Corrected Tau, is proposed and field-tested with both published and simulated single-case time series. A web-based calculator for Baseline Corrected Tau is also introduced for use by single-case investigators.}, 
  number={4}, 
  journal={Behavior Modification}, 
  publisher={SAGE Publications Inc}, 
  author={Tarlow, Kevin R.}, 
  year={2017}, month=jul, 
  pages={427–467} 
}

@article{tate2016SingleCase,
  title = {The {{Single-Case Reporting Guideline In BEhavioural Interventions}} ({{SCRIBE}}) 2016: {{Explanation}} and {{Elaboration}}},
  author = {Tate, Robyn L and Perdices, Michael and Rosenkoetter, Ulrike and Togher, Leanne and McDonald, Skye and Shadish, William and Horner, Robert and Kratochwill, Thomas and Barlow, David H and Kazdin, Alan and Sampson, Margaret and Shamseer, Larissa and Vohra, Sunita},
  year = {2016},
  pages = {22},
  abstract = {There is substantial evidence that research studies reported in the scientific literature do not provide adequate information so that readers know exactly what was done and what was found. This problem has been addressed by the development of reporting guidelines which tell authors what should be reported and how it should be described. Many reporting guidelines are now available for different types of research designs. There is no such guideline for one type of research design commonly used in the behavioral sciences, the single-case experimental design (SCED). The present study addressed this gap. This report describes the Single-Case Reporting guideline In BEhavioural interventions (SCRIBE) 2016, which is a set of 26 items that authors need to address when writing about SCED research for publication in a scientific journal. Each item is described, a rationale for its inclusion is provided, and examples of adequate reporting taken from the literature are quoted. It is recommended that the SCRIBE 2016 is used by authors preparing manuscripts describing SCED research for publication, as well as journal reviewers and editors who are evaluating such manuscripts.},
  langid = {english}
}

@article{taylor2022Promoting,
  title = {Promoting {{Knowledge Accumulation About Intervention Effects}}: {{Exploring Strategies}} for {{Standardizing Statistical Approaches}} and {{Effect Size Reporting}}},
  shorttitle = {Promoting {{Knowledge Accumulation About Intervention Effects}}},
  author = {Taylor, Joseph A. and Pigott, Terri and Williams, Ryan},
  year = {2022},
  month = jan,
  journal = {Educational Researcher},
  volume = {51},
  number = {1},
  pages = {72--80},
  issn = {0013-189X, 1935-102X},
  doi = {10.3102/0013189X211051319},
  urldate = {2022-02-25},
  abstract = {Toward the goal of more rapid knowledge accumulation via better meta-analyses, this article explores statistical approaches intended to increase the precision and comparability of effect sizes from education research. The featured estimate of the proposed approach is a standardized mean difference effect size whose numerator is a mean difference that has been adjusted for baseline differences in the outcome measure, at a minimum, and whose denominator is the total variance. The article describes the utility and efficiency of covariate adjustment through baseline measures and the need to standardize effects on a total variance that accounts for variation at multiple levels. As computation of the total variance can be complex in multilevel studies, a shiny application is provided to assist with computation of the total variance and subsequent effect size. Examples are provided for how to interpret and input the required calculator inputs.},
  langid = {english}
}

@article{Tipton_2015, 
  title={Small sample adjustments for robust variance estimation with meta-regression.}, 
  volume={20}, 
  ISSN={1939-1463, 1082-989X}, 
  DOI={10.1037/met0000011}, number={3}, 
  journal={Psychological Methods}, 
  author={Tipton, Elizabeth}, 
  year={2015}, 
  pages={375--393}, 
  language={en} 
}

@article{Tipton_Pustejovsky_2015, 
  title={Small-Sample Adjustments for Tests of Moderators and Model Fit Using Robust Variance Estimation in Meta-Regression}, 
  volume={40}, 
  ISSN={1076-9986, 1935-1054}, 
  DOI={10.3102/1076998615606099}, 
  number={6}, 
  journal={Journal of Educational and Behavioral Statistics}, 
  author={Tipton, Elizabeth and Pustejovsky, James E.}, 
  year={2015}, 
  month=dec, 
  pages={604--634}, 
  language={en} 
}

@article{van2013three,
  title={Three-level meta-analysis of dependent effect sizes},
  author={Van den Noortgate, Wim and L{\'o}pez-L{\'o}pez, Jos{\'e} Antonio and Mar{\'\i}n-Mart{\'\i}nez, Fulgencio and S{\'a}nchez-Meca, Julio},
  journal={Behavior research methods},
  volume={45},
  pages={576--594},
  year={2013},
  publisher={Springer},
  doi = {10.3758/s13428-012-0261-6}
}

@article{van2003combining,
  title={Combining single-case experimental data using hierarchical linear models.},
  author={Van den Noortgate, Wim and Onghena, Patrick},
  journal={School Psychology Quarterly},
  volume={18},
  number={3},
  pages={325},
  year={2003},
  publisher={Guilford Publications},
  doi = {10.1521/scpq.18.3.325.22577}
}

@article{van2003hierarchical,
  title={Hierarchical linear models for the quantitative integration of effect sizes in single-case research},
  author={Van Den Noortgate, Wim and Onghena, Patrick},
  journal={Behavior Research Methods, Instruments, \& Computers},
  volume={35},
  number={1},
  pages={1--10},
  year={2003},
  publisher={Springer},
  doi = {10.3758/BF03195492}
}

@article{VandenNoortgate2008multilevel,
  title = {A Multilevel Meta-Analysis of Single-Subject Experimental Design Studies},
  author = {{Van den Noortgate}, Wim and Onghena, Patrick},
  year = {2008},
  journal = {Evidence-Based Communication Assessment and Intervention},
  volume = {2},
  number = {3},
  pages = {142--151},
  issn = {1748-9539},
  doi = {10.1080/17489530802505362},
  keywords = {meta-analysis,multilevel model,single-case,single-subject}
}

@article{viechtbauer2010conducting,
  title={Conducting meta-analyses in R with the metafor package},
  author={Viechtbauer, Wolfgang},
  journal={Journal of statistical software},
  volume={36},
  pages={1--48},
  year={2010},
  doi = {10.18637/jss.v036.i03}
}

@article{walker2005using,
  title={Using the expressive writing program to improve the writing skills of high school students with learning disabilities},
  author={Walker, Barbara and Shippen, Margaret E and Alberto, Paul and Houchins, David E and Cihak, David F},
  journal={Learning Disabilities Research \& Practice},
  volume={20},
  number={3},
  pages={175--183},
  year={2005},
  publisher={Wiley Online Library},
  doi = {10.1111/j.1540-5826.2005.00131.x}
}

@article{walker2007improving,
  title={Improving the Writing Skills of High School Students with Learning Disabilities Using the {{Expressive Writing}} program},
  author={Walker, Barbara D and Shippen, Margaret E and Houchins, David E and Cihak, David F},
  journal={International Journal of Special Education},
  volume={22},
  number={2},
  pages={66--76},
  year={2007},
  publisher={ERIC}
}

@techreport{WWC2022,
  title = {What {{Works Clearinghouse Procedures and Standards Handbook}}},
  author = {{What Works Clearinghouse}},
  year = {2020},
  number = {Version 5.0},
  address = {{Washington, DC}},
  institution = {{U.S. Department of Education, Institute of Education Sciences, National Center for Education Evaluation and Regional Assistance.}}
}

@techreport{whatworksclearinghouse2020What,
  title = {What {{Works Clearinghouse Standards Handbook}}},
  author = {{What Works Clearinghouse}},
  year = {2020},
  number = {Version 4.1},
  address = {{Washington, DC}},
  institution = {{U.S. Department of Education, Institute of Education Sciences, National Center for Education Evaluation and Regional Assistance.}}
}

@article{White1987some,
  title = {Some Comments Concerning "{{The}} Quantitative Synthesis of Single-Subject Research"},
  author = {White, Owen R},
  year = {1987},
  journal = {Remedial and Special Education},
  volume = {8},
  number = {2},
  pages = {34--39},
  issn = {0741-9325},
  doi = {10.1177/074193258700800207}
}

@article{wood2018Does,
  title = {Does {{Use}} of {{Text-to-Speech}} and {{Related Read-Aloud Tools Improve Reading Comprehension}} for {{Students With Reading Disabilities}}? {{A Meta-Analysis}}},
  shorttitle = {Does {{Use}} of {{Text-to-Speech}} and {{Related Read-Aloud Tools Improve Reading Comprehension}} for {{Students With Reading Disabilities}}?},
  author = {Wood, Sarah G. and Moxley, Jerad H. and Tighe, Elizabeth L. and Wagner, Richard K.},
  year = {2018},
  month = jan,
  journal = {Journal of Learning Disabilities},
  volume = {51},
  number = {1},
  pages = {73--84},
  publisher = {{SAGE Publications Inc}},
  issn = {0022-2194},
  doi = {10.1177/0022219416688170},
  urldate = {2022-02-28},
  abstract = {Text-to-speech and related read-aloud tools are being widely implemented in an attempt to assist students' reading comprehension skills. Read-aloud software, including text-to-speech, is used to translate written text into spoken text, enabling one to listen to written text while reading along. It is not clear how effective text-to-speech is at improving reading comprehension. This study addresses this gap in the research by conducting a meta-analysis on the effects of text-to-speech technology and related read-aloud tools on reading comprehension for students with reading difficulties. Random effects models yielded an average weighted effect size of (d\textasciimacron d\textasciimacron{$<$}math display="inline" id="math1-0022219416688170" overflow="scroll" altimg="eq-00001.gif"{$><$}mover accent="true"{$><$}mi{$>$}d{$<$}/mi{$><$}mo{$>$}\textasciimacron{$<$}/mo{$><$}/mover{$><$}/math{$>$} = .35, with a 95\% confidence interval of .14 to .56, p {$<$} .01). Moderator effects of study design were found to explain some of the variance. Taken together, this suggests that text-to-speech technologies may assist students with reading comprehension. However, more studies are needed to further explore the moderating variables of text-to-speech and read-aloud tools' effectiveness for improving reading comprehension. Implications and recommendations for future research are discussed.},
  langid = {english},
  keywords = {meta-analysis,reading comprehension,reading disabilities,technology,text-to-speech}
}

@article{zimmerman2018Singlecase,
  title = {Single-Case Synthesis Tools {{I}}: {{Comparing}} Tools to Evaluate {{SCD}} Quality and Rigor},
  shorttitle = {Single-Case Synthesis Tools {{I}}},
  author = {Zimmerman, Kathleen N. and Ledford, Jennifer R. and Severini, Katherine E. and Pustejovsky, James E. and Barton, Erin E. and Lloyd, Blair P.},
  year = {2018},
  month = aug,
  journal = {Research in Developmental Disabilities},
  volume = {79},
  pages = {19--32},
  issn = {08914222},
  doi = {10.1016/j.ridd.2018.02.003},
  urldate = {2019-01-11},
  abstract = {Tools for evaluating the quality and rigor of single case research designs (SCD) are often used when conducting SCD syntheses. Preferred components include evaluations of design features related to the internal validity of SCD to obtain quality and/or rigor ratings. Three tools for evaluating the quality and rigor of SCD (Council for Exceptional Children, What Works Clearinghouse, and Single-Case Analysis and Design Framework) were compared to determine if conclusions regarding the effectiveness of antecedent sensory-based interventions for young children changed based on choice of quality evaluation tool. Evaluation of SCD quality differed across tools, suggesting selection of quality evaluation tools impacts evaluation findings. Suggestions for selecting an appropriate quality and rigor assessment tool are provided and across-tool conclusions are drawn regarding the quality and rigor of studies. Finally, authors provide guidance for using quality evaluations in conjunction with outcome analyses when conducting syntheses of interventions evaluated in the context of SCD.},
  copyright = {All rights reserved},
  langid = {english}
}
