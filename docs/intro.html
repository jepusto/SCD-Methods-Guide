<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Approaches for Effect Size Estimation and Synthesis of Single-Case Designs | Effect Size Estimation and Synthesis of Single-Case Designs: A Methods Guide</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Approaches for Effect Size Estimation and Synthesis of Single-Case Designs | Effect Size Estimation and Synthesis of Single-Case Designs: A Methods Guide" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="github-repo" content="jepusto/SCD-Methods-Guide" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Approaches for Effect Size Estimation and Synthesis of Single-Case Designs | Effect Size Estimation and Synthesis of Single-Case Designs: A Methods Guide" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

<meta name="author" content="John M. Ferron, Lodi Lippen, Megan Kirby, Wendy Machalicek, James Pustejovsky, and Man Chen" />


<meta name="date" content="2022-02-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="DC-ES.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Synthesis of Single-Case Designs</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preamble</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Approaches for Effect Size Estimation and Synthesis of Single-Case Designs</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#background"><i class="fa fa-check"></i><b>1.1</b> Background</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#purpose-of-the-methods-guide"><i class="fa fa-check"></i><b>1.2</b> Purpose of the Methods Guide</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#study-quality"><i class="fa fa-check"></i><b>1.3</b> Study Quality</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#selecting-an-approach-for-effect-estimation-and-synthesis"><i class="fa fa-check"></i><b>1.4</b> Selecting an Approach for Effect Estimation and Synthesis</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="intro.html"><a href="intro.html#design-comparable-effect-sizes"><i class="fa fa-check"></i><b>1.4.1</b> Design-Comparable Effect Sizes</a></li>
<li class="chapter" data-level="1.4.2" data-path="intro.html"><a href="intro.html#case-specific-effect-sizes"><i class="fa fa-check"></i><b>1.4.2</b> Case-Specific Effect Sizes</a></li>
<li class="chapter" data-level="1.4.3" data-path="intro.html"><a href="intro.html#multilevel-modeling-of-individual-participant-interrupted-time-series-data"><i class="fa fa-check"></i><b>1.4.3</b> Multilevel Modeling of Individual Participant Interrupted Time-Series Data</a></li>
<li class="chapter" data-level="1.4.4" data-path="intro.html"><a href="intro.html#summary-of-options-for-effect-estimation-and-synthesis"><i class="fa fa-check"></i><b>1.4.4</b> Summary of Options for Effect Estimation and Synthesis</a></li>
<li class="chapter" data-level="1.4.5" data-path="intro.html"><a href="intro.html#limitations-in-selecting-an-approach-for-effect-estimation-and-synthesis"><i class="fa fa-check"></i><b>1.4.5</b> Limitations in Selecting an Approach for Effect Estimation and Synthesis</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#structure-of-the-methods-guide"><i class="fa fa-check"></i><b>1.5</b> Structure of the Methods Guide</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="DC-ES.html"><a href="DC-ES.html"><i class="fa fa-check"></i><b>2</b> Introduction to Design-Comparable Effect Sizes</a>
<ul>
<li class="chapter" data-level="2.1" data-path="DC-ES.html"><a href="DC-ES.html#background-1"><i class="fa fa-check"></i><b>2.1</b> Background</a></li>
<li class="chapter" data-level="2.2" data-path="DC-ES.html"><a href="DC-ES.html#when-to-use-design-comparable-effect-sizes"><i class="fa fa-check"></i><b>2.2</b> When to Use Design-Comparable Effect Sizes</a></li>
<li class="chapter" data-level="2.3" data-path="DC-ES.html"><a href="DC-ES.html#a-general-definition-of-design-comparable-effect-sizes"><i class="fa fa-check"></i><b>2.3</b> A General Definition of Design-Comparable Effect Sizes</a></li>
<li class="chapter" data-level="2.4" data-path="DC-ES.html"><a href="DC-ES.html#what-we-assume-with-design-comparable-effect-size-estimation-and-synthesis"><i class="fa fa-check"></i><b>2.4</b> What We Assume with Design-Comparable Effect Size Estimation and Synthesis</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="DC-ES.html"><a href="DC-ES.html#exchangeable-effect-sizes"><i class="fa fa-check"></i><b>2.4.1</b> Exchangeable Effect Sizes</a></li>
<li class="chapter" data-level="2.4.2" data-path="DC-ES.html"><a href="DC-ES.html#es-estimation-homogeneity-of-variance"><i class="fa fa-check"></i><b>2.4.2</b> ES Estimation: Homogeneity of Variance</a></li>
<li class="chapter" data-level="2.4.3" data-path="DC-ES.html"><a href="DC-ES.html#es-estimation-normality"><i class="fa fa-check"></i><b>2.4.3</b> ES Estimation: Normality</a></li>
<li class="chapter" data-level="2.4.4" data-path="DC-ES.html"><a href="DC-ES.html#es-estimation-appropriate-structural-model"><i class="fa fa-check"></i><b>2.4.4</b> ES Estimation: Appropriate Structural Model</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="DC-ES.html"><a href="DC-ES.html#modeling-options-for-design-comparable-effect-size-estimation"><i class="fa fa-check"></i><b>2.5</b> Modeling Options for Design-comparable Effect Size Estimation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Effect Size Estimation and Synthesis of Single-Case Designs: A Methods Guide</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="intro" class="section level1" number="1">
<h1><span class="header-section-number">Chapter 1</span> Approaches for Effect Size Estimation and Synthesis of Single-Case Designs</h1>
<p>This chapter provides the background and purpose for this methods guide. It also gives an overview of the three major approaches for estimation and synthesis of single-case studies:</p>
<ol style="list-style-type: lower-alpha">
<li>design-comparable effect sizes,</li>
<li>case-specific effect sizes, and</li>
<li>multilevel modeling of the raw individual participant interrupted time-series data.</li>
</ol>
<p>We describe the motivation and rationale for each of these approaches and provide a series of decision rules to guide researchers in selecting among them. The remaining chapters provide details and illustrations of the methods.</p>
<div id="background" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Background</h2>
<p>Educational decisions at the state, district, school, and student level are now expected to be informed by empirical evidence <span class="citation">(<a href="#ref-2014Council" role="doc-biblioref"><span>“Council for <span>Exceptional Children</span>,”</span> 2014</a>; <a href="#ref-whatworksclearinghouse2020What" role="doc-biblioref">What Works Clearinghouse, 2020</a>)</span>.
These expectations create a major need for synthesis, or the integration of research findings from multiple, existing sources of evidence—including findings from single-case designs (SCDs). <span class="citation">(<a href="#ref-cooper2010Research" role="doc-biblioref">Cooper, 2010</a>; <a href="#ref-pustejovsky2017Research" role="doc-biblioref">Pustejovsky &amp; Ferron, 2017</a>)</span>y developments in education related to the documentation of evidence-based practices.
The first is the expanded use of SCD research methods across varying disciplines over the last 50 years, in both general and special education contexts <span class="citation">(<a href="#ref-kratochwill2013SingleCase" role="doc-biblioref">Kratochwill et al., 2013</a>)</span>.
Although the history of SCD methodology is beyond our present scope, innovations in design have expanded the use of SCDs across a range of professional fields, moving beyond SCD’s quasi-experimental and behavior analysis origins <span class="citation">(see <a href="#ref-kratochwill2013SingleCase" role="doc-biblioref">Kratochwill et al., 2013</a> for more detailed information on the history of SCD)</span>.
Over the past 20 years, researchers’ commitment to using rigorous procedures to identify evidence-based educational practices affirms not only the importance of randomized control trials (RCTs), but the effectiveness and efficiency of SCD.</p>
<p>The second development is the emergence of effect size (ES) measures and synthesis methods for use with SCD and other interrupted time series data <span class="citation">(<a href="#ref-shadish2015Role" role="doc-biblioref">Shadish et al., 2015</a>; <a href="#ref-Swaminathan2014effect" role="doc-biblioref">Swaminathan et al., 2014</a>)</span>.
Interest in synthesis of SCDs is long-standing <span class="citation">(e.g., <a href="#ref-Center1985methodology" role="doc-biblioref">Center et al., 1985</a>; <a href="#ref-Gingerich1984meta" role="doc-biblioref">Gingerich, 1984</a>; <a href="#ref-White1987some" role="doc-biblioref">White, 1987</a>)</span>, emerging around the same time as methods for meta-analysis of group designs were becoming more statistically rigorous and sophisticated <span class="citation">(<a href="#ref-hedges1985statistical" role="doc-biblioref">Hedges &amp; Olkin, 1985</a>; <a href="#ref-shadish2015metaanalytic" role="doc-biblioref">Shadish &amp; Lecy, 2015</a>)</span>.
However, the past twenty years have seen increased intensity of methodological research focused on SCDs and a substantial expansion in the diversity, flexibility, and accessibility of available analytic methods upon which researchers can draw.
We view the current state of methodology as falling into three strands:</p>
<ol style="list-style-type: lower-alpha">
<li>approaches that summarize the effect of each case and then synthesize these case-specific effect sizes <span class="citation">(e.g., <a href="#ref-pustejovsky2018Using" role="doc-biblioref">Pustejovsky, 2018</a>)</span>,</li>
<li>methods that use multi-level models for analyzing the raw (or standardized) data from one or multiple SCD studies <span class="citation">(<a href="#ref-VandenNoortgate2008multilevel" role="doc-biblioref">Van den Noortgate &amp; Onghena, 2008</a>)</span>, and</li>
<li>techniques that use design-comparable effect size metrics <span class="citation">(<a href="#ref-Hedges2012MB" role="doc-biblioref">Hedges et al., 2013</a>, <a href="#ref-Hedges2012ABk" role="doc-biblioref">2012</a>; <a href="#ref-Pustejovsky2014design" role="doc-biblioref">Pustejovsky et al., 2014</a>; <a href="#ref-Shadish2013d" role="doc-biblioref">Shadish et al., 2014</a>; <a href="#ref-Swaminathan2014effect" role="doc-biblioref">Swaminathan et al., 2014</a>)</span>.</li>
</ol>
<p>The present guide is organized around these three broad methodological strands.</p>
<p>The third development is the increased use of systematic review and meta-analysis procedures to identify and affirm evidence-based practices in education <span class="citation">(<a href="#ref-Beretvas2008review" role="doc-biblioref">Beretvas &amp; Chung, 2008</a>; <a href="#ref-Shadish2007methods" role="doc-biblioref">Shadish &amp; Rindskopf, 2007</a>)</span>.
Maggin and colleagues <span class="citation">(<a href="#ref-maggin2011Quantitative" role="doc-biblioref">2011</a>)</span> examined the production of systematic reviews and meta-analyses of SCDs from 1985 to 2009, finding a marked increase in the appearance of such reviews between 2000 and 2009.
In another survey, Jamshidi and colleagues <span class="citation">(<a href="#ref-jamshidi2018Methodological" role="doc-biblioref">2018</a>)</span> found increasing production through 2015, as well as improvements in the quality of published reviews.
However, they also noted that, even in more recent literature, reviews often frequently failed to use appropriate methods for combining findings across studies <span class="citation">(<a href="#ref-jamshidi2018Methodological" role="doc-biblioref">Jamshidi et al., 2018</a>)</span>.
Thus, there remains a need for guidance about how to select and apply methods for synthesizing findings from SCD research.</p>
<p>Effect size (ES) measures are a critical companion to visual analysis for the interpretation of single-case research results, and are a key input in two of the available approaches for meta-analytic synthesis of SCDs.
Although a variety of technically sound ES metrics exist for researchers to use when interpreting SCD findings, relatively few published meta-analyses use these more advanced techniques <span class="citation">(cf. <a href="#ref-barton2017TechnologyAided" role="doc-biblioref">Barton et al., 2017</a>; <a href="#ref-jamshidi2018Methodological" role="doc-biblioref">Jamshidi et al., 2018</a>; <a href="#ref-Maggin2017meta-analysis" role="doc-biblioref">Maggin et al., 2017</a>)</span>.
One reason for the lack of widespread use by researchers may be the conceptual and procedural complexity associated with advances in ES measures and meta-analysis techniques.
A more rapid uptake of ES estimation methods may be hindered by the complexity of data extraction and calculation of ES for SCD.
In addition, researchers may lack software tools for ES estimation for single-case studies when using some common statistical packages such as SPSS and SAS <span class="citation">(<a href="#ref-odom2018Betweencase" role="doc-biblioref">Odom et al., 2018</a>; <a href="#ref-Pustejovsky2014design" role="doc-biblioref">Pustejovsky et al., 2014</a>; <a href="#ref-shadish2015Role" role="doc-biblioref">Shadish et al., 2015</a>)</span>.</p>
</div>
<div id="purpose-of-the-methods-guide" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Purpose of the Methods Guide</h2>
<p>The purpose of this methods guide is to improve educational researchers’ practice in the estimation of ES measures and synthesis of findings from SCDs.
We recognize that no single method is ideal for all research goals.
Furthermore, methods that have the most to offer can be complex and may appear difficult to carry out.
Thus, through the use of this methods guide, we aim to: (a) provide guidance and decision rules to simplify the process of selecting effect size estimation and synthesis methods and (b) illustrate the step-by-step application of appropriate methods using readily available tools, such as web-based software to calculate case-specific ES or design-comparable ES for SCD studies <span class="citation">(e.g., <a href="#ref-pustejovsky2021scdhlm" role="doc-biblioref">Pustejovsky et al., 2021</a>)</span>.</p>
</div>
<div id="study-quality" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Study Quality</h2>
<p>Conducting a research synthesis (whether of group design studies, SCDs, or both) involves several stages, starting with formulating the research aims, specifying inclusion criteria, conducting a systematic search, and screening for eligible studies <span class="citation">(<a href="#ref-cooper2010Research" role="doc-biblioref">Cooper, 2010</a>; <a href="#ref-pustejovsky2017Research" role="doc-biblioref">Pustejovsky &amp; Ferron, 2017</a>)</span>.
Additionally, before carrying out effect size calculations or meta-analysis, it is critical to consider the methodological quality and potential biases of studies to be included in the synthesis.
To assist researchers in doing so, several distinct sets of standards are available for SCD studies <span class="citation">(e.g., <a href="#ref-kratochwill2013SingleCase" role="doc-biblioref">Kratochwill et al., 2013</a>; <a href="#ref-reichow2018Development" role="doc-biblioref">Reichow et al., 2018</a>; <a href="#ref-tate2016SingleCase" role="doc-biblioref">Tate et al., 2016</a>; <a href="#ref-zimmerman2018Singlecase" role="doc-biblioref">Zimmerman et al., 2018</a>)</span>.
After assessing methodological quality, researchers can use one of two strategies to guide the estimation and synthesis of effect sizes.
One strategy incorporates consideration of study quality as an inclusion criteria, so that low-quality studies are screened out and synthesis is based on the subset of studies with quality high enough so that changes in outcome(s) can reasonably be attributed to the intervention.
For example, the What Works Clearinghouse Single-Case Pilot Standards <span class="citation">(<a href="#ref-Kratochwill2010single" role="doc-biblioref">Kratochwill et al., 2010</a>)</span> indicated that effect sizes are only to be computed after studies have been shown to meet both minimum design criteria (e.g., a multiple-baseline study has at least three temporally spaced opportunities for the effect to be demonstrated, along with phases of at least three observations) and minimum evidence criteria (e.g., visual analysis of the data from the study show experimental control so that the changes in the outcome can be attributed to the intervention).
Concerns with estimating effect sizes for SCD studies without the use visual analysis to rule out alternative explanations for observed changes in the outcomes continues to be echoed in the literature <span class="citation">(<a href="#ref-kratochwill2021Singlecase" role="doc-biblioref">Kratochwill et al., 2021</a>; <a href="#ref-maggin2021Commentary" role="doc-biblioref">Maggin et al., 2021</a>)</span>.
An alternative strategy in considering study quality is to use broader inclusion criteria, code for aspects study quality, and examine study quality codes as potential moderators in a meta-analysis.
With this approach, researchers can estimate ESs for studies of varying degrees of quality and empirically explore whether the magnitude of ESs varies depending on aspects of study quality.</p>
<p>We assume that researchers who use this methods guide will have already selected a method to assess study quality and an approach for incorporating those assessments into their synthesis.
As such, we do not focus on SCD study quality assessment methods, but rather provide guidelines that can be applied to a collection of studies that have met the researchers’ inclusion criteria, which potentially include criteria related to study quality.
Thus, in this guide we focus on the final stages of a research synthesis, on the questions of how to select a method for estimating effects, how to compute ES estimates (or otherwise prepare data for synthesis), and how to synthesize findings in the form of ES estimates or individual-level data.</p>
</div>
<div id="selecting-an-approach-for-effect-estimation-and-synthesis" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Selecting an Approach for Effect Estimation and Synthesis</h2>
<p>In order to select an approach for estimating and synthesizing effects from SCDs, we recommend that researchers first reflect on the research aims that motivate their synthesis.
In some contexts, researchers’ primary aims may be focused on summarizing evidence to arrive at statements about average efficacy of a class of interventions.
In other contexts, researchers might instead or additionally be interested in understanding variation in effects and the extent to which such variation is associated with characteristics of participants, specific features of interventions, or other contextual factors.
When the focus is mostly on summarization, researchers may find it more useful to use design-comparable effect sizes that describe average effects.
If individual-level variation is the focus, then approaches using case-specific effect sizes or multi-level modeling may be more advantageous.</p>
<p>Another over-arching consideration for selecting a synthesis approach pertains to the features of the studies identified for inclusion in the synthesis.
Quantitative synthesis requires choosing an effect size metric that permits comparisons of the magnitude of effects across individual participants and studies.
Consequently, the extent to which eligible studies use different types of designs or different types of outcome measurements creates constraints on how effects from those studies can be described or compared.
For instance, if all eligible studies used multiple baseline designs across participants (or another common type of SCD), then several different synthesis approaches are feasible.
In contrast, if eligible studies include both SCDs and group design studies (such as small randomized experiments, each with a single pre-test and a single post-test), then researchers need a synthesis approach that permits comparisons across both types of designs.
Similarly, if all eligible studies used SCDs with very similar methods for assessing the dependent variable, then synthesis based on multi-level modeling of raw data is possible.
In contrast, if eligible studies used a variety of different assessments, then a synthesis approach based on case-specific effect sizes may be required.</p>
<p>These two broad considerations—the aims of the synthesis and the features of eligible studies—can guide the selection of an approach for synthesis of SCDs.
We now describe in more detail how three broad approaches to synthesis fit within these considerations.</p>
<div id="design-comparable-effect-sizes" class="section level3" number="1.4.1">
<h3><span class="header-section-number">1.4.1</span> Design-Comparable Effect Sizes</h3>
<p>In some situations, the aim of the research team is to synthesize the evidence for intervention effectiveness from both single-case and group design studies.
For example, a meta-analysis by Wood and colleagues <span class="citation">(<a href="#ref-wood2018Does" role="doc-biblioref">2018</a>)</span> analyzed 22 single-case and between-group studies to examine the effects of text-to-speech and other read-aloud tools on reading comprehension outcomes for students with reading disabilities.
The authors used the standardized mean difference to estimate read-aloud intervention effects in the group design studies and a comparable standardized mean difference from the included single-case research, resulting in an overall average weighted effect size of <span class="math inline">\(d\)</span> = 0.35, 95% CI (0.14, 0.56).
Because the purpose of the <span class="citation"><a href="#ref-wood2018Does" role="doc-biblioref">Wood et al.</a> (<a href="#ref-wood2018Does" role="doc-biblioref">2018</a>)</span> study involved the comparison and averaging of effects across single-case and group designs, it was critical to use an ES metric that is theoretically comparable across the designs.
In similar situations, researchers should select from the design-comparable effect size options <span class="citation">(<a href="#ref-Hedges2012MB" role="doc-biblioref">Hedges et al., 2013</a>, <a href="#ref-Hedges2012ABk" role="doc-biblioref">2012</a>; <a href="#ref-Pustejovsky2014design" role="doc-biblioref">Pustejovsky et al., 2014</a>; <a href="#ref-Shadish2013d" role="doc-biblioref">Shadish et al., 2014</a>; <a href="#ref-Swaminathan2014effect" role="doc-biblioref">Swaminathan et al., 2014</a>; <a href="#ref-VandenNoortgate2008multilevel" role="doc-biblioref">Van den Noortgate &amp; Onghena, 2008</a>)</span>.
However, if researchers aim to synthesize findings from only single-case studies (i.e., not to integrate findings across single-case and group design studies), other options may be preferable.</p>
</div>
<div id="case-specific-effect-sizes" class="section level3" number="1.4.2">
<h3><span class="header-section-number">1.4.2</span> Case-Specific Effect Sizes</h3>
<p>In addition to summarizing average effects across studies, researchers may also be interested in exploring variation in treatment effects across individual participants.
When dependent variables are measured differently across studies, it is important for researchers to use an effect size metric and synthesis approach that accounts for such.
For example, Bowman-Perrott and colleagues <span class="citation">(<a href="#ref-bowman-perrott2016Promoting" role="doc-biblioref">2016</a>)</span> examined five potential moderators of the effectiveness of the Good Behavior Game in promoting positive behavior in the classroom.
Results of their meta-analysis suggested that the intervention was most effective in reducing problem behaviors among students with or at risk for emotional and behavioral disorders.
Another meta-analysis by Mason and colleagues <span class="citation">(<a href="#ref-mason2016Video" role="doc-biblioref">2016</a>)</span> investigated the moderating effects of participant characteristics, targeted outcomes, and implementation components on the efficacy of video self-modeling, in which a learner with disabilities watches a video of a model engaged in targeted skills or tasks.
They found that intervention effects were stronger for younger participants with autism-spectrum disorders.</p>
<p>Because these syntheses focused on investigating variation across individuals in the effect of treatment, it was important that the effect size estimation and synthesis approach produced effect estimates for each individual participant, rather than a study-level summary effect estimates.
In addition, because the outcome measure differed among studies, the researchers needed an individual-level effect size metric that is not scale-dependent (e.g., they could not be based on simple raw score mean differences).
In contexts like these, researchers should consider selecting among the case-specific effect size estimation and synthesis options.
The case-specific effect size options are not viable for synthesis across single-case and group design studies because the case-specific effects are not comparable to the group design effects.
However, the design-comparable effects are not viable for studying within-participant variation between individuals in treatment effectiveness because they produce effect estimates at the study level (i.e., the effect averaged across the cases), not the individual level.</p>
</div>
<div id="multilevel-modeling-of-individual-participant-interrupted-time-series-data" class="section level3" number="1.4.3">
<h3><span class="header-section-number">1.4.3</span> Multilevel Modeling of Individual Participant Interrupted Time-Series Data</h3>
<p>Finally, researchers might have identified a set of SCDs that all use the same or very similar outcome measures, with the aim of studying the variation in effects over time within and between individuals.
For example, Datchuk and colleagues <span class="citation">(<a href="#ref-datchuk2020Level" role="doc-biblioref">Datchuk et al., 2020</a>)</span> meta-analyzed 15 single-case studies with 79 students to examine the effects of an intervention on the level and trend in correct writing sequences per minute for students with disabilities.
They found the effect increased with time in intervention (i.e., there was a positive effect on the trend) and that this temporal change in effect was more pronounced with younger students.
When focusing on both variation in effect over time and variation in effect across cases, it is important that the researchers select a meta-analytic approach that does not rely only on a single effect estimate for a study (e.g., design-comparable effect sizes) or even a single effect estimate for a case (e.g., case-specific effect sizes).
In contexts like this, we suggest researchers consider options for multilevel modeling of individual participant data series.</p>
</div>
<div id="summary-of-options-for-effect-estimation-and-synthesis" class="section level3" number="1.4.4">
<h3><span class="header-section-number">1.4.4</span> Summary of Options for Effect Estimation and Synthesis</h3>
<p>The flow chart in Figure <a href="intro.html#fig:synthesis-flow-chart">1.1</a> illustrates a set of heuristic decision rules for selecting among the three general approaches to estimating and synthesizing single-case research.
If the primary purpose of one’s research is to integrate findings from both single-case and group-design studies, the researcher should consider design-comparable effect sizes.
Alternately, if the primary purpose of the research is to integrate findings from SCDs only, additional questions should be addressed related to the measurement of the dependent variable.
If the outcome of interest is measured in different ways for different cases and the researcher aims to examine how effects vary across the cases, then the researcher should consider the options for estimating and synthesizing case-specific effect sizes.
If the outcome is measured the same way across cases, and there is interest in how the effect changes over the course of an intervention, the researcher should consider multilevel modeling of the raw data series.</p>
<div class="figure"><span style="display:block;" id="fig:synthesis-flow-chart"></span>
<img src="images/Synthesis-flow-chart.png" alt="Flow chart showing the approach for synthesizing effect evidence based on the types of studies being examined and whether the outcome variable is common across cases."  />
<p class="caption">
Figure 1.1: Flow chart showing the approach for synthesizing effect evidence based on the types of studies being examined and whether the outcome variable is common across cases.
</p>
</div>
</div>
<div id="limitations-in-selecting-an-approach-for-effect-estimation-and-synthesis" class="section level3" number="1.4.5">
<h3><span class="header-section-number">1.4.5</span> Limitations in Selecting an Approach for Effect Estimation and Synthesis</h3>
<p>We emphasize that Figure <a href="intro.html#fig:synthesis-flow-chart">1.1</a> presents a heuristic, simplified procedure for the selection among the three general approaches to ES estimation and synthesis, which does cover every possible research context.
There will surely be situations where researchers’ aims and contexts differ from those we describe, and thus do not align perfectly with one of our primary approaches to estimating and synthesizing single-case effect sizes.
For example, researchers who are synthesizing findings from a set of SCDs may wish to compare their results to a previously published meta-analysis of group design studies, but not to investigate individual-level variation in treatment effects.
They may therefore elect to use design-comparable effect sizes even though they are not formally integrating results from group design studies within their review.</p>
<p>A further possibility is that researchers might elect to use multiple approaches to synthesis in order to address different aims or questions.
For example, consider a project in which researchers have identified both single-case and group design studies.
They might want to integrate findings across design types while also exploring the variation in effects among individuals.
In this scenario, researchers could estimate design-comparable effect sizes for their first aim and case-specific effect sizes from the subset of single-case studies for their second aim.</p>
<p>We also note that there may be situations that falls in between those we described for case-specific effect sizes and those for multilevel modeling of the raw data series.
For example, researchers may want to examine how effects vary over time and across cases, using studies with different outcomes.
For this purpose, the researchers can use extensions of the primary approaches we present.
The researchers could either standardize the raw data before estimating a multilevel model, or they could synthesize case specific effect sizes where they use multiple standardized effects for each case (e.g., an effect that indexes the immediate shift in level, and another effect that indexes a change in slope).
Our simplified selection method cannot exhaustively cover all currently available options.</p>
<p>Finally, we anticipate that the heuristic guidance we provide here will need to be refined over time, as further methodological innovations become available.
We anticipate that research presently underway will provide even more meta-analytic options in the future, with implications for how to select an approach for synthesis.
At some point it may be possible to compute case-specific effect sizes that are also design-comparable, or it may be possible to standardize the data for multilevel models in a way that leads to parameter estimates from the model that correspond to design-comparable effect estimates.
If such methods become available, some of the distinctions made here will become artificial.
In the time, remainder of this methods guide follows the proposed heuristics for selecting among the three major approaches to effect size estimation and synthesis.
Even as methodology continues to advance, researchers need guidance that acknowledges the complexity of research purposes and contexts and is dynamic in its accommodation of such variation, while also being concrete and straightforward enough to be put into practice.</p>
</div>
</div>
<div id="structure-of-the-methods-guide" class="section level2" number="1.5">
<h2><span class="header-section-number">1.5</span> Structure of the Methods Guide</h2>
<p>We divide the remainder of this guide into three major sections:</p>
<ol style="list-style-type: lower-alpha">
<li>design-comparable effect size estimation and synthesis,</li>
<li>case-specific effect size estimation and synthesis, and</li>
<li>multilevel modeling to estimate and synthesize effects.</li>
</ol>
<p>These sections do not build upon each other or need to be read in order. Rather, we expect those using this guide to follow the decision rules in Figure <a href="intro.html#fig:synthesis-flow-chart">1.1</a> to determine which section of the guide is most appropriate for them, and then to jump immediately to that section.</p>
<p>Each major section is divided into chapters.
The initial chapter of each section introduces the specific approach and its assumptions, discusses when to use it, and what options exist within the approach.
Furthermore, we provide additional decision rules for selecting among the specific techniques and options available with a given broad approach.
We encourage researchers to use the decision rules within the initial chapter of the major section to get to an appropriate option.
Then, researchers can directly refer to the chapter that has the illustration for that specific option.
For each illustration, we:</p>
<ol style="list-style-type: decimal">
<li>describe the purposes for estimating and synthesizing effects, and the available data,</li>
<li>demonstrate how to use the decision rules in Figure <a href="intro.html#fig:synthesis-flow-chart">1.1</a>, along with the additional decision rules within the initial chapter of the major section, to arrive at the option being illustrated,</li>
<li>present the data for the illustration showing how it needs to be structured for the analysis, and</li>
<li>provide a step-by-step illustration of how to estimate and synthesize effects using readily available analysis tools.</li>
</ol>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-barton2017TechnologyAided" class="csl-entry">
Barton, E. E., Pustejovsky, J. E., Maggin, D. M., &amp; Reichow, B. (2017). Technology-<span>Aided Instruction</span> and <span>Intervention</span> for <span>Students With ASD</span>: <span>A Meta-Analysis Using Novel Methods</span> of <span>Estimating Effect Sizes</span> for <span>Single-Case Research</span>. <em>Remedial and Special Education</em>, <em>38</em>(6), 371–386. <a href="https://doi.org/10.1177/0741932517729508">https://doi.org/10.1177/0741932517729508</a>
</div>
<div id="ref-Beretvas2008review" class="csl-entry">
Beretvas, S. N., &amp; Chung, H. (2008). A review of meta-analyses of single-subject experimental designs: <span>Methodological</span> issues and practice. <em>Evidence-Based Communication Assessment and Intervention</em>, <em>2</em>(3), 129–141. <a href="https://doi.org/10.1080/17489530802446302">https://doi.org/10.1080/17489530802446302</a>
</div>
<div id="ref-bowman-perrott2016Promoting" class="csl-entry">
Bowman-Perrott, L., Burke, M. D., Zaini, S., Zhang, N., &amp; Vannest, K. (2016). Promoting <span>Positive Behavior Using</span> the <span>Good Behavior Game</span>: <span>A Meta-Analysis</span> of <span>Single-Case Research</span>. <em>Journal of Positive Behavior Interventions</em>, <em>18</em>(3), 180–190. <a href="https://doi.org/10.1177/1098300715592355">https://doi.org/10.1177/1098300715592355</a>
</div>
<div id="ref-Center1985methodology" class="csl-entry">
Center, B. A., Skiba, R. J., &amp; Casey, A. (1985). A methodology for the quantitative synthesis of intra-subject design research. <em>The Journal of Special Education</em>, <em>19</em>(4), 387.
</div>
<div id="ref-cooper2010Research" class="csl-entry">
Cooper, H. M. (2010). <em>Research <span>Synthesis</span> and <span>Meta-Analysis</span></em> (Fourth). <span>SAGE Publications</span>.
</div>
<div id="ref-2014Council" class="csl-entry">
Council for <span>Exceptional Children</span>: <span>Standards</span> for <span>Evidence-Based Practices</span> in <span>Special Education</span>. (2014). <em>TEACHING Exceptional Children</em>, <em>46</em>(6), 206–212. <a href="https://doi.org/10.1177/0040059914531389">https://doi.org/10.1177/0040059914531389</a>
</div>
<div id="ref-datchuk2020Level" class="csl-entry">
Datchuk, S. M., Wagner, K., &amp; Hier, B. O. (2020). Level and <span>Trend</span> of <span>Writing Sequences</span>: <span>A Review</span> and <span>Meta-Analysis</span> of <span>Writing Interventions</span> for <span>Students With Disabilities</span>. <em>Exceptional Children</em>, <em>86</em>(2), 174–192. <a href="https://doi.org/10.1177/0014402919873311">https://doi.org/10.1177/0014402919873311</a>
</div>
<div id="ref-Gingerich1984meta" class="csl-entry">
Gingerich, W. J. (1984). Meta-analysis of applied time-series data. <em>The Journal of Applied Behavioral Science</em>, <em>20</em>(1), 71–79. <a href="https://doi.org/10.1177/002188638402000113">https://doi.org/10.1177/002188638402000113</a>
</div>
<div id="ref-hedges1985statistical" class="csl-entry">
Hedges, L. V., &amp; Olkin, I. (1985). <em>Statistical <span>Methods</span> for <span>Meta-Analysis</span></em>. <span>Academic Press</span>.
</div>
<div id="ref-Hedges2012MB" class="csl-entry">
Hedges, L. V., Pustejovsky, J. E., &amp; Shadish, W. R. (2013). A standardized mean difference effect size for multiple baseline designs across individuals. <em>Research Synthesis Methods</em>. <a href="https://doi.org/10.1002/jrsm.1086">https://doi.org/10.1002/jrsm.1086</a>
</div>
<div id="ref-Hedges2012ABk" class="csl-entry">
Hedges, L. V., Pustejovsky, J. E., &amp; Shadish, W. R. (2012). A standardized mean difference effect size for single case designs. <em>Research Synthesis Methods</em>, <em>3</em>, 224–239. <a href="https://doi.org/10.1002/jrsm.1052">https://doi.org/10.1002/jrsm.1052</a>
</div>
<div id="ref-jamshidi2018Methodological" class="csl-entry">
Jamshidi, L., Heyvaert, M., Declercq, L., Fernández-Castilla, B., Ferron, J. M., Moeyaert, M., Beretvas, S. N., Onghena, P., &amp; Van den Noortgate, W. (2018). Methodological quality of meta-analyses of single-case experimental studies. <em>Research in Developmental Disabilities</em>, <em>79</em>, 97–115. <a href="https://doi.org/10.1016/j.ridd.2017.12.016">https://doi.org/10.1016/j.ridd.2017.12.016</a>
</div>
<div id="ref-kratochwill2013SingleCase" class="csl-entry">
Kratochwill, T. R., Hitchcock, J. H., Horner, R. H., Levin, J. R., Odom, S. L., Rindskopf, D. M., &amp; Shadish, W. R. (2013). Single-<span>Case Intervention Research Design Standards</span>. <em>Remedial and Special Education</em>, <em>34</em>(1), 26–38. <a href="https://doi.org/10.1177/0741932512452794">https://doi.org/10.1177/0741932512452794</a>
</div>
<div id="ref-Kratochwill2010single" class="csl-entry">
Kratochwill, T. R., Hitchcock, J., Horner, R. H., Levin, J. R., Odom, S. L., Rindskopf, D. M., &amp; Shadish, W. R. (2010). <em>Single-<span>Case Designs Technical Documentation</span>, <span>Version</span> 1.0 (<span>Pilot</span>)</em> (June; Vol. 0). <span>What Works Clearinghouse</span>.
</div>
<div id="ref-kratochwill2021Singlecase" class="csl-entry">
Kratochwill, T. R., Horner, R. H., Levin, J. R., Machalicek, W., Ferron, J., &amp; Johnson, A. (2021). Single-case design standards: <span>An</span> update and proposed upgrades. <em>Journal of School Psychology</em>, <em>89</em>, 91–105. <a href="https://doi.org/10.1016/j.jsp.2021.10.006">https://doi.org/10.1016/j.jsp.2021.10.006</a>
</div>
<div id="ref-maggin2021Commentary" class="csl-entry">
Maggin, D. M., Barton, E., Reichow, B., Lane, K., &amp; Shogren, K. A. (2021). Commentary on the <span><em>What Works Clearinghouse Standards</em></span><span> <em>and</em> </span><span><em>Procedures Handbook</em></span> (v. 4.1) for the <span>Review</span> of <span>Single-Case Research</span>. <em>Remedial and Special Education</em>, 074193252110513. <a href="https://doi.org/10.1177/07419325211051317">https://doi.org/10.1177/07419325211051317</a>
</div>
<div id="ref-maggin2011Quantitative" class="csl-entry">
Maggin, D. M., O’Keeffe, B. V., &amp; Johnson, A. H. (2011). A <span>Quantitative Synthesis</span> of <span>Methodology</span> in the <span>Meta-Analysis</span> of <span>Single-Subject Research</span> for <span>Students</span> with <span>Disabilities</span>: 1985. <em>Exceptionality</em>, <em>19</em>(2), 109–135. <a href="https://doi.org/10.1080/09362835.2011.565725">https://doi.org/10.1080/09362835.2011.565725</a>
</div>
<div id="ref-Maggin2017meta-analysis" class="csl-entry">
Maggin, D. M., Pustejovsky, J. E., &amp; Johnson, A. H. A. H. (2017). A meta-analysis of school-based group contingency interventions for students with challenging behavior: <span>An</span> update. <em>Remedial and Special Education</em>, <em>38</em>(6), 353–370. <a href="https://doi.org/10.1177/0741932517716900">https://doi.org/10.1177/0741932517716900</a>
</div>
<div id="ref-mason2016Video" class="csl-entry">
Mason, R. A., Davis, H. S., Ayres, K. M., Davis, J. L., &amp; Mason, B. A. (2016). Video <span>Self-Modeling</span> for <span>Individuals</span> with <span>Disabilities</span>: <span>A Best-Evidence</span>, <span>Single Case Meta-Analysis</span>. <em>Journal of Developmental and Physical Disabilities</em>, <em>28</em>(4), 623–642. <a href="https://doi.org/10.1007/s10882-016-9484-2">https://doi.org/10.1007/s10882-016-9484-2</a>
</div>
<div id="ref-odom2018Betweencase" class="csl-entry">
Odom, S. L., Barton, E. E., Reichow, B., Swaminathan, H., &amp; Pustejovsky, J. E. (2018). Between-case standardized effect size analysis of single case designs: <span>Examination</span> of the two methods. <em>Research in Developmental Disabilities</em>, <em>79</em>, 88–96. <a href="https://doi.org/10.1016/j.ridd.2018.05.009">https://doi.org/10.1016/j.ridd.2018.05.009</a>
</div>
<div id="ref-pustejovsky2018Using" class="csl-entry">
Pustejovsky, J. E. (2018). Using response ratios for meta-analyzing single-case designs with behavioral outcomes. <em>Journal of School Psychology</em>, <em>68</em>, 99–112. <a href="https://doi.org/10.1016/j.jsp.2018.02.003">https://doi.org/10.1016/j.jsp.2018.02.003</a>
</div>
<div id="ref-pustejovsky2021scdhlm" class="csl-entry">
Pustejovsky, J. E., Chen, M., &amp; Hamilton, B. (2021). <em>Scdhlm: <span>A</span> web-based calculator for between-case standardized mean differences</em>.
</div>
<div id="ref-pustejovsky2017Research" class="csl-entry">
Pustejovsky, J. E., &amp; Ferron, J. (2017). Research <span>Synthesis</span> and <span>Meta-Analysis</span> of <span>Single-Case Designs</span>. In <em>Handbook of <span>Special Education</span></em> (2nd Edition, p. 63). <span>Routledge</span>.
</div>
<div id="ref-Pustejovsky2014design" class="csl-entry">
Pustejovsky, J. E., Hedges, L. V., &amp; Shadish, W. R. (2014). Design-comparable effect sizes in multiple baseline designs: <span>A</span> general modeling framework. <em>Journal of Educational and Behavioral Statistics</em>, <em>39</em>(5), 368–393. <a href="https://doi.org/10.3102/1076998614547577">https://doi.org/10.3102/1076998614547577</a>
</div>
<div id="ref-reichow2018Development" class="csl-entry">
Reichow, B., Barton, E. E., &amp; Maggin, D. M. (2018). Development and applications of the single-case design risk of bias tool for evaluating single-case design research study reports. <em>Research in Developmental Disabilities</em>, <em>79</em>, 53–64. <a href="https://doi.org/10.1016/j.ridd.2018.05.008">https://doi.org/10.1016/j.ridd.2018.05.008</a>
</div>
<div id="ref-shadish2015Role" class="csl-entry">
Shadish, W. R., Hedges, L. V., Horner, R. H., &amp; Odom, S. L. (2015). <em>The <span>Role</span> of <span>Between-Case Effect Size</span> in <span>Conducting</span>, <span>Interpreting</span>, and <span>Summarizing Single-Case Research</span></em> (NCER 2015-002; p. 109). <span>National Center for Education Research, Institute of Education Sciences, U.S. Department of Education</span>.
</div>
<div id="ref-Shadish2013d" class="csl-entry">
Shadish, W. R., Hedges, L. V., &amp; Pustejovsky, J. E. (2014). Analysis and meta-analysis of single-case designs with a standardized mean difference statistic: <span>A</span> primer and applications. <em>Journal of School Psychology</em>, <em>52</em>(2), 123–147. <a href="https://doi.org/10.1016/j.jsp.2013.11.005">https://doi.org/10.1016/j.jsp.2013.11.005</a>
</div>
<div id="ref-shadish2015metaanalytic" class="csl-entry">
Shadish, W. R., &amp; Lecy, J. D. (2015). The meta-analytic big bang. <em>Research Synthesis Methods</em>, <em>6</em>(3), 246–264. <a href="https://doi.org/10.1002/jrsm.1132">https://doi.org/10.1002/jrsm.1132</a>
</div>
<div id="ref-Shadish2007methods" class="csl-entry">
Shadish, W. R., &amp; Rindskopf, D. M. (2007). Methods for evidence-based practice: <span>Quantitative</span> synthesis of single-subject designs. <em>New Directions for Evaluation</em>, <em>113</em>(113), 95–109. <a href="https://doi.org/10.1002/ev.217">https://doi.org/10.1002/ev.217</a>
</div>
<div id="ref-Swaminathan2014effect" class="csl-entry">
Swaminathan, H., Rogers, H. J., &amp; Horner, R. H. (2014). An effect size measure and <span>Bayesian</span> analysis of single-case designs. <em>Journal of School Psychology</em>. <a href="https://doi.org/10.1016/j.jsp.2013.12.002">https://doi.org/10.1016/j.jsp.2013.12.002</a>
</div>
<div id="ref-tate2016SingleCase" class="csl-entry">
Tate, R. L., Perdices, M., Rosenkoetter, U., Togher, L., McDonald, S., Shadish, W., Horner, R., Kratochwill, T., Barlow, D. H., Kazdin, A., Sampson, M., Shamseer, L., &amp; Vohra, S. (2016). <em>The <span>Single-Case Reporting Guideline In BEhavioural Interventions</span> (<span>SCRIBE</span>) 2016: <span>Explanation</span> and <span>Elaboration</span></em>. 22.
</div>
<div id="ref-VandenNoortgate2008multilevel" class="csl-entry">
Van den Noortgate, W., &amp; Onghena, P. (2008). A multilevel meta-analysis of single-subject experimental design studies. <em>Evidence-Based Communication Assessment and Intervention</em>, <em>2</em>(3), 142–151. <a href="https://doi.org/10.1080/17489530802505362">https://doi.org/10.1080/17489530802505362</a>
</div>
<div id="ref-whatworksclearinghouse2020What" class="csl-entry">
What Works Clearinghouse. (2020). <em>What <span>Works Clearinghouse Standards Handbook</span></em> (Version 4.1). <span>U.S. Department of Education, Institute of Education Sciences, National Center for Education Evaluation and Regional Assistance.</span>
</div>
<div id="ref-White1987some" class="csl-entry">
White, O. R. (1987). Some comments concerning "<span>The</span> quantitative synthesis of single-subject research". <em>Remedial and Special Education</em>, <em>8</em>(2), 34–39. <a href="https://doi.org/10.1177/074193258700800207">https://doi.org/10.1177/074193258700800207</a>
</div>
<div id="ref-wood2018Does" class="csl-entry">
Wood, S. G., Moxley, J. H., Tighe, E. L., &amp; Wagner, R. K. (2018). Does <span>Use</span> of <span class="nocase">Text-to-Speech</span> and <span>Related Read-Aloud Tools Improve Reading Comprehension</span> for <span>Students With Reading Disabilities</span>? <span>A Meta-Analysis</span>. <em>Journal of Learning Disabilities</em>, <em>51</em>(1), 73–84. <a href="https://doi.org/10.1177/0022219416688170">https://doi.org/10.1177/0022219416688170</a>
</div>
<div id="ref-zimmerman2018Singlecase" class="csl-entry">
Zimmerman, K. N., Ledford, J. R., Severini, K. E., Pustejovsky, J. E., Barton, E. E., &amp; Lloyd, B. P. (2018). Single-case synthesis tools <span>I</span>: <span>Comparing</span> tools to evaluate <span>SCD</span> quality and rigor. <em>Research in Developmental Disabilities</em>, <em>79</em>, 19–32. <a href="https://doi.org/10.1016/j.ridd.2018.02.003">https://doi.org/10.1016/j.ridd.2018.02.003</a>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="DC-ES.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
