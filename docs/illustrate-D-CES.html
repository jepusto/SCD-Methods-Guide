<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Illustration of Design-Comparable Effect Sizes When No Trends Are Assumed | Methods Guide for Effect Estimation and Synthesis of Single-Case Studies</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Illustration of Design-Comparable Effect Sizes When No Trends Are Assumed | Methods Guide for Effect Estimation and Synthesis of Single-Case Studies" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="github-repo" content="jepusto/SCD-Methods-Guide" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Illustration of Design-Comparable Effect Sizes When No Trends Are Assumed | Methods Guide for Effect Estimation and Synthesis of Single-Case Studies" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  



<meta name="date" content="2023-11-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="D-CES.html"/>
<link rel="next" href="illustrate-D-CES-Ttrends.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Synthesis of Single-Case Designs</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Authors</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#disclaimer"><i class="fa fa-check"></i>Disclaimer</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#citation"><i class="fa fa-check"></i>Citation</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#preface"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#datasets"><i class="fa fa-check"></i>Datasets</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Approaches for Effect Size Estimation and Synthesis of Single-Case Designs (the old version DO NOT USE)</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#background"><i class="fa fa-check"></i><b>1.1</b> Background</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#purpose-of-the-methods-guide"><i class="fa fa-check"></i><b>1.2</b> Purpose of the Methods Guide</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#study-quality"><i class="fa fa-check"></i><b>1.3</b> Study Quality</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#selecting-an-approach-for-effect-estimation-and-synthesis"><i class="fa fa-check"></i><b>1.4</b> Selecting an Approach for Effect Estimation and Synthesis</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="intro.html"><a href="intro.html#design-comparable-effect-sizes"><i class="fa fa-check"></i><b>1.4.1</b> Design-Comparable Effect Sizes</a></li>
<li class="chapter" data-level="1.4.2" data-path="intro.html"><a href="intro.html#case-specific-effect-sizes"><i class="fa fa-check"></i><b>1.4.2</b> Case-Specific Effect Sizes</a></li>
<li class="chapter" data-level="1.4.3" data-path="intro.html"><a href="intro.html#multilevel-modeling-of-individual-participant-interrupted-time-series-data"><i class="fa fa-check"></i><b>1.4.3</b> Multilevel Modeling of Individual Participant Interrupted Time-Series Data</a></li>
<li class="chapter" data-level="1.4.4" data-path="intro.html"><a href="intro.html#summary-of-options-for-effect-estimation-and-synthesis"><i class="fa fa-check"></i><b>1.4.4</b> Summary of Options for Effect Estimation and Synthesis</a></li>
<li class="chapter" data-level="1.4.5" data-path="intro.html"><a href="intro.html#limitations-in-selecting-an-approach-for-effect-estimation-and-synthesis"><i class="fa fa-check"></i><b>1.4.5</b> Limitations in Selecting an Approach for Effect Estimation and Synthesis</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#structure-of-the-methods-guide"><i class="fa fa-check"></i><b>1.5</b> Structure of the Methods Guide</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="D-CES.html"><a href="D-CES.html"><i class="fa fa-check"></i><b>2</b> Introduction to Design-Comparable Effect Sizes</a>
<ul>
<li class="chapter" data-level="2.1" data-path="D-CES.html"><a href="D-CES.html#background-1"><i class="fa fa-check"></i><b>2.1</b> Background</a></li>
<li class="chapter" data-level="2.2" data-path="D-CES.html"><a href="D-CES.html#when-to-use-design-comparable-effect-sizes"><i class="fa fa-check"></i><b>2.2</b> When to Use Design-Comparable Effect Sizes</a></li>
<li class="chapter" data-level="2.3" data-path="D-CES.html"><a href="D-CES.html#general-definition-of-design-comparable-effect-sizes"><i class="fa fa-check"></i><b>2.3</b> General Definition of Design-Comparable Effect Sizes</a></li>
<li class="chapter" data-level="2.4" data-path="D-CES.html"><a href="D-CES.html#what-we-assume-when-we-synthesize-design-comparable-effect-sizes"><i class="fa fa-check"></i><b>2.4</b> What We Assume When We Synthesize Design-Comparable Effect Sizes</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="D-CES.html"><a href="D-CES.html#what-we-assume-when-we-estimate-design-comparable-effect-size"><i class="fa fa-check"></i><b>2.4.1</b> What We Assume When We Estimate Design-Comparable Effect Size</a></li>
<li class="chapter" data-level="2.4.2" data-path="D-CES.html"><a href="D-CES.html#normality"><i class="fa fa-check"></i><b>2.4.2</b> Normality</a></li>
<li class="chapter" data-level="2.4.3" data-path="D-CES.html"><a href="D-CES.html#homogeneity-of-variance"><i class="fa fa-check"></i><b>2.4.3</b> Homogeneity of Variance</a></li>
<li class="chapter" data-level="2.4.4" data-path="D-CES.html"><a href="D-CES.html#appropriate-structural-model"><i class="fa fa-check"></i><b>2.4.4</b> Appropriate Structural Model</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="D-CES.html"><a href="D-CES.html#modeling-options-for-design-comparable-effect-size-estimation"><i class="fa fa-check"></i><b>2.5</b> Modeling Options for Design-comparable Effect Size Estimation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="illustrate-D-CES.html"><a href="illustrate-D-CES.html"><i class="fa fa-check"></i><b>3</b> Illustration of Design-Comparable Effect Sizes When No Trends Are Assumed</a>
<ul>
<li class="chapter" data-level="3.1" data-path="illustrate-D-CES.html"><a href="illustrate-D-CES.html#selecting-a-design-comparable-effect-size-for-the-single-case-studies"><i class="fa fa-check"></i><b>3.1</b> Selecting a Design-Comparable Effect Size for the Single-Case Studies</a></li>
<li class="chapter" data-level="3.2" data-path="illustrate-D-CES.html"><a href="illustrate-D-CES.html#details-of-the-no-trend-models-for-design-comparable-effect-sizes"><i class="fa fa-check"></i><b>3.2</b> Details of the No Trend Models for Design-Comparable Effect Sizes</a></li>
<li class="chapter" data-level="3.3" data-path="illustrate-D-CES.html"><a href="illustrate-D-CES.html#estimating-the-design-comparable-effect-size-for-the-single-case-studies"><i class="fa fa-check"></i><b>3.3</b> Estimating the Design-Comparable Effect Size for the Single-Case Studies</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="illustrate-D-CES.html"><a href="illustrate-D-CES.html#example-1-multiple-baseline-study-by-case1992improving"><i class="fa fa-check"></i><b>3.3.1</b> Example 1: Multiple Baseline Study by <span class="citation">Case et al. (1992)</span></a></li>
<li class="chapter" data-level="3.3.2" data-path="illustrate-D-CES.html"><a href="illustrate-D-CES.html#example-2-multiple-baseline-study-by-peltier2020effects"><i class="fa fa-check"></i><b>3.3.2</b> Example 2: Multiple Baseline Study by <span class="citation">Peltier et al. (2020)</span></a></li>
<li class="chapter" data-level="3.3.3" data-path="illustrate-D-CES.html"><a href="illustrate-D-CES.html#example-3-replicated-abab-design-by-lambert2006effects"><i class="fa fa-check"></i><b>3.3.3</b> Example 3: Replicated ABAB Design by <span class="citation">Lambert et al. (2006)</span></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="illustrate-D-CES.html"><a href="illustrate-D-CES.html#estimating-the-design-comparable-effect-size-for-the-group-studies"><i class="fa fa-check"></i><b>3.4</b> Estimating the Design-Comparable Effect Size for the Group Studies</a></li>
<li class="chapter" data-level="3.5" data-path="illustrate-D-CES.html"><a href="illustrate-D-CES.html#analyzing-the-effect-sizes"><i class="fa fa-check"></i><b>3.5</b> Analyzing the Effect Sizes</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="illustrate-D-CES-Ttrends.html"><a href="illustrate-D-CES-Ttrends.html"><i class="fa fa-check"></i><b>4</b> Illustration of Design-Comparable Effect Sizes When Assuming Only Trends in The Treatment Phases</a>
<ul>
<li class="chapter" data-level="4.1" data-path="illustrate-D-CES-Ttrends.html"><a href="illustrate-D-CES-Ttrends.html#selecting-a-design-comparable-effect-size-for-the-single-case-studies-1"><i class="fa fa-check"></i><b>4.1</b> Selecting a Design-Comparable Effect Size for the Single-Case Studies</a></li>
<li class="chapter" data-level="4.2" data-path="illustrate-D-CES-Ttrends.html"><a href="illustrate-D-CES-Ttrends.html#details-of-the-models-for-design-comparable-effect-sizes"><i class="fa fa-check"></i><b>4.2</b> Details of the Models for Design-Comparable Effect Sizes</a></li>
<li class="chapter" data-level="4.3" data-path="illustrate-D-CES-Ttrends.html"><a href="illustrate-D-CES-Ttrends.html#estimating-the-design-comparable-effect-size-for-the-single-case-studies-1"><i class="fa fa-check"></i><b>4.3</b> Estimating the Design-Comparable Effect Size for the Single-Case Studies</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="illustrate-D-CES-Ttrends.html"><a href="illustrate-D-CES-Ttrends.html#example-1-multiple-baselise-study-by-gunning2003psychological"><i class="fa fa-check"></i><b>4.3.1</b> Example 1: Multiple Baselise Study by <span class="citation">Gunning &amp; Espie (2003)</span></a></li>
<li class="chapter" data-level="4.3.2" data-path="illustrate-D-CES-Ttrends.html"><a href="illustrate-D-CES-Ttrends.html#example-2-multiple-baseline-study-by-delemere2018parentimplemented"><i class="fa fa-check"></i><b>4.3.2</b> Example 2: Multiple Baseline Study by <span class="citation">Delemere &amp; Dounavi (2018)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="illustrate-D-CES-Ttrends.html"><a href="illustrate-D-CES-Ttrends.html#estimating-the-design-comparable-effect-size-for-the-group-studies-1"><i class="fa fa-check"></i><b>4.4</b> Estimating the Design-Comparable Effect Size for the Group Studies</a></li>
<li class="chapter" data-level="4.5" data-path="illustrate-D-CES-Ttrends.html"><a href="illustrate-D-CES-Ttrends.html#analyzing-the-effect-sizes-1"><i class="fa fa-check"></i><b>4.5</b> Analyzing the Effect Sizes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Methods Guide for Effect Estimation and Synthesis of Single-Case Studies</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="illustrate-D-CES" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Illustration of Design-Comparable Effect Sizes When No Trends Are Assumed<a href="illustrate-D-CES.html#illustrate-D-CES" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This chapter illustrates the computation of design-comparable effect sizes in contexts where we assume no trend in either baseline or treatment phases. We provide step-by-step instructions to demonstrate the selection and estimation of design-comparable effect sizes, using two multiple baseline studies, a replicated ABAB study, and a group design study. We use the <em>scdhlm</em> app to show how to estimate the design-comparable effect sizes for the single-case studies and discuss estimating the effect size for the group study. Then, we illustrate how to combine the effects using a fixed effect meta-analysis.</p>
<p>In this chapter, we illustrate the computation of design-comparable effect sizes based on models that do not include time trends (i.e., Models 1 and 2 in Figure <a href="D-CES.html#fig:DC-ES-flow-chart">2.4</a>). These models assume that the dependent variable has a stable level throughout the baseline phase for each case and that introduction of treatment leads to an immediate shift in the level of the dependent variable.
In this chapter, we pretend we are researchers who want to synthesize evidence from several single-case design (SCD) and group design studies that examine intervention effects on the improvement of math problem solving for students with disabilities. To model the process succinctly, we limit our illustration to four included studies comprised of two multiple baseline designs, a replicated ABAB design, and one group design.
In one of the SCDs, <span class="citation">Peltier et al. (<a href="#ref-peltier2020Effects">2020</a>)</span> used a multiple baseline design across three small groups to examine the effect of a mathematics intervention for 12 students struggling with mathematical word problems. In another SCD study, <span class="citation">Case et al. (<a href="#ref-case1992Improving">1992</a>)</span> used a multiple baseline design across four students with math word problem solving difficulties to examine the impact of a mathematics intervention. The third SCD study by <span class="citation">Lambert et al. (<a href="#ref-lambert2006effects">2006</a>)</span> used an ABAB design replicated across nine participants with disruptive behaviors from two classrooms to examine intervention effects on math problem solving. The fourth included study by <span class="citation">Hutchinson (<a href="#ref-hutchinson1993Effects">1993</a>)</span> used a group design where students with learning disabilities were randomly assigned to intervention and control conditions and students’ math word problem solving accuracy was measured pre- and post-intervention.</p>
<p>Because our goal is to aggregate effects across single-case and group design studies, we consider design-comparable effect sizes (see the decision rules in Figure <a href="intro.html#fig:synthesis-flow-chart">1.1</a>). We break down this illustration into four stages: (1) selecting a design-comparable effect size for the SCD studies, (2) estimating the design-comparable effect size for the SCD studies using the <em>scdhlm</em> app <span class="citation">(<a href="#ref-pustejovsky2021scdhlm">Pustejovsky et al., 2021</a>)</span>, which is a web-based calculator for design-comparable effect sizes, (3) estimating the effect size for the group study, and (4) synthesizing the effect sizes across the included studies. We concentrate primarily on the first two steps, because well-developed methods for group studies are illustrated in detail elsewhere (e.g., <span class="citation">Borenstein et al. (<a href="#ref-borenstein2021introduction">2021</a>)</span>; <span class="citation">H. Cooper et al. (<a href="#ref-cooper2019handbook">2019</a>)</span>; <span class="citation">Hedges (<a href="#ref-Hedges1981distribution">1981</a>)</span>; <span class="citation">Hedges (<a href="#ref-Hedges2007effect">2007</a>)</span>).</p>
<div id="selecting-a-design-comparable-effect-size-for-the-single-case-studies" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Selecting a Design-Comparable Effect Size for the Single-Case Studies<a href="illustrate-D-CES.html#selecting-a-design-comparable-effect-size-for-the-single-case-studies" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We first select an appropriate design-comparable effect size using the decision rules in Figure <a href="intro.html#fig:synthesis-flow-chart">1.1</a>. To formulate hypotheses about potential trends in baseline and/or treatment phases (i.e., if and why we expect those trends), we rely on previous experience and existing knowledge of the population, study contexts, and outcome of interest. Based on such, we assume that students in the studies would not improve mathematics skills without intervention. We also expect the interventions to abruptly change the outcomes, so we anticipate an immediate shift in the level of performance from baseline to treatment and no trend in the treatment phase. Finally, because we assume no trends in either baseline or treatment phases, we tentatively consider Model 1 or Model 2 from Figure <a href="intro.html#fig:synthesis-flow-chart">1.1</a>. These models are the only options for the ABAB design and are appropriate for multiple baseline designs when not anticipating trends.</p>
<p>Ideally, we make the choice between Model 1 and Model 2 based on our conceptual understanding and our a priori expectation that either the treatment effect would not vary from case to case (i.e., Model 1) or would vary from one case to the next (Model 2). Due to the participants’ differential learning histories and existing skills or abilities, we anticipate some variation in the effect across cases. We tentatively select Model 2 as the best fit, as it is most consistent with our understanding of this context. To verify the tenability of our assumptions, we conduct a visual analysis of the SCD study graphs. We present the data extracted from each SCD study, with data from <span class="citation">Case et al. (<a href="#ref-case1992Improving">1992</a>)</span> shown in Figure <a href="illustrate-D-CES.html#fig:Case-1992">3.1</a>, <span class="citation">Peltier et al. (<a href="#ref-peltier2020Effects">2020</a>)</span> data in Figure <a href="illustrate-D-CES.html#fig:Peltier-2020">3.2</a>, and data extracted from <span class="citation">Lambert et al. (<a href="#ref-lambert2006effects">2006</a>)</span> in Figure <a href="illustrate-D-CES.html#fig:Lambert-2006">3.3</a>.</p>
<p>Prior to examining the data to evaluate its consistency with the Model 2 trend assumptions, we consider whether the data are reasonably aligned with the homogeneity and normality assumptions underlying all design-comparable effect size models. Note that in Figures <a href="illustrate-D-CES.html#fig:Case-1992">3.1</a> and <a href="illustrate-D-CES.html#fig:Peltier-2020">3.2</a>, we generally see similar variation from one case to the next, and similar within-case variation across phases. We also fail to see any outliers that would call into question the normality assumption. However, in Figure <a href="illustrate-D-CES.html#fig:Lambert-2006">3.3</a>, there is a tendency for the treatment phases to have more variability than the baseline phases. Further, in Figure <a href="illustrate-D-CES.html#fig:Lambert-2006">3.3</a>, all observations for Student A4 have values near zero in the first baseline phase—inconsistent with our homogeneity and normality assumptions. Although it appears that some of the design-comparable effect size assumptions were violated, we proceed because the violations appear relatively minor and our combining effect sizes across single-case and group studies requires that we compute a design-comparable effect size for each study (or drop the study from the synthesis).</p>
<p>Next, given the data observed in the primary studies, we consider if a model absent of baseline trends appears reasonable. Across the cases in Figures <a href="illustrate-D-CES.html#fig:Case-1992">3.1</a>-<a href="illustrate-D-CES.html#fig:Lambert-2006">3.3</a>, the typical baseline pattern is one of no trend and is consistent with our expectations. However, there are a few exceptions. In Figure <a href="illustrate-D-CES.html#fig:Case-1992">3.1</a>, Ben’s data appear to have an upward baseline trend with observed values of 2, 2, and 3. This visual trend is ambiguous; it could be an artifact of Ben just happening to solve one more math problem on Day 3 (i.e., chance). Yet, based on our previously stated expectations and an absence of baseline improvement for the other participants, we find it more reasonable to assume that Ben would continue to solve two or three problems per session (i.e., no trend) as opposed to continuing to improve by about one problem per day (i.e., linear trend). The former assumption is also consistent with the decision made by <span class="citation">Case et al. (<a href="#ref-case1992Improving">1992</a>)</span> to intervene with Ben. In other words, had the researchers thought Ben’s math problem solving was improving in baseline, there would be less reason for Ben to receive the intervention.</p>
<p>We also consider other case examples. In Figure <a href="illustrate-D-CES.html#fig:Peltier-2020">3.2</a>, Gary’s data appear to have an upward baseline trend and Kyrie’s data appear to have a downward trend in baseline. These trends are inconsistent with each other, and with the typical pattern seen across the <span class="citation">Peltier et al. (<a href="#ref-peltier2020Effects">2020</a>)</span> study cases. As with Ben (see Figure <a href="illustrate-D-CES.html#fig:Case-1992">3.1</a>), we are not confident that the apparent trend would continue. If Gary’s trend were to continue, there would be no need for intervention. If Kyrie’s trend were to continue, we would predict negative percentages correct in the upcoming sessions, which is not possible. Thus, we proceed with the assumption of no baseline trend because the typical pattern for most cases in each study is consistent with our expectations of no trend, and because of the ambiguity surrounding the few possible exceptions.</p>
<p>Next, focusing on the treatment phases, we conduct similar analyses. In Figures <a href="illustrate-D-CES.html#fig:Case-1992">3.1</a> and <a href="illustrate-D-CES.html#fig:Lambert-2006">3.3</a>, where there is an immediate shift in performance and no trend in the treatment phases, we find patterns that match our hypothesized treatment trend expectations. In Figure <a href="illustrate-D-CES.html#fig:Peltier-2020">3.2</a>, we again see a typical pattern where the shift in performance is immediate and stable over time. While here there are a few exceptions where cases appear to have a decline in performance toward the end of the intervention phase (e.g., Andy and Ellie), we find it best to select a model consistent with the typical and expected pattern. We proceed with a model that assumes no trend in baseline and treatment phases (i.e., Model 1 or Model 2 from Figure <a href="D-CES.html#fig:DC-ES-flow-chart">2.4</a>) because the design-comparable effect sizes assume a common model across cases and estimate an average effect across cases.</p>
<div class="figure"><span style="display:block;" id="fig:Case-1992"></span>
<img src="images/Case1992.png" alt="Multiple Baseline Data Extracted from Case et al. (1992)" width="75%" height="75%" />
<p class="caption">
Figure 3.1: Multiple Baseline Data Extracted from Case et al. (1992)
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:Peltier-2020"></span>
<img src="images/Peltier2020.png" alt="Multiple Baseline Data Extracted from Peltier et al. (2020)" width="75%" height="75%" />
<p class="caption">
Figure 3.2: Multiple Baseline Data Extracted from Peltier et al. (2020)
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:Lambert-2006"></span>
<img src="images/Lambert2006.png" alt="Replicated ABAB Data Extracted from Lambert et al. (2006)" width="75%" height="75%" />
<p class="caption">
Figure 3.3: Replicated ABAB Data Extracted from Lambert et al. (2006)
</p>
</div>
</div>
<div id="details-of-the-no-trend-models-for-design-comparable-effect-sizes" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Details of the No Trend Models for Design-Comparable Effect Sizes<a href="illustrate-D-CES.html#details-of-the-no-trend-models-for-design-comparable-effect-sizes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To fully differentiate between Model 1 and Model 2, we present the formal specification of each. For both Model 1 and Model 2, we can write the within-case model as:
<span class="math display" id="eq:M1M2-L1">\[\begin{equation}
\tag{3.1}
Y_{ij} = \beta_{0j} + \beta_{1j}Tx_{ij} + e_{ij},
\end{equation}\]</span>
where <span class="math inline">\(Y_ij\)</span> is the score on the outcome variable <span class="math inline">\(Y\)</span> at measurement occasion <span class="math inline">\(i\)</span> for case <span class="math inline">\(j\)</span>, and <span class="math inline">\(Tx_{ij}\)</span> is dummy coded with a value of 0 for baseline observations and a value of 1 for the treatment phase observations. The mean baseline level for case <span class="math inline">\(j\)</span> is <span class="math inline">\(\beta_{0j}\)</span> (see Figure <a href="illustrate-D-CES.html#fig:Peltier-Asher-2020">3.4</a> for a visual representation of <span class="math inline">\(\beta_{0j}\)</span> and <span class="math inline">\(\beta_{1j}\)</span>). The raw score treatment effect for case <span class="math inline">\(j\)</span> is indexed by <span class="math inline">\(\beta_{1j}\)</span>, which is the difference between the treatment phase outcome mean and the baseline phase mean. The error (<span class="math inline">\(e_{ij}\)</span>) is time- and case-specific and assumed normally distributed and first-order autoregressive with variance <span class="math inline">\(\sigma_e^2\)</span>.</p>
<div class="figure"><span style="display:block;" id="fig:Peltier-Asher-2020"></span>
<img src="images/Peltier2020_Asher.png" alt="Illustration of Treatment Effect for Asher (Peltier et al., 2020)" width="75%" />
<p class="caption">
Figure 3.4: Illustration of Treatment Effect for Asher (Peltier et al., 2020)
</p>
</div>
<p>For Model 1, the between-case model is:
<span class="math display" id="eq:M1-intercept">\[\begin{equation}
\tag{3.2}
\beta_{0j} = \gamma_{00} + u_{0j}
\end{equation}\]</span>
<span class="math display" id="eq:M1-slope">\[\begin{equation}
\tag{3.3}
\beta_{1j} = \gamma_{10}
\end{equation}\]</span>
where <span class="math inline">\(\gamma_{00}\)</span> is the across-case average baseline mean and <span class="math inline">\(u_{0j}\)</span> is a case-specific error, which is the deviation from the overall average for case <span class="math inline">\(j\)</span>. We assume the error is normally distributed with variance <span class="math inline">\(\sigma_{u_0}^2\)</span>. We assume the across-case average raw score treatment effect, <span class="math inline">\(\gamma_{10}\)</span>, to be constant for all cases. Thus, there is no error term in the equation for <span class="math inline">\(\beta_{1j}\)</span>. Based on this model, the design-comparable effect size is defined as the average raw score treatment effect (<span class="math inline">\(\gamma_{10}\)</span>) divided by a SD that is comparable to the SD used to standardize mean differences in group-design studies <span class="citation">(<a href="#ref-Pustejovsky2014design">Pustejovsky et al., 2014</a>)</span>:
<span class="math display" id="eq:delta-M1">\[\begin{equation}
\tag{3.4}
\delta = \frac{\gamma_{10}}{\sqrt{\sigma_{u_0}^2 + \sigma_e^2}}
\end{equation}\]</span></p>
<p>The Model 2 specification is like Model 1, with the only difference being an error term (<span class="math inline">\(u_{1j}\)</span>) added to Equation <a href="illustrate-D-CES.html#eq:M1-slope">(3.3)</a> to account for between-case variation in the treatment effect. More specifically:
<!-- MC: I corrected the Equation number. James, please double check.-->
<span class="math display" id="eq:M1M2-L1-repeat">\[\begin{equation}
\tag{3.5}
Y_{ij} = \beta_{0j} + \beta_{1j}Tx_{ij} + e_{ij},
\end{equation}\]</span>
<span class="math display" id="eq:M2-intercept">\[\begin{equation}
\tag{3.6}
\beta_{0j} = \gamma_{00} + u_{0j}
\end{equation}\]</span>
<span class="math display" id="eq:M2-slope">\[\begin{equation}
\tag{3.7}
\beta_{1j} = \gamma_{10} + u_{1j}
\end{equation}\]</span>
Again, the across-case average baseline mean is <span class="math inline">\(\gamma_{00}\)</span> and the across-case average raw score treatment effect (i.e., difference in treatment and baseline phase means) is <span class="math inline">\(\gamma_{10}\)</span>. The case-specific errors (<span class="math inline">\(u_{0j}\)</span> and <span class="math inline">\(u_{1j}\)</span>) account for between-case differences in baseline level and response to treatment. They are assumed multivariate normal with covariance
<span class="math inline">\(\Sigma_u = \begin{bmatrix} \sigma_{u_0}^2 &amp; \\ \sigma_{u_1u_0} &amp; \sigma_{u_1}^2 \\ \end{bmatrix}\)</span>.
We define the design-comparable effect size exactly as in Equation <a href="illustrate-D-CES.html#eq:delta-M1">(3.4)</a>,
<!-- MC: I corrected the Equation number. James, please double check.-->
because the effect size is scaled by the SD of the outcome in the absence of intervention and not dependent on the addition of <span class="math inline">\(u_{1j}\)</span>, which only impacts between-case variance in the treatment phase.</p>
<p>Because we tentatively selected Model 2 based on our a priori considerations, we use it to illustrate the estimation of design-comparable effect sizes for this data set. For several reasons, we also contrast the Model 2 results to what we obtain from Model 1. First, contrasting Model 1 and 2 results provides us with an additional way to examine the empirical support for our model selection. For instance, we could report that visual analyses of the model-implied individual trajectories for Model 2 fit the raw data better than the model implied individual trajectories from Model 1. Second, the contrast allows us to examine the sensitivity of our effect size estimates to the model chosen. For instance, this contrast may lead us to rule out between-case effect size variation as having a large impact on the estimated design-comparable effect size. Finally, contrasting allows us to illustrate a method of selecting between models in circumstances where a priori information is not sufficient.</p>
</div>
<div id="estimating-the-design-comparable-effect-size-for-the-single-case-studies" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Estimating the Design-Comparable Effect Size for the Single-Case Studies<a href="illustrate-D-CES.html#estimating-the-design-comparable-effect-size-for-the-single-case-studies" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="example-1-multiple-baseline-study-by-case1992improving" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Example 1: Multiple Baseline Study by <span class="citation">Case et al. (<a href="#ref-case1992Improving">1992</a>)</span><a href="illustrate-D-CES.html#example-1-multiple-baseline-study-by-case1992improving" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can estimate design-comparable effect sizes for all models suggested in Figure <a href="illustrate-D-CES.html#fig:Peltier-Asher-2020">3.4</a>, using a web-based calculator for design-comparable effect sizes (<em>scdhlm</em>; <span class="citation">Pustejovsky et al. (<a href="#ref-pustejovsky2021scdhlm">2021</a>)</span>). The scdhlm app is available at <a href="https://jepusto.shinyapps.io/scdhlm/" class="uri">https://jepusto.shinyapps.io/scdhlm/</a>. To use this app, researchers must store their dataset in an Excel file (.xlsx), comma delimited file (.csv), or text file (.txt). In addition, we recommend that users inspect their data to ensure the inclusion of the following variables: case identifier, phase identifier, session number, and the outcome. Although not required, researchers may want to arrange the data columns by order of variable appearance in the app.
We show this arrangement for the <span class="citation">Case et al. (<a href="#ref-case1992Improving">1992</a>)</span> study in Figure (fig:Excel-Case-1992). There, we demonstrate the following order of column headers with case identifier representing data in the first column, followed by variables in this order (left-to-right): phase identifier, session number, and the outcomes in the fourth column. Researchers have the flexibility to use any labeling scheme that clearly distinguishes between baseline and intervention conditions. For example, for the phase identifier, one can use <span class="math inline">\(b\)</span> or 0 to indicate baseline observations and <span class="math inline">\(i\)</span> or 1 to indicate intervention observations. However, the app requires that numerical values be used for both session number and outcome. Finally, we recommend that users arrange the data first by case (i.e., enter all the rows of data for the first case before any of the rows of data for the second case) and then by session number.</p>
<div class="figure"><span style="display:block;" id="fig:Excel-Case-1992"></span>
<img src="images/excel_Case1992.jpeg" alt="Illustration of Treatment Effect for Asher (Peltier et al., 2020)" width="75%" height="75%" />
<p class="caption">
Figure 3.5: Illustration of Treatment Effect for Asher (Peltier et al., 2020)
</p>
</div>
<p>After starting the app, we use the Load tab to load the data file, as illustrated in Figure (fig:Load-Case-1992). As mentioned previously, the data file can be a .txt or .csv file that includes one dataset, or an Excel (.xlsx) file that has either one (e.g., a data set for one study) or multiple spreadsheets (one spreadsheet for each of several studies). If using a .xlsx file with multiple spreadsheets, the <em>scdhlm</em> app allows us to select the spreadsheet containing the data for the study of interest from the <em>Load</em> tab. Then, we use the drop-down menus on the right of the screen to indicate the study design (<em>treatment reversal</em> versus <em>Multiple Baseline/Multiple Probe across participants</em>) and which variables in the data set correspond to the case identifier, phase identifier, session, and outcome (see Figure (fig:Load-Case-1992)).</p>
<div class="figure"><span style="display:block;" id="fig:Load-Case-1992"></span>
<img src="images/app.load_Case1992.jpeg" alt="Between-Case Standardized Mean Difference Estimator (scdhlm, v. 0.6.0) Load Tab" width="75%" />
<p class="caption">
Figure 3.6: Between-Case Standardized Mean Difference Estimator (scdhlm, v. 0.6.0) Load Tab
</p>
</div>
<p>After loading our data, we use the <em>Inspect</em> tab to ensure the accurate import of raw data into the app and assigned variable names are accurate (Figure (fig:Inspect-Case-1992)). In addition, we can use the <em>Inspect</em> tab to view a graph of the data (Figure (fig:Graph-Case-1992)). We recommend that researchers compare these data with the graphed data from the original studies to ensure accuracy in uploading the data and specifying the design and variable names on the <em>Load</em> tab. Check the graphed data again for consistency with the (tentatively) selected model for the design-comparable effect size.</p>
<div class="figure"><span style="display:block;" id="fig:Inspect-Case-1992"></span>
<img src="images/app.inspect.data_Case1992.jpeg" alt="Between-Case Standardized Mean Difference Estimator (scdhlm, v. 0.6.0) Data Tab within Inspect Tab for Case et al. (1992)" width="75%" height="75%" />
<p class="caption">
Figure 3.7: Between-Case Standardized Mean Difference Estimator (scdhlm, v. 0.6.0) Data Tab within Inspect Tab for Case et al. (1992)
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:Graph-Case-1992"></span>
<img src="images/app.inspect.graph_Case1992.jpeg" alt="Between-Case Standardized Mean Difference Estimator (scdhlm, v. 0.6.0) Graph Tab Within Inspect Tab for Case et al. (1992)" width="75%" />
<p class="caption">
Figure 3.8: Between-Case Standardized Mean Difference Estimator (scdhlm, v. 0.6.0) Graph Tab Within Inspect Tab for Case et al. (1992)
</p>
</div>
<p>After inspecting the data, we next specify the model for the design-comparable effect size using the <em>Model</em> tab. Figure <a href="illustrate-D-CES.html#fig:Model2-Case-1992">3.9</a> shows our specification for Model 2 (i.e., the model that assumes no trends and an effect that varies across cases). The specification process begins with the selection of trend type for the baseline phase. For this example, under <em>Type of time trend</em>, we select <em>level</em> because we assume no time trends in the baseline phases. Then, we opt to include <em>level</em> as a fixed effect and check the box to enable the model estimation of the average baseline level (i.e., <span class="math inline">\(\gamma_{00}\)</span> from Equation <a href="illustrate-D-CES.html#eq:M2-intercept">(3.6)</a>.
<!-- MC: I corrected the Equation number. James, please double check.-->
We also include <em>level</em> as a random effect, so that the baseline level can vary from case to case. Focusing on the treatment phase next, we select <em>change in level</em> as the <em>Type of time trend</em>, because we assume that the treatment will only change the level of the outcome (not trend) and include <em>change in level</em> as a fixed effect. As a fixed effect, we can obtain an estimate of the average shift in level between baseline and treatment phases (i.e., <span class="math inline">\(\gamma_{10}\)</span> from Equation <a href="illustrate-D-CES.html#eq:M2-slope">(3.7)</a>.
<!-- MC: I corrected the Equation number. James, please double check.-->
Finally, we choose to include <em>change in level</em> as a random effect to allow the change in level (i.e., treatment effect) to vary from case to case. Note the <em>scdhlm</em> app allows us to make different potential assumptions about the correlation structure of the session-level errors. Shown in Figure <a href="illustrate-D-CES.html#fig:Model2-Case-1992">3.9</a> are the default options of auto-regautoregressive and constant variance across phases. These defaults match the model presented in Equation <a href="illustrate-D-CES.html#eq:M1M2-L1">(3.1)</a>. At this point, our model for the design-comparable effect size matches Model 2 from Figure <a href="D-CES.html#fig:DC-ES-flow-chart">2.4</a>.</p>
<p>At the bottom of the screen, the <em>scdhlm</em> app provides a graph of the data with trend lines based on the specified model. We recommend that users inspect this graph to ensure that the trend lines fit the data reasonably well. If the trend lines do not fit the data well, a question about model selection arises. For example, we find the baseline trend line for Abernathy is relatively high compared to their actual baseline observations. However, other model specifications (e.g., Model 1 shown later) did not improve the fit of Abernathy’s baseline trend line. So, we decide to proceed with estimation because most other trend lines look appropriate, and the model is consistent with our a priori expectations. Throughout this process, researchers should maintain a high standard for transparency in decision-making when reporting methods and results.</p>
<div class="figure"><span style="display:block;" id="fig:Model2-Case-1992"></span>
<img src="images/app.model.model2_Case1992.png" alt="Between-Case Standardized Mean Difference Estimator (scdhlm, v. 0.6.0) Model Tab Showing Model 2 Specification for Case et al. (1992)" width="75%" />
<p class="caption">
Figure 3.9: Between-Case Standardized Mean Difference Estimator (scdhlm, v. 0.6.0) Model Tab Showing Model 2 Specification for Case et al. (1992)
</p>
</div>
<p>For the <span class="citation">Case et al. (<a href="#ref-case1992Improving">1992</a>)</span> data set, the a priori-identified Model 2 provides trajectories that fit the data reasonably well<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. Thus, we proceed to the <em>Effect size</em> tab. As shown in Figure <a href="illustrate-D-CES.html#fig:ES-Model2-Case-1992">3.10</a>, the estimated design-comparable effect size for this study is 2.57 with a standard error (SE) of 0.45 and <span class="math inline">\(95\%\)</span> confidence interval (CI) [1.65, 3.50].</p>
<p>Additional information reported on the <em>Effect size</em> tab include estimates of other quantities from the model, information about the model specification, and the assumed time-points used in calculating the design-comparable effect size. The reported degrees of freedom are used in making a small-sample correction to the effect size estimate, analogous to the Hedges’ <em>g</em> correction used with group designs <span class="citation">(<a href="#ref-Hedges1981distribution">Hedges, 1981</a>)</span>. Larger estimated degrees of freedom imply more precision in estimating the denominator of the design-comparable effect size, making the small-sample correction less consequential. Conversely, smaller degrees of freedom are indicative of imprecise design-comparable effect size denominator estimation, making the small-sample correction more consequential. The <em>Effect size</em> tab, shown in <a href="illustrate-D-CES.html#fig:ES-Model2-Case-1992">3.10</a>, also reports autocorrelation, which is the estimate of the correlation between level-1 errors of the model for the same case, differing by one time point (i.e., session) based on a first-order autoregressive model. Given the selected follow-up time, the reported intra-class correlation is an estimate of the between-case variance of the outcome as a proportion of the total variation in the outcome (including both between-case and within-case variance). Larger values of the intra-class correlation indicate that more of the variation in the outcome is between participants. The remaining output information (<em>Study design</em>, <em>Estimation method</em>, <em>Baseline specification</em>, <em>Treatment specification</em>, <em>Initial treatment time</em>, <em>Follow-up time</em>) describe the model specification and assumptions used in the effect size calculations. The <em>scdhlm</em> app includes such to allow for reproducibility of the calculations.</p>
<div class="figure"><span style="display:block;" id="fig:ES-Model2-Case-1992"></span>
<img src="images/app.ES.model2_Case1992.jpeg" alt="Between-Case Standardized Mean Difference Estimator (scdhlm, v. 0.6.0) Effect size Tab Showing Model 2 Estimate for Case et al. (1992)" width="75%" />
<p class="caption">
Figure 3.10: Between-Case Standardized Mean Difference Estimator (scdhlm, v. 0.6.0) Effect size Tab Showing Model 2 Estimate for Case et al. (1992)
</p>
</div>
<p>If our a priori arguments for selecting Model 2 were weak, or we wanted to consider Model 1, we can easily compute the results of a different model by going back to the <em>Model</em> tab and changing our specification. The only difference is that Model 1 does not include <em>change in level</em> as a random effect for the treatment phase. For illustration purposes, we present Model 1 specification results for this study in Figure <a href="illustrate-D-CES.html#fig:Model1-Case-1992">3.11</a>. As before, we keep the <em>level</em> option selected as the <em>Type of time trend</em> for the baseline phase, as both a fixed effect and a random effect. In the treatment phase, we keep <em>change in level</em> as the <em>Type of time trend</em> but select only <em>change in level</em> as a fixed effect. The Model 1 trend lines fit similarly to those from Model 2; the Model 1 design-comparable effect size is 2.59 with an SE of 0.41 and <span class="math inline">\(95\%\)</span> CI [1.76, 3.42]. This suggests that the effect size estimate is not sensitive to our decision about whether the treatment effect varies across cases. Despite this information, we proceed with the Model 2 estimate because it is consistent with our expectations for the research in this area.</p>
<div class="figure"><span style="display:block;" id="fig:Model1-Case-1992"></span>
<img src="images/app.model.model1_Case1992.png" alt="Between-Case Standardized Mean Difference Estimator (scdhlm, v. 0.6.0) Model Tab Showing Model 1 Specification for Case et al. (1992)" width="75%" height="75%" />
<p class="caption">
Figure 3.11: Between-Case Standardized Mean Difference Estimator (scdhlm, v. 0.6.0) Model Tab Showing Model 1 Specification for Case et al. (1992)
</p>
</div>
</div>
<div id="example-2-multiple-baseline-study-by-peltier2020effects" class="section level3 hasAnchor" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Example 2: Multiple Baseline Study by <span class="citation">Peltier et al. (<a href="#ref-peltier2020Effects">2020</a>)</span><a href="illustrate-D-CES.html#example-2-multiple-baseline-study-by-peltier2020effects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now have a design-comparable effect size for the first single-case study by <span class="citation">Case et al. (<a href="#ref-case1992Improving">1992</a>)</span>. Next, we repeat these steps for all other SCD studies in our synthesis. For this illustration, we include a second multiple baseline study <span class="citation">(<a href="#ref-peltier2020Effects">Peltier et al., 2020</a>)</span>. For this second study, we ran through the same sequence of steps:
1. load the data;
2. inspect the data in both tabular and graphic form;
3. specify our selected model for the data (i.e., Model 2 for this illustration); and
4. estimate the design-comparable effect size.</p>
<p>After performing these steps, we found that the estimated model trajectories for the Model 2 specification fit the <span class="citation">Peltier et al. (<a href="#ref-peltier2020Effects">2020</a>)</span> data well, as shown in Figure <a href="illustrate-D-CES.html#fig:Model2-Peltier-2020">3.12</a>. For the <span class="citation">Peltier et al. (<a href="#ref-peltier2020Effects">2020</a>)</span> study, the design-comparable effect size is 2.95 with an SE of 0.28 and <span class="math inline">\(95\%\)</span> CI [2.42, 3.53]. We then estimated the effect for Model 1 (i.e., we remove the check next to <em>change in level</em> under <em>include random effect</em>), which is the more restrictive model that does not allow the treatment effect to vary across cases. Model 1 produced a design-comparable effect size estimate of 2.83 with an SE of 0.24 and <span class="math inline">\(95\%\)</span> CI [2.34, 3.31], like the Model 2 design-comparable effect size.</p>
<div class="figure"><span style="display:block;" id="fig:Model2-Peltier-2020"></span>
<img src="images/app.model.model2_Peltier2020.png" alt="Between-Case Standardized Mean Difference Estimator (scdhlm, v. 0.6.0) Model 2 Specification for Peltier et al. (2020)" width="75%" height="75%" />
<p class="caption">
Figure 3.12: Between-Case Standardized Mean Difference Estimator (scdhlm, v. 0.6.0) Model 2 Specification for Peltier et al. (2020)
</p>
</div>
</div>
<div id="example-3-replicated-abab-design-by-lambert2006effects" class="section level3 hasAnchor" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Example 3: Replicated ABAB Design by <span class="citation">Lambert et al. (<a href="#ref-lambert2006effects">2006</a>)</span><a href="illustrate-D-CES.html#example-3-replicated-abab-design-by-lambert2006effects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For the third SCD study, we illustrate use of the <em>scdhlm</em> app using data from a replicated ABAB design by <span class="citation">Lambert et al. (<a href="#ref-lambert2006effects">2006</a>)</span>. As with the previous two SCD studies, we load our spreadsheet containing the study data in the usual manner. However, unlike <span class="citation">Case et al. (<a href="#ref-case1992Improving">1992</a>)</span> and <span class="citation">Peltier et al. (<a href="#ref-peltier2020Effects">2020</a>)</span>, the <span class="citation">Lambert et al. (<a href="#ref-lambert2006effects">2006</a>)</span> study does not utilize a multiple baseline design. Therefore, on the right side of the <em>Load</em> tab, using the drop-down menu under <em>Please specify the study design</em>, we must select <em>Treatment Reversal</em> (as opposed to <em>Multiple baseline/Multiple probe across participants</em>). After loading the data, we again use the <em>Inspect</em> tab to visually inspect the data in both tabular and graphic form. Then using the <em>Model</em> tab (shown in Figure <a href="illustrate-D-CES.html#fig:Model2-Lambert-2006">3.13</a>), we continue to specify Model 2 by checking the option under the <em>Baseline phase</em> to include <em>level</em> as both a fixed effect and a random effect and checking under the <em>Treatment phase</em> to include <em>change in level</em> as both a fixed effect and a random effect. Users of the app will note that for reversal designs, there is no drop-down menu from which they can potentially add trends (i.e., <em>level</em> is the only option for baseline specification, and <em>change in level</em> is the only option for treatment phase specification). This makes specification using the <em>Model</em> tab simpler than in our previous examples of multiple baseline studies, although it does also constrain the user’s ability to specify a well-fitting model. On the <em>Model</em> tab, we see that the fitted trajectories fit these data well.</p>
<p>After specifying the model, we view the Effect size tab to obtain the design-comparable effect size (see Figure <a href="illustrate-D-CES.html#fig:Model2-Lambert-2006">3.13</a>). For <span class="citation">Lambert et al. (<a href="#ref-lambert2006effects">2006</a>)</span>, the estimated Model 2 design-comparable effect size is 6.37 with an SE of 0.39 and <span class="math inline">\(95\%\)</span> CI [5.60, 7.14]. Like the previous SCD studies, we also estimate the effect for Model 1, which is the more restrictive model that does not allow the treatment effect to vary across cases. Model 1 produces an estimate of 6.34 with an SE of 0.37 and <span class="math inline">\(95\%\)</span> CI [5.61, 7.08], a similar result to the Model 2 design-comparable effect size.</p>
<div class="figure"><span style="display:block;" id="fig:Model2-Lambert-2006"></span>
<img src="images/app.model.model2_Lambert2006.png" alt="Between-Case Standardized Mean Difference Estimator (scdhlm, v. 0.6.0) Model 2 Specification for Lambert et al. (2006)" width="75%" height="75%" />
<p class="caption">
Figure 3.13: Between-Case Standardized Mean Difference Estimator (scdhlm, v. 0.6.0) Model 2 Specification for Lambert et al. (2006)
</p>
</div>
</div>
</div>
<div id="estimating-the-design-comparable-effect-size-for-the-group-studies" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Estimating the Design-Comparable Effect Size for the Group Studies<a href="illustrate-D-CES.html#estimating-the-design-comparable-effect-size-for-the-group-studies" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>After estimating the design-comparable effect size for each SCD study, we turn our attention to estimating the design-comparable effect size for each group study. <span class="citation">Hutchinson (<a href="#ref-hutchinson1993Effects">1993</a>)</span> used random assignment of individual students with learning disabilities to either the intervention (<span class="math inline">\(n = 12\)</span>) or control (<span class="math inline">\(n = 8\)</span>) conditions. After intervention, both groups of students completed 25 math word problems selected from the <em>British Columbia Mathematics Achievement Test</em>. Because details for estimating standardized mean difference effect sizes from group studies are readily available from a variety of sources including chapters <span class="citation">(<a href="#ref-Borenstein2019effect">Borenstein, 2019</a>)</span>, books <span class="citation">(e.g., <a href="#ref-borenstein2021introduction">Borenstein et al., 2021</a>)</span> and journal articles <span class="citation">(<a href="#ref-Hedges1981distribution">Hedges, 1981</a>; e.g., <a href="#ref-Hedges2007effect">Hedges, 2007</a>)</span>, we do not model the calculation procedures and simply report the results. Using the <span class="citation">Hutchinson (<a href="#ref-hutchinson1993Effects">1993</a>)</span> group design study summary statistics at posttest, we calculate a standardized mean difference in math word problem solving as 0.71, with an SE of 0.45 based on Hedges’ <em>g</em> to correct for small-sample bias.</p>
</div>
<div id="analyzing-the-effect-sizes" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Analyzing the Effect Sizes<a href="illustrate-D-CES.html#analyzing-the-effect-sizes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>After we have obtained effect sizes from each single-case and group study, we can proceed with synthesizing the effect sizes. Depending on our synthesis goals, we have a variety of tools and approaches available. We can: (a) create graphical displays (e.g., forest plots) to show the effect size for each study along with their confidence interval, (b) average the effect sizes and create a confidence interval for the overall average effect, (c) estimate the extent of variation in effects across studies, (d) examine the effect sizes for evidence of publication bias, and (e) explore potential moderators of the effect. Since the use of design-comparable effect sizes for the SCD studies produce estimates having the same metric as the commonly used standardized mean difference effect sizes from group studies, researchers can accomplish these goals (e.g., averaging the effect sizes or running a moderator analysis) using methods developed for group studies. Details on these methods are readily available elsewhere <span class="citation">(e.g., <a href="#ref-borenstein2021introduction">Borenstein et al., 2021</a>; <a href="#ref-cooper2019handbook">H. Cooper et al., 2019</a>)</span>. We illustrate the averaging of the effect sizes from our studies here using a fixed effect meta-analysis, so that this illustration is consistent with the approach used in What Works Clearinghouse intervention reports <span class="citation">(<a href="#ref-whatworksclearinghouse2020What">What Works Clearinghouse, 2020</a>)</span>.</p>
<p>Table <a href="illustrate-D-CES.html#tab:ES-est">3.1</a> reports the effect size estimates, SEs, and fixed effect meta-analysis calculations for our four included studies. In fixed effect meta-analysis, the overall average effect size estimate is a weighted average of the effect size estimates from the individual studies, with weights proportional to the inverse of the sampling variance (squared SE) of each effect size estimate. Further, the SE of the overall effect size is the square root of the inverse of the total weight. In Table <a href="illustrate-D-CES.html#tab:ES-est">3.1</a>, column C reports the inverse variance weight assigned to each study, with the percentage of the total weight listed in parentheses. For instance, the effect size estimate from <span class="citation">Peltier et al. (<a href="#ref-peltier2020Effects">2020</a>)</span> receives 43.7% of the total weight, while the effect size estimates from <span class="citation">Case et al. (<a href="#ref-case1992Improving">1992</a>)</span> and <span class="citation">Hutchinson (<a href="#ref-hutchinson1993Effects">1993</a>)</span> each receive 16.9% of the total weight. The total inverse variance weight is 29.21. The overall average effect size estimate is 3.28 with an SE of 0.18 and an approximate <span class="math inline">\(95\%\)</span> CI [2.91, 3.64]. The <em>Q</em>-test for heterogeneity is highly significant, Q(3) = 99.3, p &lt; .0001, indicating that the included effect size estimates are more variable than we would expect due to sampling error alone. In other words, it is unlikely that we would observe such a wide dispersion of effect size estimates if the studies were all estimating the same true effect size parameter.</p>
<table class=" lightable-classic" style="font-family: &quot;Arial Narrow&quot;, &quot;Source Sans Pro&quot;, sans-serif; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:ES-est">Table 3.1: </span>Fixed Effect Meta-Analysis Calculations for
Example Math Invervention Studies
</caption>
<thead>
<tr>
<th style="text-align:left;">
Study
</th>
<th style="text-align:center;">
Effect Size Estimate (A)
</th>
<th style="text-align:center;">
Standard Error (B)
</th>
<th style="text-align:center;">
Inverse-variance Weight (%) (C)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Case et al. (1992)
</td>
<td style="text-align:center;">
2.57
</td>
<td style="text-align:center;">
0.45
</td>
<td style="text-align:center;">
4.94 (16.9)
</td>
</tr>
<tr>
<td style="text-align:left;">
Peltier et al. (2020)
</td>
<td style="text-align:center;">
2.95
</td>
<td style="text-align:center;">
0.28
</td>
<td style="text-align:center;">
12.76 (43.7)
</td>
</tr>
<tr>
<td style="text-align:left;">
Lambert et al. (2006)
</td>
<td style="text-align:center;">
6.37
</td>
<td style="text-align:center;">
0.39
</td>
<td style="text-align:center;">
6.57 (22.5)
</td>
</tr>
<tr>
<td style="text-align:left;border-bottom: 1px solid;">
Hutchinson (1993)
</td>
<td style="text-align:center;border-bottom: 1px solid;">
0.71
</td>
<td style="text-align:center;border-bottom: 1px solid;">
0.45
</td>
<td style="text-align:center;border-bottom: 1px solid;">
4.94 (16.9)
</td>
</tr>
<tr>
<td style="text-align:left;">
Fixed effect meta-analysis
</td>
<td style="text-align:center;">
3.28
</td>
<td style="text-align:center;">
0.19
</td>
<td style="text-align:center;">
29.21 (100)
</td>
</tr>
</tbody>
</table>
<p>In fixed effect meta-analysis, the overall average effect size estimate is a summary of the effect size estimates across the included studies, where studies are treated as fixed. Therefore, the SE and CI in fixed effect meta-analysis account for the uncertainty in the process of effect size estimation that occurs in each of the individual studies. However, they do not account for uncertainty in the process of identifying studies for inclusion in the meta-analysis <span class="citation">(<a href="#ref-konstantopoulos2019statistically">Konstantopoulos &amp; Hedges, 2019</a>; <a href="#ref-Rice_Higgins_Lumley_2018">Rice et al., 2018</a>)</span>, nor do they provide a basis for generalization beyond the included studies. When conducting syntheses of larger bodies of literature—and especially of studies with heterogeneous populations, design features, or dependent effect sizes—researchers will often prefer to use random effects models <span class="citation">(<a href="#ref-Hedges_Vevea_1998">Hedges &amp; Vevea, 1998</a>)</span> or their further extensions <span class="citation">(<a href="#ref-PustejovskyTipton2021">Pustejovsky &amp; Tipton, 2021</a>; <a href="#ref-van2013three">Van den Noortgate et al., 2013</a>)</span>.</p>
<p>Of the studies we use as illustrative examples in this chapter, the dependent variable of the group study <span class="citation">(<a href="#ref-hutchinson1993Effects">Hutchinson, 1993</a>)</span> was the broadest. Conversely, the <span class="citation">Case et al. (<a href="#ref-case1992Improving">1992</a>)</span> and <span class="citation">Peltier et al. (<a href="#ref-peltier2020Effects">2020</a>)</span> MB studies used more narrowly defined dependent variables. Finally, we note the outcome from the replicated ABAB design <span class="citation">(<a href="#ref-lambert2006effects">Lambert et al., 2006</a>)</span> included a behavioral component (i.e., it is not purely academic). In a synthesis of many studies, researchers might use moderator analysis (i.e., meta-regression analysis) to explore the extent to which variation in effect size is related to dependent variable characteristics or other study features. Methods for conducting such moderator analysis are described elsewhere <span class="citation">(<a href="#ref-borenstein2021introduction">Borenstein et al., 2021, Chapters 19–21</a>; <a href="#ref-konstantopoulos2019statistically">Konstantopoulos &amp; Hedges, 2019</a>)</span>.</p>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-Borenstein2019effect" class="csl-entry">
Borenstein, M. (2019). Effect sizes for continuous data. In H. M. Cooper, L. V. Hedges, &amp; J. C. Valentine (Eds.), <em>The <span>Handbook</span> of <span>Research Synthesis</span> and <span>Meta-Analysis</span></em>. <span>Russell Sage Foundation</span>.
</div>
<div id="ref-borenstein2021introduction" class="csl-entry">
Borenstein, M., Hedges, L. V., Higgins, J. P. T., &amp; Rothstein, H. R. (2021). <em>Introduction to <span>Meta-Analysis</span></em>. <span>John Wiley &amp; Sons, Ltd</span>.
</div>
<div id="ref-case1992Improving" class="csl-entry">
Case, L. P., Harris, K. R., &amp; Graham, S. (1992). Improving the <span>Mathematical Problem-Solving Skills</span> of <span>Students</span> with <span>Learning Disabilities</span>: <span>Self-Regulated Strategy Development</span>. <em>The Journal of Special Education</em>, <em>26</em>(1), 1–19. <a href="https://doi.org/10.1177/002246699202600101">https://doi.org/10.1177/002246699202600101</a>
</div>
<div id="ref-cooper2019handbook" class="csl-entry">
Cooper, H., Hedges, L. V., &amp; Valentine, J. C. (2019). <em>The handbook of research synthesis and meta-analysis</em>. Russell Sage Foundation.
</div>
<div id="ref-Hedges1981distribution" class="csl-entry">
Hedges, L. V. (1981). Distribution theory for <span>Glass</span>’s estimator of effect size and related estimators. <em>Journal of Educational Statistics</em>, <em>6</em>(2), 107–128.
</div>
<div id="ref-Hedges2007effect" class="csl-entry">
Hedges, L. V. (2007). Effect sizes in cluster-randomized designs. <em>Journal of Educational and Behavioral Statistics</em>, <em>32</em>(4), 341–370. <a href="https://doi.org/10.3102/1076998606298043">https://doi.org/10.3102/1076998606298043</a>
</div>
<div id="ref-Hedges_Vevea_1998" class="csl-entry">
Hedges, L. V., &amp; Vevea, J. L. (1998). Fixed- and random-effects models in meta-analysis. <em>Psychological Methods</em>, <em>3</em>(4), 486–504. <a href="https://doi.org/10.1037/1082-989X.3.4.486">https://doi.org/10.1037/1082-989X.3.4.486</a>
</div>
<div id="ref-hutchinson1993Effects" class="csl-entry">
Hutchinson, N. L. (1993). Effects of <span>Cognitive Strategy Instruction</span> on <span>Algebra Problem Solving</span> of <span>Adolescents</span> with <span>Learning Disabilities</span>. <em>Learning Disability Quarterly</em>, <em>16</em>(1), 34–63. <a href="https://doi.org/10.2307/1511158">https://doi.org/10.2307/1511158</a>
</div>
<div id="ref-konstantopoulos2019statistically" class="csl-entry">
Konstantopoulos, S., &amp; Hedges, L. V. (2019). Statistically analyzing effect sizes: Fixed-and random-effects models. In H. Cooper Harris &amp; J. C. Valentine (Eds.), <em>The handbook of research synthesis and meta-analysis</em> (3rd Edition, pp. 246–280). Russell Sage Foundation.
</div>
<div id="ref-lambert2006effects" class="csl-entry">
Lambert, M. C., Cartledge, G., Heward, W. L., &amp; Lo, Y. (2006). Effects of response cards on disruptive behavior and academic responding during math lessons by fourth-grade urban students. <em>Journal of Positive Behavior Interventions</em>, <em>8</em>(2), 88.
</div>
<div id="ref-peltier2020Effects" class="csl-entry">
Peltier, C., Sinclair, T. E., Pulos, J. M., &amp; Suk, A. (2020). Effects of <span>Schema-Based Instruction</span> on <span>Immediate</span>, <span>Generalized</span>, and <span>Combined Structured Word Problems</span>. <em>The Journal of Special Education</em>, <em>54</em>(2), 101–112. <a href="https://doi.org/10.1177/0022466919883397">https://doi.org/10.1177/0022466919883397</a>
</div>
<div id="ref-pustejovsky2021scdhlm" class="csl-entry">
Pustejovsky, J. E., Chen, M., &amp; Hamilton, B. (2021). <em>Scdhlm: <span>A</span> web-based calculator for between-case standardized mean differences</em>.
</div>
<div id="ref-Pustejovsky2014design" class="csl-entry">
Pustejovsky, J. E., Hedges, L. V., &amp; Shadish, W. R. (2014). Design-comparable effect sizes in multiple baseline designs: <span>A</span> general modeling framework. <em>Journal of Educational and Behavioral Statistics</em>, <em>39</em>(5), 368–393. <a href="https://doi.org/10.3102/1076998614547577">https://doi.org/10.3102/1076998614547577</a>
</div>
<div id="ref-PustejovskyTipton2021" class="csl-entry">
Pustejovsky, J. E., &amp; Tipton, E. (2021). Meta-analysis with robust variance estimation: Expanding the range of working models. <em>Prevention Science</em>. <a href="https://doi.org/10.1007/s11121-021-01246-3">https://doi.org/10.1007/s11121-021-01246-3</a>
</div>
<div id="ref-Rice_Higgins_Lumley_2018" class="csl-entry">
Rice, K., Higgins, J. P. T., &amp; Lumley, T. (2018). A re-evaluation of fixed effect(s) meta-analysis. <em>Journal of the Royal Statistical Society Series A: Statistics in Society</em>, <em>181</em>(1), 205–227. <a href="https://doi.org/10.1111/rssa.12275">https://doi.org/10.1111/rssa.12275</a>
</div>
<div id="ref-van2013three" class="csl-entry">
Van den Noortgate, W., López-López, J. A., Marı́n-Martı́nez, F., &amp; Sánchez-Meca, J. (2013). Three-level meta-analysis of dependent effect sizes. <em>Behavior Research Methods</em>, <em>45</em>, 576–594. <a href="https://doi.org/10.3758/s13428-012-0261-6">https://doi.org/10.3758/s13428-012-0261-6</a>
</div>
<div id="ref-whatworksclearinghouse2020What" class="csl-entry">
What Works Clearinghouse. (2020). <em>What <span>Works Clearinghouse Standards Handbook</span></em> (Version 4.1). <span>U.S. Department of Education, Institute of Education Sciences, National Center for Education Evaluation and Regional Assistance.</span>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Meta-analysis must specify their criteria for “reasonably well”.<a href="illustrate-D-CES.html#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="D-CES.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="illustrate-D-CES-Ttrends.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["SCD-Methods-Guide.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
