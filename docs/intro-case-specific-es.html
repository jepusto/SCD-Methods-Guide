<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Introduction to Case-Specific Effect Sizes | Methods Guide for Effect Estimation and Synthesis of Single-Case Studies</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Introduction to Case-Specific Effect Sizes | Methods Guide for Effect Estimation and Synthesis of Single-Case Studies" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="github-repo" content="jepusto/SCD-Methods-Guide" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Introduction to Case-Specific Effect Sizes | Methods Guide for Effect Estimation and Synthesis of Single-Case Studies" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  



<meta name="date" content="2024-01-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="MLM-Trend.html"/>
<link rel="next" href="app-case-specific-es.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Synthesis of Single-Case Designs</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Authors</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#disclaimer"><i class="fa fa-check"></i>Disclaimer</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#citation"><i class="fa fa-check"></i>Citation</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#preface"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#datasets"><i class="fa fa-check"></i>Datasets</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Approaches for Estimation and Synthesis of Single-Case Studies</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#background"><i class="fa fa-check"></i><b>1.1</b> Background</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#purpose-of-the-methods-guide"><i class="fa fa-check"></i><b>1.2</b> Purpose of the Methods Guide</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#study-quality"><i class="fa fa-check"></i><b>1.3</b> Study Quality</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#selecting-an-approach-for-effect-estimation-and-synthesis"><i class="fa fa-check"></i><b>1.4</b> Selecting an Approach for Effect Estimation and Synthesis</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#design-comparable-effect-sizes"><i class="fa fa-check"></i><b>1.5</b> Design-Comparable Effect Sizes</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#case-specific-effect-sizes"><i class="fa fa-check"></i><b>1.6</b> Case-Specific Effect Sizes</a></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#multilevel-modeling-of-individual-participant-interrupted-time-series-data"><i class="fa fa-check"></i><b>1.7</b> Multilevel Modeling of Individual Participant Interrupted Time-Series Data</a></li>
<li class="chapter" data-level="1.8" data-path="intro.html"><a href="intro.html#summary-of-options-for-effect-estimation-and-synthesis"><i class="fa fa-check"></i><b>1.8</b> Summary of Options for Effect Estimation and Synthesis</a></li>
<li class="chapter" data-level="1.9" data-path="intro.html"><a href="intro.html#limitations-in-selecting-an-approach-for-effect-estimation-and-synthesis"><i class="fa fa-check"></i><b>1.9</b> Limitations in Selecting an Approach for Effect Estimation and Synthesis</a></li>
<li class="chapter" data-level="1.10" data-path="intro.html"><a href="intro.html#structure-of-the-methods-guide"><i class="fa fa-check"></i><b>1.10</b> Structure of the Methods Guide</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="D-CES.html"><a href="D-CES.html"><i class="fa fa-check"></i><b>2</b> Introduction to Design-Comparable Effect Sizes</a>
<ul>
<li class="chapter" data-level="2.1" data-path="D-CES.html"><a href="D-CES.html#background-1"><i class="fa fa-check"></i><b>2.1</b> Background</a></li>
<li class="chapter" data-level="2.2" data-path="D-CES.html"><a href="D-CES.html#when-to-use-design-comparable-effect-sizes"><i class="fa fa-check"></i><b>2.2</b> When to Use Design-Comparable Effect Sizes</a></li>
<li class="chapter" data-level="2.3" data-path="D-CES.html"><a href="D-CES.html#general-definition-of-design-comparable-effect-sizes"><i class="fa fa-check"></i><b>2.3</b> General Definition of Design-Comparable Effect Sizes</a></li>
<li class="chapter" data-level="2.4" data-path="D-CES.html"><a href="D-CES.html#what-we-assume-when-we-synthesize-design-comparable-effect-sizes"><i class="fa fa-check"></i><b>2.4</b> What We Assume When We Synthesize Design-Comparable Effect Sizes</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="D-CES.html"><a href="D-CES.html#what-we-assume-when-we-estimate-design-comparable-effect-size"><i class="fa fa-check"></i><b>2.4.1</b> What We Assume When We Estimate Design-Comparable Effect Size</a></li>
<li class="chapter" data-level="2.4.2" data-path="D-CES.html"><a href="D-CES.html#normality"><i class="fa fa-check"></i><b>2.4.2</b> Normality</a></li>
<li class="chapter" data-level="2.4.3" data-path="D-CES.html"><a href="D-CES.html#homogeneity-of-variance"><i class="fa fa-check"></i><b>2.4.3</b> Homogeneity of Variance</a></li>
<li class="chapter" data-level="2.4.4" data-path="D-CES.html"><a href="D-CES.html#appropriate-structural-model"><i class="fa fa-check"></i><b>2.4.4</b> Appropriate Structural Model</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="D-CES.html"><a href="D-CES.html#modeling-options-for-design-comparable-effect-size-estimation"><i class="fa fa-check"></i><b>2.5</b> Modeling Options for Design-comparable Effect Size Estimation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="illustrate-D-CES.html"><a href="illustrate-D-CES.html"><i class="fa fa-check"></i><b>3</b> Illustration of Design-Comparable Effect Sizes When No Trends Are Assumed</a>
<ul>
<li class="chapter" data-level="3.1" data-path="illustrate-D-CES.html"><a href="illustrate-D-CES.html#selecting-a-design-comparable-effect-size-for-the-single-case-studies"><i class="fa fa-check"></i><b>3.1</b> Selecting a Design-Comparable Effect Size for the Single-Case Studies</a></li>
<li class="chapter" data-level="3.2" data-path="illustrate-D-CES.html"><a href="illustrate-D-CES.html#details-of-the-no-trend-models-for-design-comparable-effect-sizes"><i class="fa fa-check"></i><b>3.2</b> Details of the No Trend Models for Design-Comparable Effect Sizes</a></li>
<li class="chapter" data-level="3.3" data-path="illustrate-D-CES.html"><a href="illustrate-D-CES.html#estimating-the-design-comparable-effect-size-for-the-single-case-studies"><i class="fa fa-check"></i><b>3.3</b> Estimating the Design-Comparable Effect Size for the Single-Case Studies</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="illustrate-D-CES.html"><a href="illustrate-D-CES.html#example-1-multiple-baseline-study-by-case1992improving"><i class="fa fa-check"></i><b>3.3.1</b> Example 1: Multiple Baseline Study by <span class="citation">Case et al. (1992)</span></a></li>
<li class="chapter" data-level="3.3.2" data-path="illustrate-D-CES.html"><a href="illustrate-D-CES.html#example-2-multiple-baseline-study-by-peltier2020effects"><i class="fa fa-check"></i><b>3.3.2</b> Example 2: Multiple Baseline Study by <span class="citation">Peltier et al. (2020)</span></a></li>
<li class="chapter" data-level="3.3.3" data-path="illustrate-D-CES.html"><a href="illustrate-D-CES.html#example-3-replicated-abab-design-by-lambert2006effects"><i class="fa fa-check"></i><b>3.3.3</b> Example 3: Replicated ABAB Design by <span class="citation">Lambert et al. (2006)</span></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="illustrate-D-CES.html"><a href="illustrate-D-CES.html#estimating-the-design-comparable-effect-size-for-the-group-studies"><i class="fa fa-check"></i><b>3.4</b> Estimating the Design-Comparable Effect Size for the Group Studies</a></li>
<li class="chapter" data-level="3.5" data-path="illustrate-D-CES.html"><a href="illustrate-D-CES.html#analyzing-the-effect-sizes"><i class="fa fa-check"></i><b>3.5</b> Analyzing the Effect Sizes</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="illustrate-D-CES-Ttrends.html"><a href="illustrate-D-CES-Ttrends.html"><i class="fa fa-check"></i><b>4</b> Illustration of Design-Comparable Effect Sizes When Assuming Only Trends in The Treatment Phases</a>
<ul>
<li class="chapter" data-level="4.1" data-path="illustrate-D-CES-Ttrends.html"><a href="illustrate-D-CES-Ttrends.html#selecting-a-design-comparable-effect-size-for-the-single-case-studies-1"><i class="fa fa-check"></i><b>4.1</b> Selecting a Design-Comparable Effect Size for the Single-Case Studies</a></li>
<li class="chapter" data-level="4.2" data-path="illustrate-D-CES-Ttrends.html"><a href="illustrate-D-CES-Ttrends.html#details-of-the-models-for-design-comparable-effect-sizes"><i class="fa fa-check"></i><b>4.2</b> Details of the Models for Design-Comparable Effect Sizes</a></li>
<li class="chapter" data-level="4.3" data-path="illustrate-D-CES-Ttrends.html"><a href="illustrate-D-CES-Ttrends.html#estimating-the-design-comparable-effect-size-for-the-single-case-studies-1"><i class="fa fa-check"></i><b>4.3</b> Estimating the Design-Comparable Effect Size for the Single-Case Studies</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="illustrate-D-CES-Ttrends.html"><a href="illustrate-D-CES-Ttrends.html#example-1-multiple-baselise-study-by-gunning2003psychological"><i class="fa fa-check"></i><b>4.3.1</b> Example 1: Multiple Baselise Study by <span class="citation">Gunning &amp; Espie (2003)</span></a></li>
<li class="chapter" data-level="4.3.2" data-path="illustrate-D-CES-Ttrends.html"><a href="illustrate-D-CES-Ttrends.html#example-2-multiple-baseline-study-by-delemere2018parentimplemented"><i class="fa fa-check"></i><b>4.3.2</b> Example 2: Multiple Baseline Study by <span class="citation">Delemere &amp; Dounavi (2018)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="illustrate-D-CES-Ttrends.html"><a href="illustrate-D-CES-Ttrends.html#estimating-the-design-comparable-effect-size-for-the-group-studies-1"><i class="fa fa-check"></i><b>4.4</b> Estimating the Design-Comparable Effect Size for the Group Studies</a></li>
<li class="chapter" data-level="4.5" data-path="illustrate-D-CES-Ttrends.html"><a href="illustrate-D-CES-Ttrends.html#analyzing-the-effect-sizes-1"><i class="fa fa-check"></i><b>4.5</b> Analyzing the Effect Sizes</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="illustrate-D-CES-Btrends.html"><a href="illustrate-D-CES-Btrends.html"><i class="fa fa-check"></i><b>5</b> Illustration of Design-Comparable Effect Sizes When Assuming Trends in Baseline and Different Trends in Treatment</a>
<ul>
<li class="chapter" data-level="5.1" data-path="illustrate-D-CES-Btrends.html"><a href="illustrate-D-CES-Btrends.html#selecting-a-design-comparable-effect-size-for-the-single-case-studies-2"><i class="fa fa-check"></i><b>5.1</b> Selecting a Design-Comparable Effect Size for the Single-Case Studies</a></li>
<li class="chapter" data-level="5.2" data-path="illustrate-D-CES-Btrends.html"><a href="illustrate-D-CES-Btrends.html#details-of-the-models-for-design-comparable-effect-sizes-1"><i class="fa fa-check"></i><b>5.2</b> Details of the Models for Design-Comparable Effect Sizes</a></li>
<li class="chapter" data-level="5.3" data-path="illustrate-D-CES-Btrends.html"><a href="illustrate-D-CES-Btrends.html#estimating-the-design-comparable-effect-size-for-the-single-case-studies-2"><i class="fa fa-check"></i><b>5.3</b> Estimating the Design-Comparable Effect Size for the Single-Case Studies</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="illustrate-D-CES-Btrends.html"><a href="illustrate-D-CES-Btrends.html#example-1-multiple-probe-study-by-datchuk2016writing"><i class="fa fa-check"></i><b>5.3.1</b> Example 1: Multiple Probe Study by <span class="citation">Datchuk (2016)</span></a></li>
<li class="chapter" data-level="5.3.2" data-path="illustrate-D-CES-Btrends.html"><a href="illustrate-D-CES-Btrends.html#example-2-multiple-baseline-study-by-rodgers2021effects"><i class="fa fa-check"></i><b>5.3.2</b> Example 2: Multiple Baseline Study by <span class="citation">Rodgers et al. (2021)</span></a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="illustrate-D-CES-Btrends.html"><a href="illustrate-D-CES-Btrends.html#estimating-the-design-comparable-effect-size-for-the-group-study"><i class="fa fa-check"></i><b>5.4</b> Estimating the Design-Comparable Effect Size for the Group Study</a></li>
<li class="chapter" data-level="5.5" data-path="illustrate-D-CES-Btrends.html"><a href="illustrate-D-CES-Btrends.html#analyzing-the-effect-sizes-2"><i class="fa fa-check"></i><b>5.5</b> Analyzing the Effect Sizes</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="MLM-raw-data.html"><a href="MLM-raw-data.html"><i class="fa fa-check"></i><b>6</b> Introduction to Multilevel Modeling of Raw Participant Data</a>
<ul>
<li class="chapter" data-level="6.1" data-path="MLM-raw-data.html"><a href="MLM-raw-data.html#background-2"><i class="fa fa-check"></i><b>6.1</b> Background</a></li>
<li class="chapter" data-level="6.2" data-path="MLM-raw-data.html"><a href="MLM-raw-data.html#when-to-use-multilevel-models-of-the-raw-data"><i class="fa fa-check"></i><b>6.2</b> When to Use Multilevel Models of the Raw Data</a></li>
<li class="chapter" data-level="6.3" data-path="MLM-raw-data.html"><a href="MLM-raw-data.html#what-we-assume-with-multilevel-models-of-the-raw-data"><i class="fa fa-check"></i><b>6.3</b> What We Assume with Multilevel Models of the Raw Data</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="MLM-raw-data.html"><a href="MLM-raw-data.html#within-case-model-assumptions"><i class="fa fa-check"></i><b>6.3.1</b> Within-Case Model Assumptions</a></li>
<li class="chapter" data-level="6.3.2" data-path="MLM-raw-data.html"><a href="MLM-raw-data.html#case-similarity-assumptions"><i class="fa fa-check"></i><b>6.3.2</b> Case Similarity Assumptions</a></li>
<li class="chapter" data-level="6.3.3" data-path="MLM-raw-data.html"><a href="MLM-raw-data.html#study-similarity-assumptions"><i class="fa fa-check"></i><b>6.3.3</b> Study Similarity Assumptions</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="MLM-raw-data.html"><a href="MLM-raw-data.html#comparison-to-other-synthesis-approaches"><i class="fa fa-check"></i><b>6.4</b> Comparison to Other Synthesis Approaches</a></li>
<li class="chapter" data-level="6.5" data-path="MLM-raw-data.html"><a href="MLM-raw-data.html#multilevel-modeling-options-for-synthesizing-single-case-research"><i class="fa fa-check"></i><b>6.5</b> Multilevel Modeling Options for Synthesizing Single-Case Research</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="MLM-NoTrend.html"><a href="MLM-NoTrend.html"><i class="fa fa-check"></i><b>7</b> Illustration of Multilevel Modeling When No Trends Are Assumed</a>
<ul>
<li class="chapter" data-level="7.1" data-path="MLM-NoTrend.html"><a href="MLM-NoTrend.html#selecting-a-multilevel-model-for-the-single-case-studies"><i class="fa fa-check"></i><b>7.1</b> Selecting a Multilevel Model for the Single-Case Studies</a></li>
<li class="chapter" data-level="7.2" data-path="MLM-NoTrend.html"><a href="MLM-NoTrend.html#details-of-the-no-trend-multilevel-model"><i class="fa fa-check"></i><b>7.2</b> Details of the No-Trend Multilevel Model</a></li>
<li class="chapter" data-level="7.3" data-path="MLM-NoTrend.html"><a href="MLM-NoTrend.html#estimating-a-multilevel-model-for-the-behavior-specific-praise-studies"><i class="fa fa-check"></i><b>7.3</b> Estimating a Multilevel Model for the Behavior Specific Praise Studies</a></li>
<li class="chapter" data-level="7.4" data-path="MLM-NoTrend.html"><a href="MLM-NoTrend.html#appendix"><i class="fa fa-check"></i><b>7.4</b> Appendix</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="MLM-NoTrend.html"><a href="MLM-NoTrend.html#sas-code"><i class="fa fa-check"></i><b>7.4.1</b> SAS Code</a></li>
<li class="chapter" data-level="7.4.2" data-path="MLM-NoTrend.html"><a href="MLM-NoTrend.html#r-code"><i class="fa fa-check"></i><b>7.4.2</b> R Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="MLM-Trend.html"><a href="MLM-Trend.html"><i class="fa fa-check"></i><b>8</b> Illustration of Multilevel Modeling When No Trends Are Assumed</a>
<ul>
<li class="chapter" data-level="8.1" data-path="MLM-Trend.html"><a href="MLM-Trend.html#selecting-a-multilevel-model-for-the-single-case-studies-1"><i class="fa fa-check"></i><b>8.1</b> Selecting a Multilevel Model for the Single-Case Studies</a></li>
<li class="chapter" data-level="8.2" data-path="MLM-Trend.html"><a href="MLM-Trend.html#details-of-the-multilevel-model-with-trends"><i class="fa fa-check"></i><b>8.2</b> Details of the Multilevel Model with Trends</a></li>
<li class="chapter" data-level="8.3" data-path="MLM-Trend.html"><a href="MLM-Trend.html#estimating-the-multilevel-model-for-the-included-writing-intervention-studies"><i class="fa fa-check"></i><b>8.3</b> Estimating the Multilevel Model for the Included Writing Intervention Studies</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="intro-case-specific-es.html"><a href="intro-case-specific-es.html"><i class="fa fa-check"></i><b>9</b> Introduction to Case-Specific Effect Sizes</a>
<ul>
<li class="chapter" data-level="9.1" data-path="intro-case-specific-es.html"><a href="intro-case-specific-es.html#background-3"><i class="fa fa-check"></i><b>9.1</b> Background</a></li>
<li class="chapter" data-level="9.2" data-path="intro-case-specific-es.html"><a href="intro-case-specific-es.html#when-to-use-case-specific-effect-sizes"><i class="fa fa-check"></i><b>9.2</b> When to Use Case-Specific Effect Sizes</a></li>
<li class="chapter" data-level="9.3" data-path="intro-case-specific-es.html"><a href="intro-case-specific-es.html#assumptions-and-limitations-of-case-specific-effect-sizes"><i class="fa fa-check"></i><b>9.3</b> Assumptions and Limitations of Case-Specific Effect Sizes</a></li>
<li class="chapter" data-level="9.4" data-path="intro-case-specific-es.html"><a href="intro-case-specific-es.html#assumptions-and-limitations-of-nonoverlap-indices"><i class="fa fa-check"></i><b>9.4</b> Assumptions and Limitations of Nonoverlap Indices</a></li>
<li class="chapter" data-level="9.5" data-path="intro-case-specific-es.html"><a href="intro-case-specific-es.html#assumptions-and-limitations-of-standardized-mean-differences"><i class="fa fa-check"></i><b>9.5</b> Assumptions and Limitations of Standardized Mean Differences</a></li>
<li class="chapter" data-level="9.6" data-path="intro-case-specific-es.html"><a href="intro-case-specific-es.html#assumptions-and-limitations-of-percentage-change-indices-and-log-response-ratios"><i class="fa fa-check"></i><b>9.6</b> Assumptions and Limitations of Percentage Change Indices and Log Response Ratios</a></li>
<li class="chapter" data-level="9.7" data-path="intro-case-specific-es.html"><a href="intro-case-specific-es.html#assumptions-and-limitations-of-percent-of-goal-obtained"><i class="fa fa-check"></i><b>9.7</b> Assumptions and Limitations of Percent of Goal Obtained</a></li>
<li class="chapter" data-level="9.8" data-path="intro-case-specific-es.html"><a href="intro-case-specific-es.html#case-specific-effect-size-options-for-synthesizing-single-case-research"><i class="fa fa-check"></i><b>9.8</b> Case-Specific Effect Size Options for Synthesizing Single-Case Research</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="app-case-specific-es.html"><a href="app-case-specific-es.html"><i class="fa fa-check"></i><b>10</b> Application of Case-Specific Effect Sizes</a>
<ul>
<li class="chapter" data-level="10.1" data-path="app-case-specific-es.html"><a href="app-case-specific-es.html#selecting-case-specific-effect-sizes-for-the-single-case-studies"><i class="fa fa-check"></i><b>10.1</b> Selecting Case-Specific Effect Sizes for the Single-Case Studies</a></li>
<li class="chapter" data-level="10.2" data-path="app-case-specific-es.html"><a href="app-case-specific-es.html#estimating-the-case-specific-effect-sizes-for-the-included-aac-intervention-studies"><i class="fa fa-check"></i><b>10.2</b> Estimating the Case-Specific Effect Sizes for the Included AAC Intervention Studies</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="app-case-specific-es.html"><a href="app-case-specific-es.html#entering-the-data-into-excel"><i class="fa fa-check"></i><b>10.2.1</b> Entering the Data into Excel</a></li>
<li class="chapter" data-level="10.2.2" data-path="app-case-specific-es.html"><a href="app-case-specific-es.html#accessing-the-app"><i class="fa fa-check"></i><b>10.2.2</b> Accessing the App</a></li>
<li class="chapter" data-level="10.2.3" data-path="app-case-specific-es.html"><a href="app-case-specific-es.html#loading-the-data-into-the-app"><i class="fa fa-check"></i><b>10.2.3</b> Loading the Data into the App</a></li>
<li class="chapter" data-level="10.2.4" data-path="app-case-specific-es.html"><a href="app-case-specific-es.html#defining-the-variable-within-the-app"><i class="fa fa-check"></i><b>10.2.4</b> Defining the Variable within the App</a></li>
<li class="chapter" data-level="10.2.5" data-path="app-case-specific-es.html"><a href="app-case-specific-es.html#examining-the-graphs-within-the-app"><i class="fa fa-check"></i><b>10.2.5</b> Examining the Graphs within the App</a></li>
<li class="chapter" data-level="10.2.6" data-path="app-case-specific-es.html"><a href="app-case-specific-es.html#estimating-the-effect-sizes-within-the-app"><i class="fa fa-check"></i><b>10.2.6</b> Estimating the Effect Sizes within the App</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="app-case-specific-es.html"><a href="app-case-specific-es.html#syntex-for-r"><i class="fa fa-check"></i><b>10.3</b> Syntex for R</a></li>
<li class="chapter" data-level="10.4" data-path="app-case-specific-es.html"><a href="app-case-specific-es.html#examining-the-alignment-of-the-case-specific-effect-sizes-with-our-visual-analysis"><i class="fa fa-check"></i><b>10.4</b> Examining the Alignment of the Case-Specific Effect Sizes with our Visual Analysis</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="app-case-specific-es.html"><a href="app-case-specific-es.html#nap"><i class="fa fa-check"></i><b>10.4.1</b> NAP</a></li>
<li class="chapter" data-level="10.4.2" data-path="app-case-specific-es.html"><a href="app-case-specific-es.html#smd-results"><i class="fa fa-check"></i><b>10.4.2</b> SMD Results</a></li>
<li class="chapter" data-level="10.4.3" data-path="app-case-specific-es.html"><a href="app-case-specific-es.html#lrri-results"><i class="fa fa-check"></i><b>10.4.3</b> LRRi Results</a></li>
<li class="chapter" data-level="10.4.4" data-path="app-case-specific-es.html"><a href="app-case-specific-es.html#pogomuparrow"><i class="fa fa-check"></i><b>10.4.4</b> PoGO<sub>M<span class="math inline">\(\uparrow\)</span></sub></a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="app-case-specific-es.html"><a href="app-case-specific-es.html#averaging-the-case-specific-effect-sizes"><i class="fa fa-check"></i><b>10.5</b> Averaging the Case-Specific Effect Sizes</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="app-case-specific-es.html"><a href="app-case-specific-es.html#fixed-effects-meta-analysis"><i class="fa fa-check"></i><b>10.5.1</b> Fixed Effects Meta-Analysis</a></li>
<li class="chapter" data-level="10.5.2" data-path="app-case-specific-es.html"><a href="app-case-specific-es.html#random-effects-meta-analysis"><i class="fa fa-check"></i><b>10.5.2</b> Random Effects Meta-Analysis</a></li>
<li class="chapter" data-level="10.5.3" data-path="app-case-specific-es.html"><a href="app-case-specific-es.html#further-directions-for-synthesizing-case-specific-effect-sizes"><i class="fa fa-check"></i><b>10.5.3</b> Further Directions for Synthesizing Case-Specific Effect Sizes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Methods Guide for Effect Estimation and Synthesis of Single-Case Studies</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="intro-case-specific-es" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">Chapter 9</span> Introduction to Case-Specific Effect Sizes<a href="intro-case-specific-es.html#intro-case-specific-es" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>This chapter provides background on case-specific effect sizes and describes when this approach is useful in the synthesis of single-case research. We discuss different case-specific effect size metrics, their underlying assumptions, and when it is appropriate to use them. We conclude this chapter by providing a set of decision rules for meta-analysts to use when selecting among the various available case-specific effect sizes.</em></p>
<div id="background-3" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> Background<a href="intro-case-specific-es.html#background-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Case-specific effect sizes in single-case design (SCD) research provide an effect estimate for each participant on a common metric. In the context of a research synthesis that includes multiple studies, case-specific effect sizes are useful for summarizing the overall average effectiveness of an intervention, investigating the degree of heterogeneity of effects across participants, and identifying for whom and under what conditions the intervention is most effective. In meta-analyses of SCD studies, researchers can use several different metrics to summarize the intervention effect for a given case. One approach quantifies the effect based on the degree of nonoverlap between observations in the treatment phase and baseline phase <span class="citation">(<a href="#ref-Brossart_Vannest_Davis_Patience_2014">Brossart et al., 2014</a>; <a href="#ref-Parker_Vannest_Davis_Sauber_2011">R. I. Parker et al., 2011</a>; <a href="#ref-Parker_Vannest_2009">R. I. Parker &amp; Vannest, 2009</a>; <a href="#ref-parker2014Nonoverlap">V. Parker R. L. &amp; Davis, 2014</a>; <a href="#ref-Scruggs_Mastropieri_Casto_1987">Scruggs et al., 1987</a>; <a href="#ref-Tarlow_2017">Tarlow, 2017</a>)</span>. A second approach standardizes a summary of the change between baseline and treatment phases using standardized mean differences <span class="citation">(<a href="#ref-Busk_Serlin_1992">Busk &amp; Serlin, 1992</a>; <a href="#ref-Gingerich1984meta">Gingerich, 1984</a>)</span> or standardized regression coefficients <span class="citation">(<a href="#ref-maggin2011Quantitative">Maggin et al., 2011</a>; <a href="#ref-van2003hierarchical">Van Den Noortgate &amp; Onghena, 2003</a>)</span>. A third approach compares the baseline and treatment levels through response ratios or percentage change indices <span class="citation">(<a href="#ref-pustejovsky2018Using">Pustejovsky, 2018</a>)</span>. Finally, a fourth approach indexes the effect relative to goal attainment by finding the percent of zero data <span class="citation">(<a href="#ref-Scotti_Evans_Meyer_Walker_1991">Scotti et al., 1991</a>)</span> or the percentage of goal obtained <span class="citation">(<a href="#ref-Ferron_Goldstein_Olszewski_Rohrer_2020">J. Ferron et al., 2020</a>)</span>. Researchers can choose among these metrics to meet the needs of their single-case synthesis context, understanding that no single approach is universally appropriate for all SCDs.</p>
<p>Each of the case-specific effect size estimation methods have limitations that can hinder their utility and, if not purposefully selected, lead to effect estimates misaligned with visual analysis. Consequently, researchers have developed methods for choosing among the different effect quantifications based on study purpose(s) and characteristics of the data <span class="citation">(<a href="#ref-Manolov_Moeyaert_Fingerhut_2022">Manolov et al., 2022</a>)</span>. Further, researchers can conduct sensitivity analyses and report results from multiple indices to allow examination of the similarity in conclusions across indices <span class="citation">(<a href="#ref-Kratochwill2010single">Kratochwill et al., 2010</a>)</span>.</p>
<p>In this chapter, we detail the available case-specific effect size options and their underlying assumptions. We provide guidance on selecting one or more metric types for a synthesis, as well as recommendations for selecting among the specific effect indices within a metric type.</p>
</div>
<div id="when-to-use-case-specific-effect-sizes" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> When to Use Case-Specific Effect Sizes<a href="intro-case-specific-es.html#when-to-use-case-specific-effect-sizes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When choosing an effect size for SCD study data, researchers should consider their aims or reason for using one or multiple effect sizes. There are two common motivations for using case-specific effect sizes to synthesize SCD effects. First, in some contexts, researchers want to quantify the treatment effect for each individual participant to summarize the average effect and examine variation in treatment effectiveness across participants, along with predictive factors that may help to identify for whom the intervention is most effective. When one purpose of a synthesis is to examine individual-level effects, case-specific effect sizes are more aligned with such a goal than are study-level effect sizes (i.e., design-comparable effect sizes that average the effect across participants). Second, in contexts where a variety of different approaches are used to measure outcomes across the cases and studies to be synthesized, researchers need to find a common metric with which to index the effects. If the same outcome is available for all cases for all studies, researchers could consider a multilevel model. However, multilevel modeling is not viable when outcomes are so disparate across cases that transformation of the raw data to a common scale is not feasible. Consider, for example, a situation where researchers are interested in examining individual-level effects of an intervention targeting the reduction of disruptive behavior during home routines. One included study measures the dependent variable as elapsed time from the beginning of a routine until the first disruptive behavior (i.e., latency). Another included study reports the effects of the intervention on reducing the number of disruptive behavior events during the routine (count per observation). In circumstances such as these, where researchers are interested in studying the variation in effects across individuals and the outcomes differ substantially across these individuals, case-specific effect sizes are preferable because they provide multiple options for indexing effects using a common metric.</p>
</div>
<div id="assumptions-and-limitations-of-case-specific-effect-sizes" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> Assumptions and Limitations of Case-Specific Effect Sizes<a href="intro-case-specific-es.html#assumptions-and-limitations-of-case-specific-effect-sizes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When computing case-specific effect sizes, assumptions and limitations encountered depend on both metric type used (nonoverlap, standardized, response-ratios, or goal attainment) and the specific index used within that type (e.g., NAP versus Tau-U). Thus, we divide this chapter into subsections for each metric type and detail the assumptions and limitations associated with selected indices of each type. We conclude with a discussion of approaches for synthesizing the effect size estimates. Researchers can calculate most of the illustrated effect size metrics using a web application called the Single-Case Effect Size Calculator [<a href="https://jepusto.shinyapps.io/SCD-effect-sizes/\" class="uri">https://jepusto.shinyapps.io/SCD-effect-sizes/\</a>; <span class="citation">Pustejovsky et al. (<a href="#ref-pustejovsky2023SingleCaseES">2023</a>)</span>] or using the SingleCaseES R package <span class="citation">(<a href="#ref-pustejovsky2023SingleCaseES">Pustejovsky et al., 2023</a>)</span>. We provide a detailed demonstration of these tools in Chapter 10.</p>
</div>
<div id="assumptions-and-limitations-of-nonoverlap-indices" class="section level2 hasAnchor" number="9.4">
<h2><span class="header-section-number">9.4</span> Assumptions and Limitations of Nonoverlap Indices<a href="intro-case-specific-es.html#assumptions-and-limitations-of-nonoverlap-indices" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One of the first nonoverlap indices developed, percent of nonoverlapping data <span class="citation">(PND; <a href="#ref-Scruggs_Mastropieri_Casto_1987">Scruggs et al., 1987</a>)</span>, computes the percentage of treatment phase observations that do not overlap with baseline phase observations. Unfortunately, the index has several technical drawbacks: (a) it is sensitive to outliers, (b) it has no known SE, and (c) its expected value depends on the number of observations in the baseline phase <span class="citation">(<a href="#ref-Allison_Gorman_1994">Allison &amp; Gorman, 1994</a>; <a href="#ref-Pustejovsky2019Procedural">Pustejovsky, 2019</a>; <a href="#ref-White1987some">White, 1987</a>)</span>. Two newer nonoverlap indices largely address these technical problems and are more widely recommended for use: nonoverlap of all pairs <span class="citation">(NAP; <a href="#ref-Parker_Vannest_2009">R. I. Parker &amp; Vannest, 2009</a>)</span> and Tau <span class="citation">(<a href="#ref-Parker_Vannest_Davis_Sauber_2011">R. I. Parker et al., 2011</a>)</span>.</p>
<p>To compute NAP, each baseline observation is compared to each treatment phase observation. For each of these comparisons, researchers must determine whether the treatment observation is more favorable (indicating a positive result), less favorable (indicating a negative result), or identical to the baseline phase observation (indicating a null result). After obtaining a rating for each paired baseline and treatment phase observation, comparisons are summed by their category to give the total number of positive comparisons (P), negative comparisons (N), and tied comparisons (T). With these totals, NAP is the percentage of the comparisons that are positive, <span class="math inline">\(NAP = \frac{P + .5\ T}{P + N + T} \times 100\)</span>. The maximum value of NAP is 100, a value that indicates all comparisons had a positive result, suggesting a positive treatment effect. If half the comparisons were positive and half were negative, the value of NAP would be 50, suggesting a null treatment effect. The minimum value of NAP is 0, when all comparisons are negative, indicating a detrimental or iatrogenic intervention effect. However, NAP is not restricted to this scale, and outcomes can be reported as a proportion using a scale of 0-1<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a>.</p>
<p>Tau is closely related to NAP and is also based on the comparison of each baseline observation to each treatment observation. However, unlike the NAP scale of 0-100 usually reported in studies, the Tau scale is -1 to 1, where 1 indicates a maximal positive effect, 0 indicates no effect, and -1 indicates a maximal negative effect. Tau can be computed from the sum of the positive, negative, and tied comparisons or can be computed directly from NAP; <span class="math inline">\(Tau = \frac{P-N}{P+N+T} = \big(2\times \frac{NAP}{100} \big)-1.\)</span> Because of the similarities between NAP and Tau, neither is statistically superior over the other. Where one can use NAP, they one could also use Tau, and vice versa. Thus, choice of NAP or Tau is a matter of the researcher’s preferred scale (i.e., an index that runs from 0 to 100, or an index that runs from -1 to 1).
NAP and Tau are preferable to PND for several reasons. Outliers have less influence on NAP and Tau estimates, resulting in more stable sampling distributions <span class="citation">(<a href="#ref-Parker_Vannest_2009">R. I. Parker &amp; Vannest, 2009</a>)</span>. Unlike PND, the expected values of NAP and Tau are not dependent on the baseline length <span class="citation">(<a href="#ref-Pustejovsky2019Procedural">Pustejovsky, 2019</a>)</span>. Furthermore, one can compute SEs for both NAP and Tau given the assumptions that the observations are mutually independent and homogeneously distributed within each condition. However, it is reasonable to believe that these assumptions may be violated, at least to some extent. Single-case data can be autocorrelated <span class="citation">(<a href="#ref-matyas1997Serial">Matyas &amp; Greenwood, 1997</a>; <a href="#ref-Shadish_Sullivan_2011">Shadish &amp; Sullivan, 2011</a>)</span>, violating the assumption of mutual independence. Single-case data series can also involve time trends, leading to heterogeneous distributions within each phase. Yet, having SEs under relatively restrictive assumptions is preferable to not having SEs at all. Moreover, techniques such as robust variance estimation can be deployed in the synthesis of the effect sizes that can tolerate inaccuracies in estimation of the SEs for individual effect size estimates <span class="citation">(<a href="#ref-Chen_Pustejovsky_2022">M. Chen &amp; Pustejovsky, 2022</a>)</span>.</p>
<p>A limitation of both NAP and Tau, as well as for the other nonoverlap indices, is the presence of a maximum value that restricts the sensitivity of this index to large effects. Consider the multiple baseline study by <span class="citation">Knochel et al. (<a href="#ref-knochel2021culturally">2021</a>)</span> shown in Figure <a href="intro-case-specific-es.html#fig:Knochel-2021">9.1</a>. In this study, the researchers tested a culturally focused training to increase staff use of behavior-specific praise with students. Visual analysis suggests that the treatment effect varies across dyads, with Dyad 4’s effect being the largest and the remaining dyads having variations in variability, level, and overall magnitude of effect (i.e., differences in level across phases). However, the value of NAP for all three cases is 100 and the value of Tau is 1, because treatment phase observations do not overlap with any baseline phase observation. The ceiling effect for the nonoverlap indices leads to less variation in the effect size estimates than there would be in an examination of true effects.</p>
<p>When choosing to use NAP or Tau, one implicitly assumes that these metrics will reflect meaningful variation in the magnitude of the effects. If the synthesis purpose is simply to confirm or rule out the presence of an effect, it may not be critical to distinguish between large and very large effects. However, if the research goal is to estimate the degree to which the size of the effect varies with some participant characteristic, NAP and Tau insensitivities to effect size at the high end of the scale can be problematic. Ceiling effects like those in the <span class="citation">Knochel et al. (<a href="#ref-knochel2021culturally">2021</a>)</span> example can limit meta-analytic efforts to examine the degree to which participant characteristics explain intervention effectiveness.</p>
<div class="figure"><span style="display:block;" id="fig:Knochel-2021"></span>
<img src="images/Knochel2021.jpeg" alt="Multiple Baseline Design Across Three Participants (Knochel et al., 2021)" width="60%" />
<p class="caption">
Figure 9.1: Multiple Baseline Design Across Three Participants (Knochel et al., 2021)
</p>
</div>
<p>Another limitation of interpreting both NAP and Tau results as effect sizes is that one must assume there is no baseline trend and that the difference between the baseline and treatment observations results from a functional relation between the intervention and outcome, rather than some extraneous variable. Figure <a href="intro-case-specific-es.html#fig:Rodgers2020Denny">9.2</a> shows the data from Denny, one of the participants in the <span class="citation">Rodgers et al. (<a href="#ref-rodgers2021Effects">2021</a>)</span> writing intervention study. Denny’s graph portrays a trend in baseline observations that suggests that some other factor has an influence on the behavior (e.g., maturation). While the value of NAP for this illustration is 80 (<span class="math inline">\(\text{SE} = 10\)</span>; <span class="math inline">\(\text{CI}_{95} = 56,\ 92\)</span>) and the value of Tau is 0.60 (<span class="math inline">\(\text{SE} = 0.19\)</span>; <span class="math inline">\(\text{CI}_{95} = 0.13,\ 0.84\)</span>), these positive values cannot be attributed to the treatment alone. In some synthesis contexts, studies with unstable baselines may not meet the quality inclusion criteria; however, ultimately it is up to the researchers to determine the inclusion criterion based on their specific research questions. If included, researchers should consider alternative effect sizes that have baseline trend adjustments.</p>
<div class="figure"><span style="display:block;" id="fig:Rodgers2020Denny"></span>
<img src="images/Rodgers-2020-Denny.jpg" alt="Effect for Denny (Rodgers et al., 2020)" width="100%" />
<p class="caption">
Figure 9.2: Effect for Denny (Rodgers et al., 2020)
</p>
</div>
<p>Researchers’ desire for a metric that can adjust for baseline trends led to the development of Tau-U <span class="citation">(<a href="#ref-Parker_Vannest_Davis_Sauber_2011">R. I. Parker et al., 2011</a>)</span>. Tau-U adds a correction for monotonic trend to the numerator of the Tau computation. Unfortunately, this correction introduces multiple problems that make it challenging to recommend Tau-U for use in a research synthesis. As described in detail by <span class="citation">Tarlow (<a href="#ref-Tarlow_2017">2017</a>)</span>, problems with Tau-U include: (a) estimates are not bounded between -1 and 1, (b) adjustments cannot be meaningfully represented on a graph, (c) the size of the trend adjustment depends on the number of baseline observations and thus the expected value of Tau-U depends on phase lengths, and (c) the trend adjustment changes the sampling distribution of the statistic so that the Tau SEs are not appropriate for Tau-U. Furthermore, separate SEs for Tau-U have not yet been derived.</p>
<p>To correct some of the technical problems with Tau-U, <span class="citation">Tarlow (<a href="#ref-Tarlow_2017">2017</a>)</span> developed the baseline-corrected Tau <span class="citation">(<a href="#ref-Tarlow_2017">Tarlow, 2017</a>)</span> that attempts to resolve the first three of the aforementioned problems. While baseline-corrected Tau (Tau<sub>BC</sub>) is preferred over Tau-U, appropriate SEs for this index have not been derived, which leads to some challenges in using it in research syntheses. The implementation of Tau<sub>BC</sub> proposed by <span class="citation">Tarlow (<a href="#ref-Tarlow_2017">2017</a>)</span> also has two further problems. First, <span class="citation">Tarlow (<a href="#ref-Tarlow_2017">2017</a>)</span> proposed correcting baseline trends only when a pretest of the trend using a nonparametric rank correlation test is statistically significant. The power of this pretest depends on the number of observations in the baseline phase, and as a result, the size of the trend adjustment depends to an extent on the number of baseline observations. This issue can be avoided by applying the trend correction uniformly, regardless of the statistical significance of the pretest. Second, <span class="citation">Tarlow (<a href="#ref-Tarlow_2017">2017</a>)</span> proposed to calculate Tau<sub>BC</sub> using Kendall’s tau-b statistic, measuring the rank correlation between the trend-corrected dependent variable (y) and a binary indicator for the treatment phase (x). Because tau-b makes an adjustment for ties, which occur necessarily because x is binary, the tau-b statistic will not generally correspond to the proportion of comparisons that represent improvement after adjusting for baseline trend, and the differences can be quite large in practice. This issue can be avoided by calculating Tau<sub>BC</sub> directly from the formula, rather than via the tau-b statistic<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a>.</p>
</div>
<div id="assumptions-and-limitations-of-standardized-mean-differences" class="section level2 hasAnchor" number="9.5">
<h2><span class="header-section-number">9.5</span> Assumptions and Limitations of Standardized Mean Differences<a href="intro-case-specific-es.html#assumptions-and-limitations-of-standardized-mean-differences" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The within-case standardized mean difference [SMD<sub>W</sub>; <span class="citation">Busk &amp; Serlin (<a href="#ref-Busk_Serlin_1992">1992</a>)</span>; <span class="citation">Gingerich (<a href="#ref-Gingerich1984meta">1984</a>)</span>] index describes the magnitude of an intervention effect in terms of the mean difference between treatment and baseline observations, standardized by dividing the mean difference by an estimate of the within-case standard deviation (SD). Researchers can further incorporate a small-sample correction to reduce the bias of the SMD<sub>W</sub> estimate. For a given amount of variation in the dependent variable, the SMD<sub>W</sub> is larger when the raw score mean difference is larger; for a given raw score mean difference, the SMD<sub>W</sub> is larger when the within-phase variation in the dependent variable is smaller. The within-case SD can be calculated assuming homogenous or heterogeneous between-phase variance in observations. If researchers assume that the variance in observations differs across phases, only the baseline SD is used in the estimation of the effect size; if researchers assume that the variance in the observations is homogeneous across phases, the within-case SD is calculated by pooling the variance across the baseline and treatment phases.</p>
<p>The set of assumptions underlying the SMDW SE calculations, including those in the Single-Case Effect Size Calculator, are that observations in any given SCD study are mutually independent and homogeneously distributed (at least throughout the baseline phase). SCD study data may also violate the homogeneity of variance assumption, particularly when outcomes are reported as frequencies or percentages <span class="citation">(<a href="#ref-Shadish_Sullivan_2011">Shadish &amp; Sullivan, 2011</a>)</span> where the degree of variation tends to be related to the mean level. Single-case data can be autocorrelated as opposed to independent <span class="citation">(<a href="#ref-matyas1997Serial">Matyas &amp; Greenwood, 1997</a>; <a href="#ref-Shadish_Sullivan_2011">Shadish &amp; Sullivan, 2011</a>)</span>. In theory, an autocorrelation parameter could be added to the statistical model to loosen the assumption of independence. However, adjusting the SMD<sub>W</sub> estimator and the SEs for autocorrelation is challenging; autocorrelation parameters are difficult to estimate in short series, and the bias in these estimates leads to biases both in the effect size estimator and in its SE. Yet, to date, there is no consensus among researchers about the tolerability of independence and homogeneity assumption violations in calculating effects and SEs using standardized mean differences. If researchers need an approach that can tolerate some inaccuracies in the SE estimates in the synthesis of effect sizes, robust variance estimation may be preferable <span class="citation">(<a href="#ref-Chen_Pustejovsky_2022">M. Chen &amp; Pustejovsky, 2022</a>)</span>, as we illustrate in Chapter 10.</p>
<p>An additional limitation of the SMDW is that there are circumstances in which it cannot be calculated. Consider the case of Stephen (Gevarter &amp; Horan, 2019) in Figure <a href="intro-case-specific-es.html#fig:GevarterHoran-2019">9.3</a> below. Stephen’s baseline values are all equal to zero. In situations where baseline values for a case are consistently the same value (e.g., all 0s or all 2s), the baseline SD is equal to 0 and it is not possible to divide the raw score mean difference by a baseline SD of 0. Furthermore, effect size estimates using this method can conflict with visual analysis when the baseline SD is very small because the resulting SMD<sub>W</sub> may be very large, although the SCD graphs do not portray such. When this occurs, these very large values may show up in a synthesis as outlying effect sizes, creating challenges for summarizing the effects using meta-analysis.</p>
<div class="figure"><span style="display:block;" id="fig:GevarterHoran-2019"></span>
<img src="images/raw_GevarterHoran2019.png" alt="Effects of a Speech-Generating Device Intervention on Child Vocalizations (Gevarter \&amp; Horan, 2019)" width="100%" />
<p class="caption">
Figure 9.3: Effects of a Speech-Generating Device Intervention on Child Vocalizations (Gevarter &amp; Horan, 2019)
</p>
</div>
<p>Another limitation of SMD<sub>W</sub> is that to interpret it as an effect size, one must assume that there is no baseline trend and that the difference between baseline and treatment observations is due to the intervention and not some other factor. In Figure <a href="intro-case-specific-es.html#fig:Rodgers2020Denny">9.2</a> for the case of Denny, observations in baseline increase with time, suggesting an improvement in absence of the intervention. Where there is a baseline trend like this, the lack of experimental control suggests that some other factor (e.g., maturation) may have an influence on the behavior observed. It would be reasonable to assume that the upward trend in Denny’s graph is due to some extraneous factor.
Although the value of SMD<sub>W</sub> for this graph is 1.23 with a SE of 0.54 and <span class="math inline">\(95\%\)</span> CI [0.18, 2.28], this standardized mean difference cannot be attributed to the treatment alone. To interpret SMDW as the effect of treatment, baseline stability is also assumed (i.e., no baseline trend). In some synthesis contexts, studies with unstable baselines may not meet the quality inclusion criteria; however, in other contexts, researchers may decide and have justification for the inclusion of such studies. If included, there is a need to consider alternative effect sizes that have baseline trend adjustments.</p>
<p>Researchers have developed adjustments to the SMD<sub>W</sub> for baseline trend by estimating regression models where a coefficient in the model indexes the difference between the treatment and baseline phase trend lines at a specific focal time. Then, the raw score regression coefficient is standardized using an estimate of the within-case variability <span class="citation">(<a href="#ref-Maggin2017meta-analysis">Maggin et al., 2017</a>; <a href="#ref-van2003hierarchical">Van Den Noortgate &amp; Onghena, 2003</a>)</span>. Adjusting for trends, though, requires additional assumptions. One must assume a functional form for the trend (e.g., linear), and then assume that in the absence of intervention, the baseline trend would continue throughout the treatment phase.
It can be challenging to argue that the unknown and unmeasured factor(s) leading to baseline instability will continue in a way that leads to the same trend. <span class="citation">Manolov et al. (<a href="#ref-Manolov_Solanas_Sierra_2019">2019</a>)</span> provide an excellent discussion of the problems that can arise when trying to extrapolate baselines in SCD studies, as well as some potential solutions. In addition to the problems associated with trend extrapolation, the problems of trying to estimate and adjust for autocorrelation are exacerbated in models with trends <span class="citation">(<a href="#ref-Ferron_2002">J. Ferron, 2002</a>; <a href="#ref-NatesanBatley_Hedges_2021">Natesan Batley &amp; Hedges, 2021</a>)</span>. Further research is needed to provide better guidance on how to standardize mean differences when there may be both trends and autocorrelation.</p>
</div>
<div id="assumptions-and-limitations-of-percentage-change-indices-and-log-response-ratios" class="section level2 hasAnchor" number="9.6">
<h2><span class="header-section-number">9.6</span> Assumptions and Limitations of Percentage Change Indices and Log Response Ratios<a href="intro-case-specific-es.html#assumptions-and-limitations-of-percentage-change-indices-and-log-response-ratios" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The percentage change index, response ratios, and log response ratios are closely related case-specific effect sizes. Of these, the percentage change index <span class="citation">(<a href="#ref-hershberger1999meta">Hershberger et al., 1999</a>)</span> is considered the easiest to interpret. To obtain a percentage change effect size, one divides the raw score mean difference between treatment and baseline phases by the baseline mean and then multiplies the result by 100<span class="math inline">\(\%\)</span>.
For example, if the baseline mean level of problem behavior is 20 and the mean treatment phase level is 5, one divides the raw score mean difference (15) by the baseline mean (20), and then multiplies the result by 100<span class="math inline">\(\%\)</span> to obtain a percentage reduction of problem behavior (i.e., percentage change) of 75<span class="math inline">\(\%\)</span>. Although easy to interpret, the sampling distribution of the percentage change index makes it challenging to use in a meta-analysis. Alternatively, the log response ratio (LRR) uses the same information (i.e., baseline and treatment phase means), but combines them into an index with improved distributional properties that is easier to work with meta-analytically. To compute the LRR index, one calculates the ratio of the intervention mean to the baseline mean, takes the natural log of this ratio, and then makes a correction for small sample bias <span class="citation">(<a href="#ref-pustejovsky2018Using">Pustejovsky, 2018</a>)</span>. While the LRR values can be challenging to interpret directly, one can overcome this limitation by converting raw effect size estimates or meta-analytic results from the LRR scale to a percentage change, using the formula:
<span class="math display" id="eq:LRRtoPctC">\[\begin{equation}
\tag{9.1}
\% \ change= (e^{LRR}-1) \times 100 \%
\end{equation}\]</span>
Converting the LRR scale to a percentage change allows researchers to take advantage of the desirable statistical properties of the LRR for meta-analytic modeling and the transparency of the percentage change scale for interpretation.</p>
<p>Approximate SEs for the LRR have been derived under the assumption that the observations within each phase are mutually independent and homogeneously distributed <span class="citation">(<a href="#ref-pustejovsky2018Using">Pustejovsky, 2018</a>)</span>. However, as common in SCD research, observations can be autocorrelated <span class="citation">(<a href="#ref-matyas1997Serial">Matyas &amp; Greenwood, 1997</a>; <a href="#ref-Shadish_Sullivan_2011">Shadish &amp; Sullivan, 2011</a>)</span>. Positive autocorrelation will lead to downward bias in the SEs and overly narrow confidence intervals, giving the perception of greater certainty than is warranted. Although the potential for bias in the SEs is not ideal, it is still feasible to use the SEs in a meta-analysis. In situations where the accuracy of the SEs is a concern, we recommend that researchers use robust variance estimation (RVE) methods in the synthesis of the effect sizes because RVE is an approach to synthesis that can tolerate inaccuracies in the estimated SEs of individual effect sizes <span class="citation">(<a href="#ref-Chen_Pustejovsky_2022">M. Chen &amp; Pustejovsky, 2022</a>)</span>.</p>
<p>A limitation of LRRs, response ratios, and percentage change indices is that they are only appropriate for outcome measures that have a ratio scale. A ratio outcome is measured on a scale where zero corresponds to the true absence of the quantity being measured (true zero) and each unit on the scale is equal in size. Consider the variable “time spent reading.” It can be measured as the number of seconds one spends reading prior to being distracted (and not reading). The ratio scale for this outcome would include 0 seconds, interpreted as zero seconds elapsed (true zero) or absolutely no time. Since a second is a consistent unit of measurement (the duration of each second on the scale is equivalent), and there can be a true zero on the scale, the outcome “time spent reading” can be measured using a ratio scale. Other examples of ratio scales include count-based variables recorded as frequencies (e.g., number of disruptive behaviors, number of pro-social behaviors) or percentages (e.g., percentage of intervals with on-task behavior). However, variables without a true zero are not ratio variables. For example, consider the measurement of a student’s disruptive behavior using a behavior rating scale with the following four options: <em>rarely</em>, <em>sometimes</em>, <em>often</em>, and <em>almost always</em>. The variable <em>rarely</em> might be coded as 0, <em>sometimes</em> as 1, <em>often</em> as 2, and <em>almost always</em> as 3. However, the coded value 0 cannot be interpreted as an absolute zero (rarely <span class="math inline">\(\ne\)</span> never) and the newly coded units are not equal in size (e.g., 2 <span class="math inline">\(\ne\)</span> twice as much disruptive behavior as a 1). Thus, it is not appropriate to compute LRRs, response ratios, or percentage change for such a behavior rating scale or, more generally, for outcomes not measured on a ratio scale. However, these indices can also be difficult to interpret for ratio-based outcomes in reversal designs because the index is not symmetric with respect to the phase labels. Thus, a percentage change comparing treatment versus baseline is not necessarily the same magnitude as percentage change comparing baseline versus treatment.</p>
<p>Another limitation of LRRs arises in situations where the baseline has a constant value of zero because it is not possible to divide by zero or take the natural log of zero. Here it is important to consider whether we believe that the true mean level we are trying to estimate is actually zero or some value close to zero. For example, if our outcome is continuous, the true values may be close to zero but recorded as zero by our measurement instrument. As another example, consider a low-incidence behavior that happened to not be present in any of our baseline observations, but where we anticipate that we would occasionally get a non-zero value if we were to extend the baseline phase further. When the true mean is not zero, there is a method to adjust the sample mean estimate <span class="citation">(<a href="#ref-Pustejovsky2015Measurement">Pustejovsky, 2015</a>)</span> and proceed to compute the LRR. However, when the baseline mean of zero reflects the true mean, such as when the behavior being measured is not yet in the child’s behavioral repertoire, then the percentage change, response ratio, and LRR metrics are not appropriate.</p>
<p>In addition, for the LRR to be interpreted as an effect size, it must be assumed that there is no baseline trend and that the difference between the baseline and treatment observations arises from the treatment as opposed to some other factor. Returning to the graphed data for Denny (Figure <a href="intro-case-specific-es.html#fig:Rodgers2020Denny">9.2</a> where there is a baseline trend suggesting that some other factor (e.g., maturation) is impacting the observations, LRR as an effect size is somewhat problematic. The estimated value of LRR representing a treatment effect for Denny is 0.34 (<span class="math inline">\(\text{SE} = 0.12\)</span>, <span class="math inline">\(\text{CI}_{95} = 0.09, \ 0.58\)</span>), with a corresponding percentage change index of 40<span class="math inline">\(\%\)</span> (<span class="math inline">\(\text{CI}_{95} = 9, \ 79\)</span>), but these values cannot be attributed to the treatment. Put another way, to interpret LRRs or percentage changes as the effects of treatment, one must assume stability in baseline observations (i.e., there is no trend). In some synthesis contexts, studies with unstable baselines may not meet the quality inclusion criteria; however, in other contexts, such studies may be included. If included, there is a need to consider alternative effect sizes that have baseline trend adjustments. At present, we are not aware of methods in the literature that provide baseline trend corrections for LRR.</p>
</div>
<div id="assumptions-and-limitations-of-percent-of-goal-obtained" class="section level2 hasAnchor" number="9.7">
<h2><span class="header-section-number">9.7</span> Assumptions and Limitations of Percent of Goal Obtained<a href="intro-case-specific-es.html#assumptions-and-limitations-of-percent-of-goal-obtained" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Percent of goal obtained <span class="citation">(PoGO; <a href="#ref-Ferron_Goldstein_Olszewski_Rohrer_2020">J. Ferron et al., 2020</a>)</span> is a case-specific effect size that puts raw score effects onto a common scale by comparing the distance the intervention moved the outcome to the distance the intervention would have ideally moved the outcome. PoGO can only be computed if there is a goal. Goals can be set to the minimum of the outcome scale (e.g., the goal is to reduce problem behavior to 0), the maximum of the outcome scale (e.g., the goal is to increase on-task behavior to 100<span class="math inline">\(\%\)</span>), or the level of typically developing peers (e.g., the goal is to increase the number of socially appropriate interactions to the number that is typical).</p>
<p>The simplest and most readily available approach to estimating PoGO relies on the phase means (PoGO<sub>M</sub>). For example, consider the study of an intervention intended to reduce problem behavior to zero occurrences (<span class="math inline">\(\text{goal} = 0\)</span>). If the baseline mean was 40 and the treatment mean was 20 or half the level of baseline, the intervention would have moved the behavior halfway to the goal. Thus, <span class="math inline">\(PoGO_M = \frac{40-20}{40-0} \times \ 100 = 50\)</span>. A PoGO estimate of 0 indicates no movement toward the goal, whereas a PoGO estimate of 100 indicates that the goal was achieved. However, PoGO is not bounded by 0 and 100. Values less than zero can occur if the intervention is harmful and results in observations moving the opposite direction of the goal. PoGO values can also exceed 100 if one or more of the observations exceed the specified goal. For example, if the researchers set a reading fluency intervention goal of 90 words read correctly per minute and treatment phase observations consistently exceeded 90, the PoGO estimate would be more than 100.</p>
<p>The assumptions underlying the calculations of approximate SEs for PoGO<sub>M</sub> are that observations are independent and homogeneously distributed and that the sample means are approximately normally distributed <span class="citation">(<a href="#ref-Patrona2022">Patrona et al., 2022</a>)</span>. Single-case data can be autocorrelated as opposed to independent <span class="citation">(<a href="#ref-matyas1997Serial">Matyas &amp; Greenwood, 1997</a>; <a href="#ref-Shadish_Sullivan_2011">Shadish &amp; Sullivan, 2011</a>)</span>. While in theory one could add an autocorrelation parameter to the statistical model to loosen the assumption of independence, adjusting for autocorrelation is challenging. Autocorrelation parameters are difficult to estimate in short series, and the bias in these estimates would be expected to lead to biases in the SEs. The normality assumption and homogeneity assumptions may also be violated by single-case data, which are often based on frequencies or percentages <span class="citation">(<a href="#ref-Shadish_Sullivan_2011">Shadish &amp; Sullivan, 2011</a>)</span>, which can be non-normal and heterogeneous across phases, particularly if a phase mean is near the boundary of the variable scale (e.g., the mean baseline percent is near 0 or 100). Hopefully, future research will clarify the degree to which the commonly made assumptions of independence, normality, and homogeneity can be violated before the violations lead to substantial biases in the SEs.</p>
<p>An obvious limitation of PoGO is that it can only be used as an index of effects for studies that specify a goal or target criterion for the outcome under study. However, in some studies, there is no goal stated or implied. Without a goal, PoGO cannot be used as a measure of effect. In addition, to interpret PoGO<sub>M</sub> as an effect size, it must be assumed that there is no baseline trend and that the difference between baseline and treatment observations arises from the treatment as opposed to some other factor. Returning to Figure <a href="intro-case-specific-es.html#fig:Rodgers2020Denny">9.2</a>, if <span class="citation">Rodgers et al. (<a href="#ref-rodgers2021Effects">2021</a>)</span> specified a target goal of 20 correct writing sequences, then the PoGO<sub>M</sub> value for Denny is 30.1. However, Denny’s estimated PoGO<sub>M</sub> cannot be attributed to the treatment because the trend in baseline suggests some other factor, like maturation. To interpret PoGO<sub>M</sub> as the effect of treatment, baseline stability must be assumed (i.e., no trend). In some synthesis contexts, studies with unstable baselines may not meet the quality inclusion criteria; however, in other contexts researchers who decide to include such studies may need to consider alternative methods that have baseline trend adjustments.</p>
<p>To adjust PoGO for baseline trend, researchers can estimate regression models where a coefficient in the model indexes the difference between the projected baseline and actual treatment phase trend lines at a specific focal time. Next, the raw score regression coefficient is divided by the distance between the extended baseline trend line at the focal time and the outcome goal to obtain PoGO<sub>b</sub>. The SEs for PoGO<sub>b</sub> are then computed using the SEs from the regression coefficients <span class="citation">(<a href="#ref-Patrona2022">Patrona et al., 2022</a>)</span>. Adjusting for trends also requires additional assumptions, like a functional form for the trend (e.g., linear), and that the baseline trend would continue throughout the treatment phase in the absence of intervention. It can be challenging to argue that the unknown and unmeasured factor(s) leading to baseline instability would continue to have the same effect on the dependent variable over time (i.e., across subsequent phases) resulting in the same trend<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a>. In addition to problems associated with trend extrapolation, problems of estimating and adjusting for autocorrelation can exacerbate in models with trends <span class="citation">(<a href="#ref-Ferron_2002">J. Ferron, 2002</a>; <a href="#ref-NatesanBatley_Hedges_2021">Natesan Batley &amp; Hedges, 2021</a>)</span>. Hopefully, in dealing with SCD study data having both trends and autocorrelation, future research will provide better guidance on how to estimate PoGO.</p>
</div>
<div id="case-specific-effect-size-options-for-synthesizing-single-case-research" class="section level2 hasAnchor" number="9.8">
<h2><span class="header-section-number">9.8</span> Case-Specific Effect Size Options for Synthesizing Single-Case Research<a href="intro-case-specific-es.html#case-specific-effect-size-options-for-synthesizing-single-case-research" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let us suppose that a meta-analyst has chosen case-specific effect sizes based on the decision rules in Figure <a href="intro.html#fig:synthesis-flow-chart">1.1</a>. We suggest that the meta-analyst should then entertain four different case-specific effect size metrics: (a) nonoverlap, (b) standardizing, (c) response ratios, and (d) goal attainment. Using knowledge of the limitations of each case-specific effect size metric and the decision tree in Figure <a href="intro-case-specific-es.html#fig:CaseSpecificES-flowchart">9.4</a>, the meta-analyst can eliminate metrics clearly not appropriate for their synthesis context. For example, when the participant of an SCD study has a baseline of constant values of zero, one can rule out the use of standardizing. In addition to baselines values of zero, if the meta-analyst suspects that the true baseline mean is zero, they can also eliminate the response ratio approach from consideration. Likewise, one would eliminate the use of response ratios if the outcome scale was not a ratio scale (does not have a true zero). Finally, when the outcome does not have a goal, one would eliminate goal attainment from consideration. After this initial screening, there may be at least one approach (nonoverlap) and potentially several approaches left for further consideration.</p>
<p>For the approaches still under consideration, the meta-analyst should evaluate the degree to which each approach has the potential to produce effect size estimates that align with visual analysis. To do this, it may be necessary to begin estimating effect sizes. In choosing a specific effect size to estimate within an approach, researchers need to decide whether adjustments for baseline trends are necessary. When trend adjustments are unnecessary, we recommend the use of NAP/Tau for nonoverlap, SMD<sub>W</sub> for standardizing, LRR transformed into percentage change for response ratios, and PoGO<sub>M</sub> for goal attainment. If a linear trend adjustment appears most appropriate, we recommend Tau<sub>BC</sub> for nonoverlap, <span class="math inline">\(\beta\)</span> for standardizing, and PoGO<sub>b</sub> for goal attainment. Response ratio options are currently not available if baseline trend adjustments are needed.</p>
<p>If any of these effect size approaches result in estimates conflicting with visual analysis, we suggest that the approach and its results be ruled out for consideration in the overall synthesis. However, it would be beneficial for future researchers to have access to such computations and results. Thus, we encourage researchers to be transparent and make the effect sizes available in supplemental materials (or in an archive of replication materials) so that readers can assess for themselves the lack of alignment and decision to drop the index. At this point, a meta-analyst is left with only the effect size estimation approaches that align reasonably well to visual analysis. Figure <a href="intro-case-specific-es.html#fig:CaseSpecificES-flowchart">9.4</a> depicts a set of decision rules that correspond with these recommendations.</p>
<div class="figure"><span style="display:block;" id="fig:CaseSpecificES-flowchart"></span>
<img src="images/flowchart_CaseSpecificES.jpeg" alt="Flow Chart for the Selection of Case-Specific Effect Size(s)" width="60%" />
<p class="caption">
Figure 9.4: Flow Chart for the Selection of Case-Specific Effect Size(s)
</p>
</div>
<p>After working through the decision rules, situations may arise in which multiple approaches are under consideration. At this point, a researcher must determine whether one or more indices will be selected for the meta-analysis. For example, the researcher could decide to estimate effect sizes and conduct meta-analyses using each of the viable effect size indices. The argument for including more than one index is that each has its own strengths and limitations, and using multiple indices allows us to check the sensitivity of our conclusions to the choice of effect metric <span class="citation">(<a href="#ref-Kratochwill2010single">Kratochwill et al., 2010</a>)</span>. For those who prefer to select a single metric, we suggest selecting the index that best aligns with the visual analysis, or if no index aligns much better than the others, selecting the index that tends to function the best within a meta-analytic model. For example, a study by <span class="citation">M. Chen &amp; Pustejovsky (<a href="#ref-Chen_Pustejovsky_2022">2022</a>)</span> synthesizing NAP, SMD<sub>W</sub>, and LRR found the LRR case-specific effect size functioned best (e.g., showed little to no bias and relatively accurate confidence intervals) across a variety of conditions for frequency count outcomes, followed by NAP. The SMD<sub>W</sub> metric led to the most substantial problems. Thus, if viable, the LRR effect size is preferred over NAP, and NAP is preferred over SMD<sub>W</sub>.</p>
<p>After selecting one or multiple indices for the meta-analysis, the researcher then turns to synthesizing the effects. Although multiple approaches are possible, the synthesis approach that works best depends on the effect size being synthesized. With LRRs, we recommend multilevel meta-analysis coupled with robust variance estimation <span class="citation">(<a href="#ref-Chen_Pustejovsky_2022">M. Chen &amp; Pustejovsky, 2022</a>)</span>. However, with NAPs and SMDs, the correlations between the effect sizes and their SEs create problems for using multilevel meta-analysis, so it is better to use simple averages with robust variance estimation <span class="citation">(<a href="#ref-Chen_Pustejovsky_2022">M. Chen &amp; Pustejovsky, 2022</a>)</span>. Robust variance estimation is generally helpful in synthesizing any of these case-specific effect sizes because some inaccuracies in the SEs are likely due to violations to the independence assumption that is made when estimating the SEs for NAP, Tau, SMD<sub>W</sub>, LRR, and PoGO<sub>M</sub>.</p>
<p>In the following chapter of this guide, we illustrate the application of case-specific effect sizes for the synthesis of single-case studies when researchers are interested in understanding the heterogeneity of effects across individuals in studies where measures and outcomes are not homogeneous. Using the flowchart in Figure <a href="intro-case-specific-es.html#fig:CaseSpecificES-flowchart">9.4</a> as a reference point, we provide step-by-step instructions for selecting one or more of the case-specific effect sizes, then demonstrate use of the <em>Single-Case Effect Size Calculator</em> <span class="citation">(<a href="#ref-pustejovsky2023SingleCaseES">Pustejovsky et al., 2023</a>)</span> to obtain these effects and comparing them to what is found through visual inspection of participant graphs within each included study. Finally, we conclude Chapter 10 with a hypothetical synthesis using meta-analytic models.</p>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-Allison_Gorman_1994" class="csl-entry">
Allison, D. B., &amp; Gorman, B. S. (1994). <span>“Make things as simple as possible, but no simpler.”</span> A rejoinder to scruggs and mastropieri. <em>Behaviour Research and Therapy</em>, <em>32</em>(8), 885–890. <a href="https://doi.org/10.1016/0005-7967(94)90170-8">https://doi.org/10.1016/0005-7967(94)90170-8</a>
</div>
<div id="ref-Brossart_Vannest_Davis_Patience_2014" class="csl-entry">
Brossart, D. F., Vannest, K. J., Davis, J. L., &amp; Patience, M. A. (2014). Incorporating nonoverlap indices with visual analysis for quantifying intervention effectiveness in single-case experimental designs. <em>Neuropsychological Rehabilitation</em>, <em>24</em>(3–4), 464–491. <a href="https://doi.org/10.1080/09602011.2013.868361">https://doi.org/10.1080/09602011.2013.868361</a>
</div>
<div id="ref-Busk_Serlin_1992" class="csl-entry">
Busk, P. L., &amp; Serlin, R. C. (1992). Meta-analysis for single-case research. In T. R. Kratochwill &amp; J. R. Levin (Eds.), <em>Single-case research design and analysis: New directions for psychology and education</em> (pp. 187–212). Lawrence Erlbaum Associates, Inc.
</div>
<div id="ref-Chen_Pustejovsky_2022" class="csl-entry">
Chen, M., &amp; Pustejovsky, J. E. (2022). Multilevel meta-analysis of single-case experimental designs using robust variance estimation. <em>Psychological Methods</em>. <a href="https://doi.org/10.1037/met0000510.supp">https://doi.org/10.1037/met0000510.supp</a>
</div>
<div id="ref-Ferron_2002" class="csl-entry">
Ferron, J. (2002). Reconsidering the use of the general linear model with single-case data. <em>Behavior Research Methods, Instruments, &amp; Computers</em>, <em>34</em>(3), 324–331. <a href="https://doi.org/10.3758/BF03195459">https://doi.org/10.3758/BF03195459</a>
</div>
<div id="ref-Ferron_Goldstein_Olszewski_Rohrer_2020" class="csl-entry">
Ferron, J., Goldstein, H., Olszewski, A., &amp; Rohrer, L. (2020). Indexing effects in single-case experimental designs by estimating the percent of goal obtained. <em>Evidence-Based Communication Assessment and Intervention</em>, <em>14</em>(1-2), 6–27. <a href="https://doi.org/10.1080/17489539.2020.1732024">https://doi.org/10.1080/17489539.2020.1732024</a>
</div>
<div id="ref-Gingerich1984meta" class="csl-entry">
Gingerich, W. J. (1984). Meta-analysis of applied time-series data. <em>The Journal of Applied Behavioral Science</em>, <em>20</em>(1), 71–79. <a href="https://doi.org/10.1177/002188638402000113">https://doi.org/10.1177/002188638402000113</a>
</div>
<div id="ref-hershberger1999meta" class="csl-entry">
Hershberger, S., Wallace, D., Green, S., &amp; Marquis, J. (1999). Meta-analysis of single-case data. In R. H. Hoyle (Ed.), <em>Statistical strategies for small sample research</em> (pp. 107–132). Sage Newbury Park, CA.
</div>
<div id="ref-knochel2021culturally" class="csl-entry">
Knochel, A. E., Blair, K.-S. C., &amp; Sofarelli, R. (2021). Culturally focused classroom staff training to increase praise for students with autism spectrum disorder in ghana. <em>Journal of Positive Behavior Interventions</em>, <em>23</em>(2), 106–117.
</div>
<div id="ref-Kratochwill2010single" class="csl-entry">
Kratochwill, T. R., Hitchcock, J., Horner, R. H., Levin, J. R., Odom, S. L., Rindskopf, D. M., &amp; Shadish, W. R. (2010). <em>Single-<span>Case Designs Technical Documentation</span>, <span>Version</span> 1.0 (<span>Pilot</span>)</em> (June; Vol. 0). <span>What Works Clearinghouse</span>.
</div>
<div id="ref-maggin2011Quantitative" class="csl-entry">
Maggin, D. M., O’Keeffe, B. V., &amp; Johnson, A. H. (2011). A <span>Quantitative Synthesis</span> of <span>Methodology</span> in the <span>Meta-Analysis</span> of <span>Single-Subject Research</span> for <span>Students</span> with <span>Disabilities</span>: 1985. <em>Exceptionality</em>, <em>19</em>(2), 109–135. <a href="https://doi.org/10.1080/09362835.2011.565725">https://doi.org/10.1080/09362835.2011.565725</a>
</div>
<div id="ref-Maggin2017meta-analysis" class="csl-entry">
Maggin, D. M., Pustejovsky, J. E., &amp; Johnson, A. H. A. H. (2017). A meta-analysis of school-based group contingency interventions for students with challenging behavior: <span>An</span> update. <em>Remedial and Special Education</em>, <em>38</em>(6), 353–370. <a href="https://doi.org/10.1177/0741932517716900">https://doi.org/10.1177/0741932517716900</a>
</div>
<div id="ref-Manolov_Moeyaert_Fingerhut_2022" class="csl-entry">
Manolov, R., Moeyaert, M., &amp; Fingerhut, J. E. (2022). A priori justification for effect measures in single-case experimental designs. <em>Perspectives on Behavior Science</em>, <em>45</em>(1), 153–186. <a href="https://doi.org/10.1007/s40614-021-00282-2">https://doi.org/10.1007/s40614-021-00282-2</a>
</div>
<div id="ref-Manolov_Solanas_Sierra_2019" class="csl-entry">
Manolov, R., Solanas, A., &amp; Sierra, V. (2019). Extrapolating baseline trend in single-case data: Problems and tentative solutions. <em>Behavior Research Methods</em>, <em>51</em>(6), 2847–2869. <a href="https://doi.org/10.3758/s13428-018-1165-x">https://doi.org/10.3758/s13428-018-1165-x</a>
</div>
<div id="ref-matyas1997Serial" class="csl-entry">
Matyas, T. A., &amp; Greenwood, K. M. (1997). Serial dependency in single case time series. In <em>Design and analysis of single-case research</em> (pp. 215–243). Erlbaum.
</div>
<div id="ref-NatesanBatley_Hedges_2021" class="csl-entry">
Natesan Batley, P., &amp; Hedges, L. V. (2021). Accurate models vs. Accurate estimates: A simulation study of bayesian single-case experimental designs. <em>Behavior Research Methods</em>. <a href="https://doi.org/10.3758/s13428-020-01522-0">https://doi.org/10.3758/s13428-020-01522-0</a>
</div>
<div id="ref-Parker_Vannest_2009" class="csl-entry">
Parker, R. I., &amp; Vannest, K. (2009). An improved effect size for single-case research: Nonoverlap of all pairs. <em>Behavior Therapy</em>, <em>40</em>(4), 357–367. <a href="https://doi.org/10.1016/j.beth.2008.10.006">https://doi.org/10.1016/j.beth.2008.10.006</a>
</div>
<div id="ref-Parker_Vannest_Davis_Sauber_2011" class="csl-entry">
Parker, R. I., Vannest, K. J., Davis, J. L., &amp; Sauber, S. B. (2011). Combining nonoverlap and trend for single-case research: Tau-u. <em>Behavior Therapy</em>, <em>42</em>(2), 284–299. <a href="https://doi.org/10.1016/j.beth.2010.08.006">https://doi.org/10.1016/j.beth.2010.08.006</a>
</div>
<div id="ref-parker2014Nonoverlap" class="csl-entry">
Parker, V., R. L., &amp; Davis, J. L. (2014). Non-overlap analysis for single-case research. In T. R. Kratochwill &amp; J. R. Levin (Eds.), <em>Single-case intervention research: Methodological and statistical advances</em> (pp. 127–151). American Psychological Association. https://doi.org/<a href="https://doi.org/10.1037/14376-005">https://doi.org/10.1037/14376-005</a>
</div>
<div id="ref-Patrona2022" class="csl-entry">
Patrona, E., Ferron, J., Olszewski, A., Kelley, E., &amp; Goldstein, H. (2022). Effects of explicit vocabulary interventions for preschoolers: An exploratory application of the percent of goal obtained effect size metric. <em>Journal of Speech, Language &amp; Hearing Research</em>, <em>65</em>(12), 4821–4836. <a href="https://doi.org/10.1044/2022_JSLHR-22-00217">https://doi.org/10.1044/2022_JSLHR-22-00217</a>
</div>
<div id="ref-Pustejovsky2015Measurement" class="csl-entry">
Pustejovsky, J. E. (2015). Measurement-comparable effect sizes for single-case studies of free-operant behavior. <em>Psychological Methods</em>, <em>20</em>(3), 342–359. <a href="https://doi.org/10.1037/met0000019">https://doi.org/10.1037/met0000019</a>
</div>
<div id="ref-pustejovsky2018Using" class="csl-entry">
Pustejovsky, J. E. (2018). Using response ratios for meta-analyzing single-case designs with behavioral outcomes. <em>Journal of School Psychology</em>, <em>68</em>, 99–112. <a href="https://doi.org/10.1016/j.jsp.2018.02.003">https://doi.org/10.1016/j.jsp.2018.02.003</a>
</div>
<div id="ref-Pustejovsky2019Procedural" class="csl-entry">
Pustejovsky, J. E. (2019). Procedural sensitivities of effect sizes for single-case designs with directly observed behavioral outcome measures. <em>Psychological Methods</em>, <em>24</em>(2), 217–235. <a href="https://doi.org/10.1037/met0000179">https://doi.org/10.1037/met0000179</a>
</div>
<div id="ref-pustejovsky2023SingleCaseES" class="csl-entry">
Pustejovsky, J. E., Chen, M., &amp; Swan, D. M. and. (2023). <em><span>SingleCaseES</span>: <span>A Calculator</span> for <span>Single-Case Effect Sizes</span></em>.
</div>
<div id="ref-rodgers2021Effects" class="csl-entry">
Rodgers, D. B., Datchuk, S. M., &amp; Rila, A. L. (2021). Effects of a <span>Text-Writing Fluency Intervention</span> for <span>Postsecondary Students</span> with <span>Intellectual</span> and <span>Developmental Disabilities</span>. <em>Exceptionality</em>, <em>29</em>(4), 310–325. <a href="https://doi.org/10.1080/09362835.2020.1850451">https://doi.org/10.1080/09362835.2020.1850451</a>
</div>
<div id="ref-Scotti_Evans_Meyer_Walker_1991" class="csl-entry">
Scotti, J. R., Evans, I. M., Meyer, L. H., &amp; Walker, P. (1991). A meta-analysis of intervention research with problem behavior: Treatment validity and standards of practice. <em>American Journal of Mental Retardation: AJMR</em>, <em>96</em>(3), 233–256.
</div>
<div id="ref-Scruggs_Mastropieri_Casto_1987" class="csl-entry">
Scruggs, T. E., Mastropieri, M. A., &amp; Casto, G. (1987). The quantitative synthesis of single-subject research: Methodology and validation. <em>Remedial and Special Education</em>, <em>8</em>(2), 24–33. <a href="https://doi.org/10.1177/074193258700800206">https://doi.org/10.1177/074193258700800206</a>
</div>
<div id="ref-Shadish_Sullivan_2011" class="csl-entry">
Shadish, W. R., &amp; Sullivan, K. J. (2011). Characteristics of single-case designs used to assess intervention effects in 2008. <em>Behavior Research Methods</em>, <em>43</em>(4), 971–980. <a href="https://doi.org/10.3758/s13428-011-0111-y">https://doi.org/10.3758/s13428-011-0111-y</a>
</div>
<div id="ref-Tarlow_2017" class="csl-entry">
Tarlow, K. R. (2017). An improved rank correlation effect size statistic for single-case designs: Baseline corrected tau. <em>Behavior Modification</em>, <em>41</em>(4), 427–467. <a href="https://doi.org/10.1177/0145445516676750">https://doi.org/10.1177/0145445516676750</a>
</div>
<div id="ref-van2003hierarchical" class="csl-entry">
Van Den Noortgate, W., &amp; Onghena, P. (2003). Hierarchical linear models for the quantitative integration of effect sizes in single-case research. <em>Behavior Research Methods, Instruments, &amp; Computers</em>, <em>35</em>(1), 1–10. <a href="https://doi.org/10.3758/BF03195492">https://doi.org/10.3758/BF03195492</a>
</div>
<div id="ref-White1987some" class="csl-entry">
White, O. R. (1987). Some comments concerning "<span>The</span> quantitative synthesis of single-subject research". <em>Remedial and Special Education</em>, <em>8</em>(2), 34–39. <a href="https://doi.org/10.1177/074193258700800207">https://doi.org/10.1177/074193258700800207</a>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="7">
<li id="fn7"><p>The Single Case Effect Size Calculator <span class="citation">(<a href="#ref-pustejovsky2023SingleCaseES">Pustejovsky et al., 2023</a>)</span> app used in Chapter 10 to model case-specific effect size estimation methods reports NAP on a 0-1 scale.<a href="intro-case-specific-es.html#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>The Single Case Effect Size Calculator <span class="citation">(<a href="#ref-pustejovsky2023SingleCaseES">Pustejovsky et al., 2023</a>)</span> allows the user to choose whether to use a pre-test for significant trend. By default, it will implement the trend adjustment regardless of the statistical significance of the baseline trend.<a href="intro-case-specific-es.html#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p>See for an excellent discussion of the problems associated with trying to extrapolate baselines in SCD studies, as well as some potential solutions.<a href="intro-case-specific-es.html#fnref9" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="MLM-Trend.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="app-case-specific-es.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["SCD-Methods-Guide.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
