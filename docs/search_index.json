[["index.html", "Methods Guide for Effect Estimation and Synthesis of Single-Case Studies Authors Disclaimer Citation Preface Datasets Acknowledgements", " Methods Guide for Effect Estimation and Synthesis of Single-Case Studies November 19, 2023 Authors John M. Ferron, Megan Kirby, and Lodi Lipien University of South Florida James Pustejovsky, Man Chen, and Paulina Grekov University of Wisconsin - Madison Wendy Machalicek University of Oregon Disclaimer This report was supported by the Institute of Education Sciences, U.S. Department of Education, through Grant R324U190002 to the University of Oregon. The opinions expressed are those of the authors and do not represent views of the Institute or the U.S. Department of Education. Citation This report is in the public domain. While permission to reprint this publication is not necessary, it should be cited as: Ferron, J. M., Kirby, M., Lippen, L., Pustejovsky, J. E., Chen, M., Grekov, P., &amp; Machalicek, W. (2023). Effect Size Estimation and Synthesis of Single-Case Designs: A Methods Guide. Institute of Education Sciences. U.S. Department of Education. Washington, DC. Retrieved from https://jepusto.github.io/SCD-Methods-Guide/ Preface We developed the Methods Guide to support educational researchers interested in estimating effect size measures and synthesizing findings from single-case design studies. To do so, we aim to do two things: (a) provide guidance and decision rules to simplify the process of selecting effect size estimation and synthesis methods and (b) illustrate the step-by-step application of appropriate methods using readily available tools. The guide is not meant to cover all effect size methods that have been proposed for single-case design studies, nor is it meant to summarize all of the research on effect size estimation and synthesis of single-case research. Rather, it is meant to provide easy-to-follow decision rules, along with step-by-step instructions and illustrations of user-friendly tools, so that researchers aiming to conduct a synthesis involving single-case studies are more readily able to do so. The guide is organized so that researchers can go immediately to the sections and chapters that are relevant to their immediate task, rather than having to read the guide sequentially from start to finish. The first chapter provides background and a flow chart (also shown below) to help researchers select among three broad approaches to synthesizing results from single-case research. Figure 0.1: Approaches to synthesizing findings from single-case research Those interested in synthesizing both single-case and group design studies should consider the design-comparable effect size approach and start by reading Chapter 2, which introduces this approach and its assumptions. They can then turn to one of the more detailed chapters providing step-by-step illustrations of how to estimate design comparable effect sizes with a user-friendly app (Chapter 3 if there are no trends, Chapter 4 if there are trends only in the treatment phases, or Chapter 5 if there are trends in both the baseline and treatment phases). Those interested in synthesizing single-case design studies that share a common outcome, and particularly if they are interested in examining changes in the effect over time, should consider the multilevel modeling approach and read Chapter 6, which introduces this approach and its assumptions. They can then turn to one of the more detailed chapters providing step-by-step instructions along with illustrations of how to estimate multilevel models to synthesize single-case data with a user-friendly app (Chapter 7 if there are no trends, or Chapter 8 if there are trends). Finally, those interested in synthesizing single-case design studies where the outcome measures vary across cases, and especially if they are interested in examining how the size of the effect varies from case to case, should consider the case-specific effect size approach. They can turn to Chapter 9, which introduces this approach and its assumptions, and Chapter 10, which provides step-by-step instructions along with illustration of how to estimate case-specific effect sizes with a user-friendly app. Datasets Acknowledgements We are grateful for the guidance from Katherine Taylor, who served as our project officer at the Institute of Education Sciences. We are also grateful for the suggestions and comments we received from the members of our external advisory panel: Jennifer Ledford, Joel Levin, Daniel Swan, Tai Collins, Katie Wolfe, and Kimberly Vannest. "],["intro.html", "Chapter 1 Approaches for Effect Size Estimation and Synthesis of Single-Case Designs (the old version DO NOT USE) 1.1 Background 1.2 Purpose of the Methods Guide 1.3 Study Quality 1.4 Selecting an Approach for Effect Estimation and Synthesis 1.5 Structure of the Methods Guide", " Chapter 1 Approaches for Effect Size Estimation and Synthesis of Single-Case Designs (the old version DO NOT USE) This chapter provides the background and purpose for this methods guide. It also gives an overview of the three major approaches for estimation and synthesis of single-case studies: design-comparable effect sizes, case-specific effect sizes, and multilevel modeling of the raw individual participant interrupted time-series data. We describe the motivation and rationale for each of these approaches and provide a series of decision rules to guide researchers in selecting among them. The remaining chapters provide details and illustrations of the methods. 1.1 Background Educational decisions at the state, district, school, and student level are now expected to be informed by empirical evidence (“Council for Exceptional Children,” 2014; What Works Clearinghouse, 2020b). These expectations create a major need for synthesis, or the integration of research findings from multiple, existing sources of evidence—including findings from single-case designs (SCDs). (H. M. Cooper, 2010; Pustejovsky &amp; Ferron, 2017)y developments in education related to the documentation of evidence-based practices. The first is the expanded use of SCD research methods across varying disciplines over the last 50 years, in both general and special education contexts (Kratochwill et al., 2013). Although the history of SCD methodology is beyond our present scope, innovations in design have expanded the use of SCDs across a range of professional fields, moving beyond SCD’s quasi-experimental and behavior analysis origins (see Kratochwill et al., 2013 for more detailed information on the history of SCD). Over the past 20 years, researchers’ commitment to using rigorous procedures to identify evidence-based educational practices affirms not only the importance of randomized control trials (RCTs), but the effectiveness and efficiency of SCD. The second development is the emergence of effect size (ES) measures and synthesis methods for use with SCD and other interrupted time series data (Shadish et al., 2015; Swaminathan et al., 2014). Interest in synthesis of SCDs is long-standing (e.g., Center et al., 1985; Gingerich, 1984; White, 1987), emerging around the same time as methods for meta-analysis of group designs were becoming more statistically rigorous and sophisticated (Hedges &amp; Olkin, 1985; Shadish &amp; Lecy, 2015). However, the past twenty years have seen increased intensity of methodological research focused on SCDs and a substantial expansion in the diversity, flexibility, and accessibility of available analytic methods upon which researchers can draw. We view the current state of methodology as falling into three strands: approaches that summarize the effect of each case and then synthesize these case-specific effect sizes (e.g., Pustejovsky, 2018), methods that use multi-level models for analyzing the raw (or standardized) data from one or multiple SCD studies (Van den Noortgate &amp; Onghena, 2008), and techniques that use design-comparable effect size metrics (Hedges et al., 2012, 2013; Pustejovsky et al., 2014; Shadish et al., 2014; Swaminathan et al., 2014). The present guide is organized around these three broad methodological strands. The third development is the increased use of systematic review and meta-analysis procedures to identify and affirm evidence-based practices in education (Beretvas &amp; Chung, 2008; Shadish &amp; Rindskopf, 2007). Maggin and colleagues (2011) examined the production of systematic reviews and meta-analyses of SCDs from 1985 to 2009, finding a marked increase in the appearance of such reviews between 2000 and 2009. In another survey, Jamshidi and colleagues (2018) found increasing production through 2015, as well as improvements in the quality of published reviews. However, they also noted that, even in more recent literature, reviews often frequently failed to use appropriate methods for combining findings across studies (Jamshidi et al., 2018). Thus, there remains a need for guidance about how to select and apply methods for synthesizing findings from SCD research. Effect size (ES) measures are a critical companion to visual analysis for the interpretation of single-case research results, and are a key input in two of the available approaches for meta-analytic synthesis of SCDs. Although a variety of technically sound ES metrics exist for researchers to use when interpreting SCD findings, relatively few published meta-analyses use these more advanced techniques (cf. Barton et al., 2017; Jamshidi et al., 2018; Maggin et al., 2017). One reason for the lack of widespread use by researchers may be the conceptual and procedural complexity associated with advances in ES measures and meta-analysis techniques. A more rapid uptake of ES estimation methods may be hindered by the complexity of data extraction and calculation of ES for SCD. In addition, researchers may lack software tools for ES estimation for single-case studies when using some common statistical packages such as SPSS and SAS (Odom et al., 2018; Pustejovsky et al., 2014; Shadish et al., 2015). 1.2 Purpose of the Methods Guide The purpose of this methods guide is to improve educational researchers’ practice in the estimation of ES measures and synthesis of findings from SCDs. We recognize that no single method is ideal for all research goals. Furthermore, methods that have the most to offer can be complex and may appear difficult to carry out. Thus, through the use of this methods guide, we aim to: (a) provide guidance and decision rules to simplify the process of selecting effect size estimation and synthesis methods and (b) illustrate the step-by-step application of appropriate methods using readily available tools, such as web-based software to calculate case-specific ES or design-comparable ES for SCD studies (e.g., Pustejovsky et al., 2021). 1.3 Study Quality Conducting a research synthesis (whether of group design studies, SCDs, or both) involves several stages, starting with formulating the research aims, specifying inclusion criteria, conducting a systematic search, and screening for eligible studies (H. M. Cooper, 2010; Pustejovsky &amp; Ferron, 2017). Additionally, before carrying out effect size calculations or meta-analysis, it is critical to consider the methodological quality and potential biases of studies to be included in the synthesis. To assist researchers in doing so, several distinct sets of standards are available for SCD studies (e.g., Kratochwill et al., 2013; Reichow et al., 2018; Tate et al., 2016; Zimmerman et al., 2018). After assessing methodological quality, researchers can use one of two strategies to guide the estimation and synthesis of effect sizes. One strategy incorporates consideration of study quality as an inclusion criteria, so that low-quality studies are screened out and synthesis is based on the subset of studies with quality high enough so that changes in outcome(s) can reasonably be attributed to the intervention. For example, the What Works Clearinghouse Single-Case Pilot Standards (Kratochwill et al., 2010) indicated that effect sizes are only to be computed after studies have been shown to meet both minimum design criteria (e.g., a multiple-baseline study has at least three temporally spaced opportunities for the effect to be demonstrated, along with phases of at least three observations) and minimum evidence criteria (e.g., visual analysis of the data from the study show experimental control so that the changes in the outcome can be attributed to the intervention). Concerns with estimating effect sizes for SCD studies without the use visual analysis to rule out alternative explanations for observed changes in the outcomes continues to be echoed in the literature (Kratochwill et al., 2021; Maggin et al., 2021). An alternative strategy in considering study quality is to use broader inclusion criteria, code for aspects study quality, and examine study quality codes as potential moderators in a meta-analysis. With this approach, researchers can estimate ESs for studies of varying degrees of quality and empirically explore whether the magnitude of ESs varies depending on aspects of study quality. We assume that researchers who use this methods guide will have already selected a method to assess study quality and an approach for incorporating those assessments into their synthesis. As such, we do not focus on SCD study quality assessment methods, but rather provide guidelines that can be applied to a collection of studies that have met the researchers’ inclusion criteria, which potentially include criteria related to study quality. Thus, in this guide we focus on the final stages of a research synthesis, on the questions of how to select a method for estimating effects, how to compute ES estimates (or otherwise prepare data for synthesis), and how to synthesize findings in the form of ES estimates or individual-level data. 1.4 Selecting an Approach for Effect Estimation and Synthesis In order to select an approach for estimating and synthesizing effects from SCDs, we recommend that researchers first reflect on the research aims that motivate their synthesis. In some contexts, researchers’ primary aims may be focused on summarizing evidence to arrive at statements about average efficacy of a class of interventions. In other contexts, researchers might instead or additionally be interested in understanding variation in effects and the extent to which such variation is associated with characteristics of participants, specific features of interventions, or other contextual factors. When the focus is mostly on summarization, researchers may find it more useful to use design-comparable effect sizes that describe average effects. If individual-level variation is the focus, then approaches using case-specific effect sizes or multi-level modeling may be more advantageous. Another over-arching consideration for selecting a synthesis approach pertains to the features of the studies identified for inclusion in the synthesis. Quantitative synthesis requires choosing an effect size metric that permits comparisons of the magnitude of effects across individual participants and studies. Consequently, the extent to which eligible studies use different types of designs or different types of outcome measurements creates constraints on how effects from those studies can be described or compared. For instance, if all eligible studies used multiple baseline designs across participants (or another common type of SCD), then several different synthesis approaches are feasible. In contrast, if eligible studies include both SCDs and group design studies (such as small randomized experiments, each with a single pre-test and a single post-test), then researchers need a synthesis approach that permits comparisons across both types of designs. Similarly, if all eligible studies used SCDs with very similar methods for assessing the dependent variable, then synthesis based on multi-level modeling of raw data is possible. In contrast, if eligible studies used a variety of different assessments, then a synthesis approach based on case-specific effect sizes may be required. These two broad considerations—the aims of the synthesis and the features of eligible studies—can guide the selection of an approach for synthesis of SCDs. We now describe in more detail how three broad approaches to synthesis fit within these considerations. 1.4.1 Design-Comparable Effect Sizes In some situations, the aim of the research team is to synthesize the evidence for intervention effectiveness from both single-case and group design studies. For example, a meta-analysis by Wood and colleagues (2018) analyzed 22 single-case and between-group studies to examine the effects of text-to-speech and other read-aloud tools on reading comprehension outcomes for students with reading disabilities. The authors used the standardized mean difference to estimate read-aloud intervention effects in the group design studies and a comparable standardized mean difference from the included single-case research, resulting in an overall average weighted effect size of \\(d\\) = 0.35, 95% CI (0.14, 0.56). Because the purpose of the Wood et al. (2018) study involved the comparison and averaging of effects across single-case and group designs, it was critical to use an ES metric that is theoretically comparable across the designs. In similar situations, researchers should select from the design-comparable effect size options (Hedges et al., 2012, 2013; Pustejovsky et al., 2014; Shadish et al., 2014; Swaminathan et al., 2014; Van den Noortgate &amp; Onghena, 2008). However, if researchers aim to synthesize findings from only single-case studies (i.e., not to integrate findings across single-case and group design studies), other options may be preferable. 1.4.2 Case-Specific Effect Sizes In addition to summarizing average effects across studies, researchers may also be interested in exploring variation in treatment effects across individual participants. When dependent variables are measured differently across studies, it is important for researchers to use an effect size metric and synthesis approach that accounts for such. For example, Bowman-Perrott and colleagues (2016) examined five potential moderators of the effectiveness of the Good Behavior Game in promoting positive behavior in the classroom. Results of their meta-analysis suggested that the intervention was most effective in reducing problem behaviors among students with or at risk for emotional and behavioral disorders. Another meta-analysis by Mason and colleagues (2016) investigated the moderating effects of participant characteristics, targeted outcomes, and implementation components on the efficacy of video self-modeling, in which a learner with disabilities watches a video of a model engaged in targeted skills or tasks. They found that intervention effects were stronger for younger participants with autism-spectrum disorders. Because these syntheses focused on investigating variation across individuals in the effect of treatment, it was important that the effect size estimation and synthesis approach produced effect estimates for each individual participant, rather than a study-level summary effect estimates. In addition, because the outcome measure differed among studies, the researchers needed an individual-level effect size metric that is not scale-dependent (e.g., they could not be based on simple raw score mean differences). In contexts like these, researchers should consider selecting among the case-specific effect size estimation and synthesis options. The case-specific effect size options are not viable for synthesis across single-case and group design studies because the case-specific effects are not comparable to the group design effects. However, the design-comparable effects are not viable for studying within-participant variation between individuals in treatment effectiveness because they produce effect estimates at the study level (i.e., the effect averaged across the cases), not the individual level. 1.4.3 Multilevel Modeling of Individual Participant Interrupted Time-Series Data Finally, researchers might have identified a set of SCDs that all use the same or very similar outcome measures, with the aim of studying the variation in effects over time within and between individuals. For example, Datchuk and colleagues (Datchuk et al., 2020) meta-analyzed 15 single-case studies with 79 students to examine the effects of an intervention on the level and trend in correct writing sequences per minute for students with disabilities. They found the effect increased with time in intervention (i.e., there was a positive effect on the trend) and that this temporal change in effect was more pronounced with younger students. When focusing on both variation in effect over time and variation in effect across cases, it is important that the researchers select a meta-analytic approach that does not rely only on a single effect estimate for a study (e.g., design-comparable effect sizes) or even a single effect estimate for a case (e.g., case-specific effect sizes). In contexts like this, we suggest researchers consider options for multilevel modeling of individual participant data series. 1.4.4 Summary of Options for Effect Estimation and Synthesis The flow chart in Figure 1.1 illustrates a set of heuristic decision rules for selecting among the three general approaches to estimating and synthesizing single-case research. If the primary purpose of one’s research is to integrate findings from both single-case and group-design studies, the researcher should consider design-comparable effect sizes. Alternately, if the primary purpose of the research is to integrate findings from SCDs only, additional questions should be addressed related to the measurement of the dependent variable. If the outcome of interest is measured in different ways for different cases and the researcher aims to examine how effects vary across the cases, then the researcher should consider the options for estimating and synthesizing case-specific effect sizes. If the outcome is measured the same way across cases, and there is interest in how the effect changes over the course of an intervention, the researcher should consider multilevel modeling of the raw data series. Figure 1.1: Flow chart showing the approach for synthesizing effect evidence based on the types of studies being examined and whether the outcome variable is common across cases. 1.4.5 Limitations in Selecting an Approach for Effect Estimation and Synthesis We emphasize that Figure 1.1 presents a heuristic, simplified procedure for the selection among the three general approaches to ES estimation and synthesis, which does cover every possible research context. There will surely be situations where researchers’ aims and contexts differ from those we describe, and thus do not align perfectly with one of our primary approaches to estimating and synthesizing single-case effect sizes. For example, researchers who are synthesizing findings from a set of SCDs may wish to compare their results to a previously published meta-analysis of group design studies, but not to investigate individual-level variation in treatment effects. They may therefore elect to use design-comparable effect sizes even though they are not formally integrating results from group design studies within their review. A further possibility is that researchers might elect to use multiple approaches to synthesis in order to address different aims or questions. For example, consider a project in which researchers have identified both single-case and group design studies. They might want to integrate findings across design types while also exploring the variation in effects among individuals. In this scenario, researchers could estimate design-comparable effect sizes for their first aim and case-specific effect sizes from the subset of single-case studies for their second aim. We also note that there may be situations that falls in between those we described for case-specific effect sizes and those for multilevel modeling of the raw data series. For example, researchers may want to examine how effects vary over time and across cases, using studies with different outcomes. For this purpose, the researchers can use extensions of the primary approaches we present. The researchers could either standardize the raw data before estimating a multilevel model, or they could synthesize case specific effect sizes where they use multiple standardized effects for each case (e.g., an effect that indexes the immediate shift in level, and another effect that indexes a change in slope). Our simplified selection method cannot exhaustively cover all currently available options. Finally, we anticipate that the heuristic guidance we provide here will need to be refined over time, as further methodological innovations become available. We anticipate that research presently underway will provide even more meta-analytic options in the future, with implications for how to select an approach for synthesis. At some point it may be possible to compute case-specific effect sizes that are also design-comparable, or it may be possible to standardize the data for multilevel models in a way that leads to parameter estimates from the model that correspond to design-comparable effect estimates. If such methods become available, some of the distinctions made here will become artificial. In the time, remainder of this methods guide follows the proposed heuristics for selecting among the three major approaches to effect size estimation and synthesis. Even as methodology continues to advance, researchers need guidance that acknowledges the complexity of research purposes and contexts and is dynamic in its accommodation of such variation, while also being concrete and straightforward enough to be put into practice. 1.5 Structure of the Methods Guide We divide the remainder of this guide into three major sections: design-comparable effect size estimation and synthesis, case-specific effect size estimation and synthesis, and multilevel modeling to estimate and synthesize effects. These sections do not build upon each other or need to be read in order. Rather, we expect those using this guide to follow the decision rules in Figure 1.1 to determine which section of the guide is most appropriate for them, and then to jump immediately to that section. Each major section is divided into chapters. The initial chapter of each section introduces the specific approach and its assumptions, discusses when to use it, and what options exist within the approach. Furthermore, we provide additional decision rules for selecting among the specific techniques and options available with a given broad approach. We encourage researchers to use the decision rules within the initial chapter of the major section to get to an appropriate option. Then, researchers can directly refer to the chapter that has the illustration for that specific option. For each illustration, we: describe the purposes for estimating and synthesizing effects, and the available data, demonstrate how to use the decision rules in Figure 1.1, along with the additional decision rules within the initial chapter of the major section, to arrive at the option being illustrated, present the data for the illustration showing how it needs to be structured for the analysis, and provide a step-by-step illustration of how to estimate and synthesize effects using readily available analysis tools. References Barton, E. E., Pustejovsky, J. E., Maggin, D. M., &amp; Reichow, B. (2017). Technology-Aided Instruction and Intervention for Students With ASD: A Meta-Analysis Using Novel Methods of Estimating Effect Sizes for Single-Case Research. Remedial and Special Education, 38(6), 371–386. https://doi.org/10.1177/0741932517729508 Beretvas, S. N., &amp; Chung, H. (2008). A review of meta-analyses of single-subject experimental designs: Methodological issues and practice. Evidence-Based Communication Assessment and Intervention, 2(3), 129–141. https://doi.org/10.1080/17489530802446302 Bowman-Perrott, L., Burke, M. D., Zaini, S., Zhang, N., &amp; Vannest, K. (2016). Promoting Positive Behavior Using the Good Behavior Game: A Meta-Analysis of Single-Case Research. Journal of Positive Behavior Interventions, 18(3), 180–190. https://doi.org/10.1177/1098300715592355 Center, B. A., Skiba, R. J., &amp; Casey, A. (1985). A methodology for the quantitative synthesis of intra-subject design research. The Journal of Special Education, 19(4), 387. Cooper, H. M. (2010). Research Synthesis and Meta-Analysis (4th ed.). SAGE Publications. Council for Exceptional Children: Standards for Evidence-Based Practices in Special Education. (2014). TEACHING Exceptional Children, 46(6), 206–212. https://doi.org/10.1177/0040059914531389 Datchuk, S. M., Wagner, K., &amp; Hier, B. O. (2020). Level and Trend of Writing Sequences: A Review and Meta-Analysis of Writing Interventions for Students With Disabilities. Exceptional Children, 86(2), 174–192. https://doi.org/10.1177/0014402919873311 Gingerich, W. J. (1984). Meta-analysis of applied time-series data. The Journal of Applied Behavioral Science, 20(1), 71–79. https://doi.org/10.1177/002188638402000113 Hedges, L. V., &amp; Olkin, I. (1985). Statistical Methods for Meta-Analysis. Academic Press. Hedges, L. V., Pustejovsky, J. E., &amp; Shadish, W. R. (2012). A standardized mean difference effect size for single case designs. Research Synthesis Methods, 3, 224–239. https://doi.org/10.1002/jrsm.1052 Hedges, L. V., Pustejovsky, J. E., &amp; Shadish, W. R. (2013). A standardized mean difference effect size for multiple baseline designs across individuals. Research Synthesis Methods. https://doi.org/10.1002/jrsm.1086 Jamshidi, L., Heyvaert, M., Declercq, L., Fernández-Castilla, B., Ferron, J. M., Moeyaert, M., Beretvas, S. N., Onghena, P., &amp; Van den Noortgate, W. (2018). Methodological quality of meta-analyses of single-case experimental studies. Research in Developmental Disabilities, 79, 97–115. https://doi.org/10.1016/j.ridd.2017.12.016 Kratochwill, T. R., Hitchcock, J. H., Horner, R. H., Levin, J. R., Odom, S. L., Rindskopf, D. M., &amp; Shadish, W. R. (2013). Single-Case Intervention Research Design Standards. Remedial and Special Education, 34(1), 26–38. https://doi.org/10.1177/0741932512452794 Kratochwill, T. R., Hitchcock, J., Horner, R. H., Levin, J. R., Odom, S. L., Rindskopf, D. M., &amp; Shadish, W. R. (2010). Single-Case Designs Technical Documentation, Version 1.0 (Pilot) (June; Vol. 0). What Works Clearinghouse. Kratochwill, T. R., Horner, R. H., Levin, J. R., Machalicek, W., Ferron, J., &amp; Johnson, A. (2021). Single-case design standards: An update and proposed upgrades. Journal of School Psychology, 89, 91–105. https://doi.org/10.1016/j.jsp.2021.10.006 Maggin, D. M., Barton, E., Reichow, B., Lane, K., &amp; Shogren, K. A. (2021). Commentary on the What Works Clearinghouse Standards and Procedures Handbook (v. 4.1) for the Review of Single-Case Research. Remedial and Special Education, 074193252110513. https://doi.org/10.1177/07419325211051317 Maggin, D. M., O’Keeffe, B. V., &amp; Johnson, A. H. (2011). A Quantitative Synthesis of Methodology in the Meta-Analysis of Single-Subject Research for Students with Disabilities: 1985. Exceptionality, 19(2), 109–135. https://doi.org/10.1080/09362835.2011.565725 Maggin, D. M., Pustejovsky, J. E., &amp; Johnson, A. H. A. H. (2017). A meta-analysis of school-based group contingency interventions for students with challenging behavior: An update. Remedial and Special Education, 38(6), 353–370. https://doi.org/10.1177/0741932517716900 Mason, R. A., Davis, H. S., Ayres, K. M., Davis, J. L., &amp; Mason, B. A. (2016). Video Self-Modeling for Individuals with Disabilities: A Best-Evidence, Single Case Meta-Analysis. Journal of Developmental and Physical Disabilities, 28(4), 623–642. https://doi.org/10.1007/s10882-016-9484-2 Odom, S. L., Barton, E. E., Reichow, B., Swaminathan, H., &amp; Pustejovsky, J. E. (2018). Between-case standardized effect size analysis of single case designs: Examination of the two methods. Research in Developmental Disabilities, 79, 88–96. https://doi.org/10.1016/j.ridd.2018.05.009 Pustejovsky, J. E. (2018). Using response ratios for meta-analyzing single-case designs with behavioral outcomes. Journal of School Psychology, 68, 99–112. https://doi.org/10.1016/j.jsp.2018.02.003 Pustejovsky, J. E., Chen, M., &amp; Hamilton, B. (2021). Scdhlm: A web-based calculator for between-case standardized mean differences. Pustejovsky, J. E., &amp; Ferron, J. (2017). Research Synthesis and Meta-Analysis of Single-Case Designs. In Handbook of Special Education (2nd Edition, p. 63). Routledge. Pustejovsky, J. E., Hedges, L. V., &amp; Shadish, W. R. (2014). Design-comparable effect sizes in multiple baseline designs: A general modeling framework. Journal of Educational and Behavioral Statistics, 39(5), 368–393. https://doi.org/10.3102/1076998614547577 Reichow, B., Barton, E. E., &amp; Maggin, D. M. (2018). Development and applications of the single-case design risk of bias tool for evaluating single-case design research study reports. Research in Developmental Disabilities, 79, 53–64. https://doi.org/10.1016/j.ridd.2018.05.008 Shadish, W. R., Hedges, L. V., Horner, R. H., &amp; Odom, S. L. (2015). The Role of Between-Case Effect Size in Conducting, Interpreting, and Summarizing Single-Case Research (NCER 2015-002; p. 109). National Center for Education Research, Institute of Education Sciences, U.S. Department of Education. Shadish, W. R., Hedges, L. V., &amp; Pustejovsky, J. E. (2014). Analysis and meta-analysis of single-case designs with a standardized mean difference statistic: A primer and applications. Journal of School Psychology, 52(2), 123–147. https://doi.org/10.1016/j.jsp.2013.11.005 Shadish, W. R., &amp; Lecy, J. D. (2015). The meta-analytic big bang. Research Synthesis Methods, 6(3), 246–264. https://doi.org/10.1002/jrsm.1132 Shadish, W. R., &amp; Rindskopf, D. M. (2007). Methods for evidence-based practice: Quantitative synthesis of single-subject designs. New Directions for Evaluation, 113(113), 95–109. https://doi.org/10.1002/ev.217 Swaminathan, H., Rogers, H. J., &amp; Horner, R. H. (2014). An effect size measure and bayesian analysis of single-case designs. Journal of School Psychology, 52(2), 213–230. https://doi.org/10.1016/j.jsp.2013.12.002 Tate, R. L., Perdices, M., Rosenkoetter, U., Togher, L., McDonald, S., Shadish, W., Horner, R., Kratochwill, T., Barlow, D. H., Kazdin, A., Sampson, M., Shamseer, L., &amp; Vohra, S. (2016). The Single-Case Reporting Guideline In BEhavioural Interventions (SCRIBE) 2016: Explanation and Elaboration. 22. Van den Noortgate, W., &amp; Onghena, P. (2008). A multilevel meta-analysis of single-subject experimental design studies. Evidence-Based Communication Assessment and Intervention, 2(3), 142–151. https://doi.org/10.1080/17489530802505362 What Works Clearinghouse. (2020b). What Works Clearinghouse Standards Handbook (Version 4.1). U.S. Department of Education, Institute of Education Sciences, National Center for Education Evaluation and Regional Assistance. White, O. R. (1987). Some comments concerning \"The quantitative synthesis of single-subject research\". Remedial and Special Education, 8(2), 34–39. https://doi.org/10.1177/074193258700800207 Wood, S. G., Moxley, J. H., Tighe, E. L., &amp; Wagner, R. K. (2018). Does Use of Text-to-Speech and Related Read-Aloud Tools Improve Reading Comprehension for Students With Reading Disabilities? A Meta-Analysis. Journal of Learning Disabilities, 51(1), 73–84. https://doi.org/10.1177/0022219416688170 Zimmerman, K. N., Ledford, J. R., Severini, K. E., Pustejovsky, J. E., Barton, E. E., &amp; Lloyd, B. P. (2018). Single-case synthesis tools I: Comparing tools to evaluate SCD quality and rigor. Research in Developmental Disabilities, 79, 19–32. https://doi.org/10.1016/j.ridd.2018.02.003 "],["approaches-for-estimation-and-synthesis-of-single-case-studies.html", "Chapter 2 Approaches for Estimation and Synthesis of Single-Case Studies 2.1 Background 2.2 Purpose of the Methods Guide 2.3 Study Quality 2.4 Design-Comparable Effect Sizes 2.5 Case-Specific Effect Sizes 2.6 Multilevel Modeling of Individual Participant Interrupted Time-Series Data 2.7 Summary of Options for Effect Estimation and Synthesis 2.8 Limitations in Selecting an Approach for Effect Estimation and Synthesis 2.9 Structure of the Methods Guide", " Chapter 2 Approaches for Estimation and Synthesis of Single-Case Studies This chapter provides the background and purpose for the Methods Guide for Effect Estimation and Synthesis of Single-Case Studies. We provide an overview of the three major approaches for effect size estimation and synthesis of single-case studies: (a) design-comparable effect sizes, (b) multilevel modeling of raw individual participant interrupted time-series data, and (c) case-specific effect sizes. We describe the general motivation and rationale for each approach and provide a series of decision rules to guide researchers in their selection. Subsequent chapters in this guide provide more detailed illustrations of the three major approaches. 2.1 Background Educational decisions made at the state, district, school, and student levels are expected to be informed by empirical evidence (Cook et al., 2014; What Works Clearinghouse, 2020a). These expectations create a compelling need for synthesis, or the integration of research findings from multiple, existing sources of evidence—including findings from single-case designs (SCDs). The need for methods to synthesis findings from SCDs is all the greater because some educational disciplines have historically relied upon this methodology to collect evidence about interventions, such that the bulk of available evidence about some interventions comes from SCDs. This methods guide responds to three key developments in educational research related to the documentation of evidence-based practices. The first is the expanded use of SCD research methods across disciplines over the last 50 years in both general and special education contexts (L. Kratochwill Thomas R. &amp; Swoboda, 2014). Although the history of SCD methodology is outside our present scope, SCD design innovations have allowed researchers to advance its use beyond quasi-experimental and behavior analysis origins (see L. Kratochwill Thomas R. &amp; Swoboda, 2014 for more detailed information on the history of SCD). Over the past 20 years, researchers’ commitment to using rigorous procedures to identify evidence-based educational practices affirms not only the importance of randomized control trials (RCTs), but also the utility and efficiency of SCDs. The second key development in education research is the emergence of effect size measures and synthesis methods for use with SCD and other interrupted time-series data (Shadish et al., 2015; Swaminathan et al., 2014). Interest in the synthesis of SCDs is long-standing (e.g., Center et al., 1985; Gingerich, 1984; White, 1987), with methods arising concurrently with the meta-analyses of group design research becoming more statistically rigorous and sophisticated (Hedges &amp; Olkin, 1985; Shadish &amp; Lecy, 2015). The interest and intensity of methodological research focused on SCDs has increased over time, resulting in a substantial expansion in the diversity, flexibility, and accessibility of available analytic methods. The current state of SCD effect size estimation methods can be categorized into three strands: (a) approaches that summarize the effect for each case and then synthesize these case-specific effect sizes (e.g., Pustejovsky, 2018), (b) methods that use multi-level models to analyze the raw or standardized data from one or multiple SCD studies (Van den Noortgate &amp; Onghena, 2008), and (c) techniques that use design-comparable effect size metrics (Hedges et al., 2012, 2013; Pustejovsky et al., 2014; Shadish et al., 2014; Swaminathan et al., 2014). We organize the present methods guide around these three broad methodological strands. The third development in education research is the increased use of systematic review and meta-analysis procedures to identify and affirm evidence-based practices in education (Beretvas &amp; Chung, 2008; Shadish &amp; Rindskopf, 2007). In examining the production of systematic reviews and meta-analyses of SCDs from 1985 to 2009, Maggin et al. (2011) found a marked increase in such products between 2000 and 2009. Similarly, Jamshidi et al. (2018) found increasing production through 2015, as well as improvements in the quality of published reviews. However, even contemporary reviews frequently fail to use appropriate methods for combining findings across studies (Jamshidi et al., 2018). Thus, there remains a need for guidance about how to select and apply methods for synthesizing SCD research results. Effect size measures are a valuable complement to visual analysis for the interpretation of single-case research results and are a key input in two of the available approaches for meta-analytic synthesis of SCDs: case-specific and design-comparable effect size estimation methods. Researchers have a variety of technically sound effect size metrics to select from when interpreting SCD findings, but relatively few published meta-analyses use design-comparable effect sizes, multilevel modeling, or more advanced case-specific effect sizes (cf. Barton et al., 2017; Jamshidi et al., 2018; Maggin2017meta?). One reason for their lack of widespread use by researchers may be the conceptual and procedural complexity associated with advances in effect size measures and meta-analysis techniques. The complexity of data extraction and calculation of effect sizes for SCDs may also hinder a more rapid uptake of effect size estimation methods. In addition, researchers may lack software tools for effect size estimation for single-case studies when using some common statistical packages such as SPSS and SAS (Odom et al., 2018; Shadish et al., 2014; Shadish et al., 2015). 2.2 Purpose of the Methods Guide The purpose of this methods guide is to improve educational researchers’ practice of estimating effect size measures and synthesizing findings from SCDs. We recognize that no single method is ideal for all research goals. Furthermore, methods that have the most to offer can be complex and may appear difficult to carry out. Through the use of this methods guide, we aim to (a) provide guidance and decision rules to simplify the process of selecting effect size estimation and synthesis methods, and (b) illustrate the step-by-step application of appropriate methods using readily available tools, such as web-based software to calculate case-specific effect sizes or design-comparable effect sizes for SCD studies. 2.3 Study Quality Conducting a research synthesis—composed of group design studies, SCDs, or both—involves several stages: formulating the research aims, specifying inclusion criteria, conducting a systematic search, and screening for eligible studies (Cooper, 2010; Pustejovsky &amp; Ferron, 2017). Additionally, before carrying out effect size calculations or meta-analysis, it is critical to consider the methodological quality and potential biases of studies one includes in the synthesis. Several distinct sets of standards are available for SCD studies to assist researchers with assessing methodological quality (e.g., Kratochwill et al., 2013, 2021; Reichow et al., 2018; Tate et al., 2016; Zimmerman et al., 2018). After examining the methodological quality of studies eligible for inclusion in a review, researchers can use one of two strategies to guide their estimation and synthesis of effect sizes. One strategy incorporates consideration of study quality as an inclusion criterion. Researchers can screen studies and exclude low-quality studies so that the synthesis is based on a subset of studies with quality deemed adequate or high enough that outcome(s) can be attributed to the intervention. For example, the What Works Clearinghouse (2022) indicates that for researchers to include an SCD study in a meta-analysis, the study must meet minimum design criteria (e.g., a multiple baseline study has at least three temporally spaced opportunities for the effect to be demonstrated, along with phases of at least three observations) and must provide minimum evidence of experimental control over extraneous factors (e.g., baselines do not document a therapeutic trend). Other screening approaches that rely on visual analysis have also been suggested (Kratochwill et al., 2021; Maggin et al., 2021). Alternatively, researchers can use broader inclusion criteria, but then carefully code for aspects of study quality, so that they can examine study quality codes as potential moderators in a meta-analysis. With this approach, researchers can estimate effect sizes for studies of varying degrees of quality and empirically explore whether the variation in effect size across studies is dependent on aspects of study quality. We assume that researchers who use this methods guide will have already selected a method to assess study quality and an approach for incorporating those assessments into their synthesis. Thus, we do not focus on SCD study quality assessment methods, but rather provide guidelines that researchers can apply to a collection of studies meeting their inclusion criteria, which potentially include study quality. In this guide we focus on the final stages of a research synthesis—those involving questions of how to select a method for estimating effects, how to compute effect size estimates (or otherwise prepare data for synthesis), and how to synthesize findings in the form of effect size estimates or individual-level data. ## Selecting an Approach for Effect Estimation and Synthesis To select an approach for estimating and synthesizing effects from SCDs, researchers should first reflect on the research aims that motivate their synthesis. In some contexts, researchers’ primary aims may be summarizing evidence to arrive at statements about the average efficacy of a class of interventions. In other circumstances, researchers might be interested in understanding variation in effects and the extent to which such variation is associated with participants’ characteristics, specific intervention features, or other contextual factors. When summarization is a priority, researchers may find it more useful to use design-comparable effect sizes that describe average effects. If individual-level variation is the focus, then approaches using case-specific effect sizes or multilevel modeling may be more advantageous. Another overarching consideration for selecting a synthesis approach pertains to the features of the included studies. Quantitative synthesis requires choosing an effect size metric that permits comparisons of the magnitude of effects across individual participants and studies. Consequently, the extent to which eligible studies use different types of designs or different outcome measures constrains how effects from those studies can be described or compared. For instance, if all eligible studies in the review used multiple baseline designs across participants (or another common type of SCD), then several different synthesis approaches are feasible. In contrast, if eligible studies include both single-case and group design studies (e.g., small randomized experiments, each with a single pre-test and a single post-test), researchers may seek a synthesis approach that permits comparisons across both design types. If all eligible studies used SCDs with very similar methods for assessing the dependent variable, then synthesis based on multilevel modeling of raw data is possible. In contrast, if eligible studies used non-equivalent assessments, then researchers may need to use a synthesis approach based on case-specific effect sizes that are suitable for comparison across studies involving different assessments. These two broad considerations—the aims of the synthesis and the features of eligible studies—should guide the selection of an approach for synthesis of SCDs. We now detail how the three broad synthesis approaches fit within these considerations. 2.4 Design-Comparable Effect Sizes In some situations, researchers aim to synthesize the evidence for intervention effectiveness using both single-case and group design studies. For example, a meta-analysis by Wood et al. (2018) analyzed 22 single-case and between-group studies to examine the effects of text-to-speech and other read-aloud tools on reading comprehension outcomes for students with reading disabilities. The authors used the standardized mean difference to estimate read-aloud intervention effects in the group design studies and a comparable standardized mean difference to estimate effects from the included SCD research, resulting in an overall average weighted effect size of d= 0.35, 95% confidence interval (CI) [0.14, 0.56]. Because the purpose of the study involved the comparison and averaging of effects across single-case and group designs, it was critical that Wood et al. (2018) used an effect size metric that is theoretically comparable across the designs. Researchers should select from the design-comparable effect size options (e.g., accounting for the absence or presence of baseline trends) when the aim is to compare and synthesize effects across eligible SCD and group design studies (Hedges et al., 2012; 2013; Pustejovsky et al., 2014; Shadish et al., 2014; Swaminathan et al., 2014; Van den Noortgate &amp; Onghena, 2008). However, if researchers aim to synthesize findings from only SCD studies (i.e., not to integrate findings across single-case and group design studies), it may be feasible and preferable to use other options, such as synthesizing effects using case-specific effect sizes or multilevel modeling. 2.5 Case-Specific Effect Sizes In addition to averaging effects across studies, researchers may also be interested in exploring variation in treatment effects by categorical differences or individual participant characteristics (e.g., Do effects vary across settings? Are effects consistent across ethnic and racial groups?). When included studies use different outcome measures (e.g., included studies report outcomes measured on different scales such as a rate per session, occurrence/count, or ratio), it is important for researchers to use an effect size metric and synthesis approach that accounts for such. For example, Bowman-Perrott et al. (2016) examined five potential moderators (emotional and behavioral disorder risk status, reinforcement frequency, target behaviors, intervention format, and grade level) in their synthesis of 21 SCDs to obtain an overall effect of the Good Behavior Game in promoting positive behavior in the classroom. Results of their meta-analysis suggested that the intervention was more effective in reducing problem behaviors among students with or at risk for emotional and behavioral disorders. Another meta-analysis by Mason et al. (2016) first calculated and aggregated the effect sizes across all included studies, and then investigated the moderating effects of participant characteristics, targeted outcomes, and implementation components on the efficacy of video self-modeling, in which a learner with disabilities watches a video of a model engaged in targeted skills or tasks. They found that intervention effects were stronger for younger participants with autism spectrum disorders compared to those not identified as autistic or having an autism spectrum disorder. Because these syntheses focused on investigating variation across individuals in the effect of treatment, it was important that the effect size estimation and synthesis approach produced effect estimates for each individual participant (rather than a study-level summary effect estimate). Design-comparable effect size options are not viable for studying within-participant effects or variation in effects between individuals within the study because these effect sizes produce estimates at the study level (i.e., the effect averaged across the cases), not the individual level. Furthermore, outcome measures differed widely among studies included in the aforementioned reviews, so the researchers needed an individual-level effect size metric that was not scale-dependent (e.g., not based on simple raw score mean differences). When included studies use outcome measurements that cannot simply be re-scaled to be equivalent, case-specific effect size estimation and synthesis options may be best suited. However, the case-specific effect size options are not viable for meta-analysts wanting a single overall effect size after synthesizing both single-case design and group design studies because case-specific effects are not comparable to group design effects. 2.6 Multilevel Modeling of Individual Participant Interrupted Time-Series Data When a set of SCDs use the same or very similar outcome measures with the aim of studying the variation in effects over time within and between individuals, the multilevel modeling approach should be considered. For example, Datchuk et al. (2020) meta-analyzed 15 single-case studies totaling 79 students to examine the effects of an intervention on the level and trend in correct writing sequences per minute for students with disabilities. The researchers found that effect sizes were greater when students received intervention for longer durations (i.e., there was a positive effect on the trend) and that this temporal change in effect was more pronounced with younger students. In contexts like this, we suggest researchers avoid a meta-analytic approach that relies on a single effect size estimate for a study (e.g., design-comparable effect sizes) or even a single effect estimate for a case (e.g., case-specific effect sizes) and consider options for multilevel modeling of individual participant data series instead. 2.7 Summary of Options for Effect Estimation and Synthesis The flow chart in Figure 1.1 illustrates a set of heuristic decision rules for selecting among the three general approaches to synthesizing results from single-case research. If the primary purpose of one’s research is to integrate findings from both single-case and group-design studies, researchers should consider design-comparable effect sizes, contextually noting that effects are likely to be different for group design studies than from SCD studies (Chen &amp; Pustejovsky, 2022). Alternately, if the researchers plan to only include SCD studies, then they can use two other approaches (i.e., multilevel modeling and case-specific effect sizes). The choice between the latter two is related to the measurement of the dependent variable. Where there is interest in variability in effects across cases and over time, researchers should consider multilevel modeling of the raw data series, so long as the outcome is measured consistently across cases. However, if the aim is to examine how effects vary across the cases but the outcome measurements are non-equivalent and cannot be easily equated, then researchers should consider the options for estimating and synthesizing case-specific effect sizes. Figure 1.1: Approaches to Synthesizing Results from Single-Case Research 2.8 Limitations in Selecting an Approach for Effect Estimation and Synthesis We emphasize that Figure 1.1 presents a heuristic, simplified procedure for selecting among the three general approaches to effect size estimation and synthesis, which cannot and does not cover every possible research context. We anticipate and acknowledge that there will be situations where researchers’ aims and contexts differ from those we have described and thus do not align perfectly with one of our primary approaches to estimating and synthesizing single-case effect sizes. For example, researchers who are synthesizing findings from a set of SCDs may wish to compare their results to a previously published meta-analysis of group design studies, but not to investigate individual-level variation in treatment effects. They might therefore elect to use design-comparable effect sizes even though they are not formally integrating results from group design studies within their review. A further possibility is that researchers might elect to use multiple approaches to synthesis to address different aims or research questions. For example, consider a project in which researchers have identified both single-case and group design studies. They might want to integrate findings across design types while also exploring the variation in effects among individuals. In this scenario, researchers could estimate design-comparable effect sizes for their first aim and case-specific effect sizes from the subset of SCDs for their second aim. We also acknowledge that situations may arise that fall between those we described for case-specific effect sizes and those for multilevel modeling of the raw data series. For example, researchers may want to examine how effects vary over time and across cases, using studies with different outcomes. For this purpose, the researchers will need to identify and apply extensions of the primary approaches we present in this guide. For example, the researchers could either standardize the raw data before estimating a multilevel model, or they could synthesize case-specific effect sizes using multiple standardized effects for each case (e.g., an effect that indexes the immediate shift in level, and another effect that indexes a change in slope). In this guide, we aim to address what we perceive to be the most common scenarios, rather than conduct an exhaustive review of all possibilities. Finally, we anticipate that the heuristic guidance we provide here will need to be refined over time as further methodological innovations become available. We anticipate that research presently underway will provide even more meta-analytic options, with implications for how to select an approach for synthesis. At some point it may be possible to compute case-specific effect sizes that are also design-comparable, or it may be possible to standardize the data for multilevel models in a way that leads to parameter estimates from the model that correspond to design-comparable effect estimates. When such methods become available, some distinctions made here will become artificial. However, even as methodology continues to advance, researchers need guidance that acknowledges the complexity of research purposes and contexts and is dynamic in its accommodation of such variation, while also being concrete and straightforward enough for widespread implementation. To support the uptake of advanced methods for meta-analytic synthesis by educational researchers, the remainder of this guide follows the proposed heuristics for selecting among the three major approaches to effect size estimation and synthesis. 2.9 Structure of the Methods Guide We divide this guide into three major sections: (a) design-comparable effect size estimation and synthesis, (b) multilevel modeling, and (c) case-specific effect size estimation and synthesis to estimate and synthesize effects. Each section is independent; they are not in a particular chronological order, nor do they build upon each other. Rather, we expect those using this guide to follow the decision rules in Figure 1.1 to determine which approach is most appropriate for them, and then reference the corresponding relevant section. We divide each section into chapters. Each initial chapter introduces the specific approach to synthesizing results from SCD research, its assumptions, and determination for use. We then provide additional decision rules for selecting among the specific techniques and options available within a given broad approach. This is followed by an illustration where we: Describe the purposes for estimating and synthesizing effects, and the available data. Demonstrate how to use the decision rules in Figure 1.1, along with the additional decision rules specific to the initial section chapter, to arrive at the option being illustrated. Present the illustrated data to show how it needs to be structured for the analysis. Provide a step-by-step illustration of how to estimate and synthesize effects using readily available analysis tools. References Barton, E. E., Pustejovsky, J. E., Maggin, D. M., &amp; Reichow, B. (2017). Technology-Aided Instruction and Intervention for Students With ASD: A Meta-Analysis Using Novel Methods of Estimating Effect Sizes for Single-Case Research. Remedial and Special Education, 38(6), 371–386. https://doi.org/10.1177/0741932517729508 Beretvas, S. N., &amp; Chung, H. (2008). A review of meta-analyses of single-subject experimental designs: Methodological issues and practice. Evidence-Based Communication Assessment and Intervention, 2(3), 129–141. https://doi.org/10.1080/17489530802446302 Center, B. A., Skiba, R. J., &amp; Casey, A. (1985). A methodology for the quantitative synthesis of intra-subject design research. The Journal of Special Education, 19(4), 387. Cook, B., Buysse, V., Klingner, J., Landrum, T., McWilliam, R., Tankersley, M., &amp; Test, D. (2014). Council for exceptional children: Standards for evidence-based practices in special education. Teaching Exceptional Children, 46(6), 206. Gingerich, W. J. (1984). Meta-analysis of applied time-series data. The Journal of Applied Behavioral Science, 20(1), 71–79. https://doi.org/10.1177/002188638402000113 Hedges, L. V., &amp; Olkin, I. (1985). Statistical Methods for Meta-Analysis. Academic Press. Hedges, L. V., Pustejovsky, J. E., &amp; Shadish, W. R. (2012). A standardized mean difference effect size for single case designs. Research Synthesis Methods, 3, 224–239. https://doi.org/10.1002/jrsm.1052 Hedges, L. V., Pustejovsky, J. E., &amp; Shadish, W. R. (2013). A standardized mean difference effect size for multiple baseline designs across individuals. Research Synthesis Methods. https://doi.org/10.1002/jrsm.1086 Jamshidi, L., Heyvaert, M., Declercq, L., Fernández-Castilla, B., Ferron, J. M., Moeyaert, M., Beretvas, S. N., Onghena, P., &amp; Van den Noortgate, W. (2018). Methodological quality of meta-analyses of single-case experimental studies. Research in Developmental Disabilities, 79, 97–115. https://doi.org/10.1016/j.ridd.2017.12.016 Kratochwill, L., Thomas R., &amp; Swoboda, C. M. (2014). Visual analysis of single-case intervention research: Conceptual and methodological issues. In T. R. Kratochwill &amp; J. R. Levin (Eds.), Single-case intervention research: Methodological and statistical advances (pp. 91–125). American Psychological Association. https://doi.org/https://doi.org/10.1037/14376-004 Maggin, D. M., O’Keeffe, B. V., &amp; Johnson, A. H. (2011). A Quantitative Synthesis of Methodology in the Meta-Analysis of Single-Subject Research for Students with Disabilities: 1985. Exceptionality, 19(2), 109–135. https://doi.org/10.1080/09362835.2011.565725 Odom, S. L., Barton, E. E., Reichow, B., Swaminathan, H., &amp; Pustejovsky, J. E. (2018). Between-case standardized effect size analysis of single case designs: Examination of the two methods. Research in Developmental Disabilities, 79, 88–96. https://doi.org/10.1016/j.ridd.2018.05.009 Pustejovsky, J. E. (2018). Using response ratios for meta-analyzing single-case designs with behavioral outcomes. Journal of School Psychology, 68, 99–112. https://doi.org/10.1016/j.jsp.2018.02.003 Pustejovsky, J. E., Hedges, L. V., &amp; Shadish, W. R. (2014). Design-comparable effect sizes in multiple baseline designs: A general modeling framework. Journal of Educational and Behavioral Statistics, 39(5), 368–393. https://doi.org/10.3102/1076998614547577 Shadish, W. R., Hedges, L. V., Horner, R. H., &amp; Odom, S. L. (2015). The Role of Between-Case Effect Size in Conducting, Interpreting, and Summarizing Single-Case Research (NCER 2015-002; p. 109). National Center for Education Research, Institute of Education Sciences, U.S. Department of Education. Shadish, W. R., Hedges, L. V., &amp; Pustejovsky, J. E. (2014). Analysis and meta-analysis of single-case designs with a standardized mean difference statistic: A primer and applications. Journal of School Psychology, 52(2), 123–147. https://doi.org/10.1016/j.jsp.2013.11.005 Shadish, W. R., &amp; Lecy, J. D. (2015). The meta-analytic big bang. Research Synthesis Methods, 6(3), 246–264. https://doi.org/10.1002/jrsm.1132 Shadish, W. R., &amp; Rindskopf, D. M. (2007). Methods for evidence-based practice: Quantitative synthesis of single-subject designs. New Directions for Evaluation, 113(113), 95–109. https://doi.org/10.1002/ev.217 Swaminathan, H., Rogers, H. J., &amp; Horner, R. H. (2014). An effect size measure and bayesian analysis of single-case designs. Journal of School Psychology, 52(2), 213–230. https://doi.org/10.1016/j.jsp.2013.12.002 Van den Noortgate, W., &amp; Onghena, P. (2008). A multilevel meta-analysis of single-subject experimental design studies. Evidence-Based Communication Assessment and Intervention, 2(3), 142–151. https://doi.org/10.1080/17489530802505362 What Works Clearinghouse. (2020a). What Works Clearinghouse Procedures and Standards Handbook (Version 5.0). U.S. Department of Education, Institute of Education Sciences, National Center for Education Evaluation and Regional Assistance. White, O. R. (1987). Some comments concerning \"The quantitative synthesis of single-subject research\". Remedial and Special Education, 8(2), 34–39. https://doi.org/10.1177/074193258700800207 "],["D-CES.html", "Chapter 3 Introduction to Design-Comparable Effect Sizes 3.1 Background 3.2 When to Use Design-Comparable Effect Sizes 3.3 General Definition of Design-Comparable Effect Sizes 3.4 What We Assume When We Synthesize Design-Comparable Effect Sizes 3.5 Modeling Options for Design-comparable Effect Size Estimation", " Chapter 3 Introduction to Design-Comparable Effect Sizes This chapter provides background on design-comparable effect sizes, describes when to use them, and explains the key assumptions behind their use. We highlight both the assumptions for using design-comparable effect sizes in meta-analytic synthesis and the assumptions for estimation of these effect sizes from primary study data. We then describe available options for estimating design-comparable effect sizes. These options allow for different assumptions regarding trends in baseline or treatment phases and different assumptions about the variation in the treatment effect across cases. This chapter concludes with a set of decision rules for selecting among the options for design-comparable effect sizes. 3.1 Background Design-comparable effect sizes are effect size indices for single case studies that are on the same metric as effect size indices used in group comparison studies (Hedges et al., 2012, 2013; Pustejovsky et al., 2014; Shadish et al., 2014; Swaminathan et al., 2014; Van den Noortgate &amp; Onghena, 2008). These design-comparable effect sizes are valuable in a variety of synthesis contexts, particularly those that involve both single-case design (SCD) and group comparison studies. However, these methods also have several limitations. First, design-comparable effect sizes rely on an estimate of the variance between participants, requiring at least three distinct participants for estimation. This constrains their use to SCDs involving multiple participants (e.g., multiple baseline design across participants) or close replications of SCDs, each of which has a single participant (e.g., ABAB design replicated across several students). Further, existing research on design-comparable effect size methods is only available for multiple baseline, multiple probe, or replicated treatment reversal designs. Researchers have yet to develop extensions for alternating treatment designs and other types of SCDs. However, ongoing research is likely to increase the designs for which design-comparable effect sizes can be estimated. A second conceptual limitation is that design-comparable effect sizes produce a single summary effect size per outcome–which represents an average effect across participants–just as between-group design study effect sizes are summaries of average effects across participants. As a result, the design-comparable effect size might conceal heterogeneity of effects across participants in an SCD. Meta-analytic researchers are then limited to moderator analyses focused on study-level characteristics; it is not feasible to examine potential moderators that vary across cases in a study or across time points within a case. A third limitation is that design-comparable effect sizes are based on a hierarchical model that involves specific assumptions about the distribution of outcomes measured in the study. Developing a reasonable model requires care and attention to the plausibility of its assumptions. It is not a trivial or automatic process (as effect size calculations for between-group experimental designs are sometimes treated). Moreover, for some types of outcomes, the distributional assumptions of the model may be inappropriate, which further limits the applicability of the design-comparable effect size. To address these methodological limitations, we use this chapter to provide researchers with guidance on the selection and use of design-comparable effect size estimates. We describe six of the most common modeling options and provide guidance on how to select among these options when calculating design-comparable effect size for use in a research synthesis. 3.2 When to Use Design-Comparable Effect Sizes When choosing an effect size for single-case design data, researchers should begin by considering the broader purpose for computing effect sizes. In some cases, researchers may want to synthesize results across different types of studies, as in a comprehensive synthesis of all studies conducted on a topic. For example, researchers conducting a meta-analysis might include all studies examining the effects of social skills interventions on the social and academic outcomes of elementary-aged students with disabilities. In some areas of education research, it is likely that the literature identified for synthesis includes both group and single-case experimental studies. To average the effect across studies with different designs, researchers must pick an effect size index that has a comparable interpretation for each of the included designs (Hedges et al., 2012, 2013; Pustejovsky et al., 2014; Shadish et al., 2015). For exactly this purpose, methodologists developed the design-comparable effect size for SCDs, providing an effect size on a common metric by answering the question, “What would the standardized mean difference effect size be if one could somehow perform a between-group randomized experiment based on the same population of participants, intervention protocol, and outcome measures?” 3.3 General Definition of Design-Comparable Effect Sizes To understand the logic of the design-comparable effect size, it is helpful to consider how effect sizes are defined in group design studies. In a between-groups randomized experiment comparing a treatment condition (Tx) to a control condition (C) for a specified population, researchers commonly summarize results using the standardized mean difference effect size index. In Equation (3.1), we define this effect size parameter as \\[\\begin{equation} \\tag{3.1} \\delta = \\frac{\\mu_{Tx} - \\mu_C}{\\sigma_C}, \\end{equation}\\] where \\(\\mu_{Tx}\\) is the average outcome if the entire population received the treatment condition, \\(\\mu_C\\) is the average outcome if the entire population received the control condition, and \\(\\sigma_C\\) is the standard deviation of the outcome if the entire population received the control condition. The effect size may be estimated by substituting sample means and sample standard deviations in place of the corresponding population quantities (Borenstein, 2009), or by pooling sample standard deviations across the intervention and control conditions under the assumption that the population variance is equal. Alternately, the mean difference in the numerator of the effect size can be estimated based on a statistical model, such as an analysis of covariance that adjusts for between-group differences in baseline characteristics (Taylor et al., 2022). Researchers often apply the Hedges \\(g\\) small-sample correction, which reduces the bias of the effect size estimator that arises from estimating \\(\\sigma_C\\) based on a limited number of observations (Hedges, 1981). Using data from a multiple baseline, multiple probe, or replicated treatment reversal design, the design-comparable effect size for SCDs aims to estimate the same quantity as the standardized mean difference from a between-groups experiment. This task poses challenges because the data from such SCDs involve repeated measurements taken over time. To precisely define the design-comparable effect size, researchers must therefore be specific about the timing of both intervention implementation and outcome assessment. Hypothetically, if a between-groups experiment uses the same study procedures as the SCD, researchers would still need to determine and specify when to begin intervention and when to collect outcome data. Furthering this example, suppose that the SCD takes place over times \\(t=1,...,T\\). In our hypothetical between-groups experiment, intervention starts at time \\(A\\) for \\(1 \\leq A &lt; T\\) and collection of outcome data for all participants occurs at time \\(B\\) for \\(A &lt; B\\). The standardized mean difference from such an experiment would contrast the average outcome at time \\(B\\) if the entire population had started intervention at time \\(A\\) [i.e., \\(\\mu_B(A)\\)] to the average outcome at time \\(B\\) if the entire population had remained in baseline through time \\(B\\) and then started intervention later at time \\(T\\) [i.e., \\(\\mu_B(T)\\)]. The standardized mean difference would then correspond to \\[\\begin{equation} \\tag{3.2} \\delta_{AB} = \\frac{\\mu_B(A) - \\mu_B(T)}{\\sigma_B(T)}, \\end{equation}\\] where \\(\\mu_B(A)\\) is the average outcome at follow-up time \\(B\\) if the entire population were to receive the intervention at time \\(A\\). Then, \\(\\mu_B(T)\\) is the average outcome at follow-up time \\(B\\) if the entire population were to receive the intervention at time \\(T\\). Finally, \\(\\sigma_B(T)\\) is the standard deviation of the outcome at follow-up time \\(B\\) if the entire population were to receive the intervention at time \\(T\\). Note that \\(\\mu_B(T)\\) corresponds to the average outcome under the control condition (\\(\\mu_C\\), above), because participants would not yet have received intervention as of time \\(B\\). Similarly \\(\\sigma_B(T)\\) is the analogue of \\(\\sigma_C\\), the standard deviation of the outcome under the control condition because participants would not yet have received the intervention as of time \\(B\\). Pustejovsky et al. (2014) described a strategy for estimating \\(\\delta_{AB}\\) using data from an SCD study. Broadly, the strategy involves specifying a multilevel model for the data, estimating the component quantities \\(\\mu_B(A)\\), \\(\\mu_B(T)\\), and \\(\\sigma_B(T)\\) based on the specified model, and applying a small-sample correction analogous to Hedges \\(g\\). However, because of the need to estimate the standard deviation of the outcome across the participant population [\\(\\sigma_B(T)\\)], this strategy only works if the SCD study includes data from multiple participants. The approach involves a multilevel model for the data because SCDs involve repeated measurements collected for each of several participants. The first level of the model describes the pattern of repeated measurements over time nested within a given participant and the second level of the model describes how the first-level parameters vary across participants. As a result, the model involves deconstructing \\(\\sigma_B(T)\\) into two components: within-participant variation and between-participant variation. This process is not typically possible in a between-groups randomized experiment unless researchers collect repeated outcome measures for each participant. 3.4 What We Assume When We Synthesize Design-Comparable Effect Sizes The motivation for using the design-comparable effect size δ from SCD studies is strongest when researchers intend to synthesize SCD and group design studies in the same meta-analysis. If assumptions needed for synthesis are not reasonably met, it may be more appropriate to analyze the SCD and group design studies separately. Then, researchers may want to consider alternative effect size metrics for the SCD studies. For this reason, this chapter presents the broader synthesis assumptions prior to the specific assumptions needed for estimating design-comparable effect sizes. The random effects model is the predominant statistical model for synthesizing effect sizes across studies. With this statistical model, we do not assume that the population effect size estimated in one study is identical to the population effect size estimated from another study. Rather, we assume that the effect size estimated for Study \\(j\\) may differ from the effect size estimated for Study \\(k\\) (e.g., \\(\\sigma_j \\ne \\sigma_k\\)). There are several general frameworks for explaining why the effect size may vary from one study to the next. One such framework posits that differences can arise from variation across studies in the units, treatments, observing operations, and settings [UTOS; Becker (1996)]. The inclusion and exclusion criteria for the meta-analysis can be set up to constrain (but not completely eliminate) the variation from study to study on these dimensions. Use of a random effects model for the summary meta-analysis and the exploration of moderators of the treatment effects is warranted because some degree of variation in effects is anticipated. There are several different ways to understand the assumptions underlying the random effects model. One way is to imagine that the included studies in a synthesis represent a random sample from a super-population of possible studies on the topic of interest. In a Bayesian framework, the model can also be motivated by the assumption of exchangeability, meaning that the effect size of studies included in a synthesis are on a common metric that permits judgements of similarity and that their relative magnitude cannot be systematically predicted a priori (Higgins et al., 2009). For brevity, we refer to both suppositions (i.e., the super-population and Bayesian motivations) as the exchangeability assumption. Crucially, the exchangeability assumption depends on the effect size metric used for synthesis; for a given set of studies, the assumption may be reasonable for one effect size metric but unreasonable for another. Thus far, we have defined the standardized mean difference metric (\\(\\delta\\)) for design-comparable effect sizes for SCDs. Therefore, we now consider the exchangeability of \\(\\delta s\\). When intending to synthesize a set of studies where there is considerable variation among the study outcomes, sampled units, or treatments (i.e., they differ greatly from one another on one or more of these UTOS characteristics), then the \\(\\delta s\\) from these studies are likely not exchangeable. In contrast, when there is similarity in the UTOS, exchangeability is more reasonable. As an example, for the standardized mean difference metric, exchangeability is more plausible when one study’s population of participants closely mirrors the participant population characteristics from another study (i.e., similar, if not same, inclusion criteria). Further, when the populations are similar and studies use the exact same operational measure of the dependent variable, we can assume that the distribution of outcomes in the control condition has similar variance. Alternatively, if the two studies drew from populations with very different characteristics so that the disbursement of the study results (distribution of the dependent variable) varied widely, an intervention that produces identical effects on the scale of the dependent variable would have quite different effect sizes on the scale of the standardized mean difference. When populations are distinctly different, the exchangeability assumption is less tenable. Thus, we encourage researchers to examine the studies they plan to include in their synthesis for the potential lack of exchangeability. Researchers can explore this aspect of the exchangeability assumption by examining the sampling methods and measurement procedures of the included studies. When subsets of studies use the same operational measure of the dependent variable, the between-participant (case) variance in those studies can be compared. To illustrate the exploration of between-case variability, consider the sampling procedures used in the following two studies extracted from Datchuk et al. (2020) that examined the effect of interventions on writing performance as measured by correct word sequences (CWS). In one study, the sample consisted of three 7-year-old White males identified by their teachers as struggling with writing (Parker et al., 2012). In the other study (Stotz et al., 2008), the sample comprised three 10-year-old students who exhibited poor writing skills. The first student was a Black male identified with an emotional disturbance, the second student was a White male identified with a specific learning disability, and the third student was a White female identified with a specific learning disability. Presented with this information, we then seek to answer the following questions: (a) Are these samples similar enough to satisfy the exchangeability assumption with the standardized mean difference metric? (b) Might the second sample with variation in differences in participant characteristics (age, race, gender, and educational disability category), be so much more variable in writing performance that it is not reasonable to use the standardized mean difference metric to judge similarity of effects across both studies? In the first study (Parker et al., 2012), the mean number of CWS during baseline for the three participants were 8.29, 15.0, and 10.8. In the second study (Stotz et al., 2008), participants’ mean CWS baseline levels were 14.6, 29.1, and 22.1. In Study 1, the between-case variation, as indexed by the standard deviation (SD) of the three baseline means, is 3.4, whereas the between-case SD is 7.3 in Study 2. Now we must consider whether this difference is large enough to distort the design-comparable effect size. To address our questions, we first consider the raw score effect size for each study by specifying a multilevel model that assumes no trends in baseline or treatment phases, and variation in the effect across cases. In Study 1, the shift in the expected number of CWS when moving from baseline to intervention, or raw score effect size, is 10.7. In Study 2, the CWS raw score effect size is 9.6, about 1.1 times smaller than the raw score effect size for Study 1. Next, we consider the design-comparable effect size computed using the same model used for the raw score effect size. The design-comparable effect size is 0.962 for Study 1 and 0.827 for Study 2; the effect size for Study 1 is approximately 1.2 times greater than Study 2, like the ratio of the observed raw score effect sizes. We anticipate that future research will provide guidance regarding how much difference in sampling (and resulting between-case variability) researchers can accommodate without creating notable problems for the synthesis of design-comparable effect sizes. Similarly, researchers will continue to investigate, and hopefully establish consensus about, the degree to which differences among outcomes and treatments between studies are tolerable to have confidence in their effect size results. Until then, we suggest that meta-analysts be aware of the underlying exchangeability assumption (that the effect sizes expressed on a given metric are exchangeable across studies) and be forthright and transparent about their findings when reporting results of their synthesis. If the differences in the units, treatments, observing operations, and settings between the SCD studies and the group studies are much larger than the differences among either the SCD or group studies, it may be preferable to meta-analyze the SCD and group studies separately. Conversely, if the differences are negligible from one set of studies to the next, the exchangeability assumption is more tenable, and attention can be turned to the assumptions necessary to estimate the design-comparable effect size. 3.4.1 What We Assume When We Estimate Design-Comparable Effect Size Design-comparable effect sizes for SCD studies rely on multilevel models. Estimation is based on several key assumptions, including distributional assumptions about the errors. The models assume that observations for each case are normally distributed around the case-specific trend line, and the variation of observations around the trend line is homogeneous from one phase to the next and from one case to the next. In addition, there are assumptions about the underlying structural model, including whether there are trends in phases. Below, we explain these assumptions in greater detail and provide exemplars of each. 3.4.2 Normality Use of design-comparable effect sizes assumes that the experimental observations are normally distributed around the case-specific trend lines. The data may be consistent with this assumption of normality, but not always. For example, Ota &amp; DuPaul (2002) utilized a multiple baseline design across three participants to examine the effects of a math software intervention with a game component on the off-task behaviors of students with attention-deficit hyperactivity disorder (ADHD). To examine the normality assumption, we extracted the study data using Webplot Digitizer (Rohatgi, 2015) and present it in Figure 3.1. When examining the baseline phases, observations appear to be distributed somewhat normally around the baseline means, with a pooled skewness value near zero (\\(sk = 0.15\\)) and a pooled kurtosis value near zero (\\(ku = -0.77\\)). However, we observe non-normality in the treatment phases because the outcome is a percentage that has remained near the floor of 0% for much of the treatment phase (\\(sk = 1.86\\); \\(ku = 4.99\\)). Figure 3.1: Multiple Baseline Design Across Three Participants (Ota &amp; DuPaul, 2002) With count-based variables, we often anticipate some departures from the traditional normality assumption. These departures tend to be more pronounced when a count variable has a phase mean close to zero. Departures from the traditional normality assumption are more pronounced when a percentage variable has a phase mean close to either end of the \\(0\\%-100\\%\\) range. These situations are common in published SCDs because the quality of an SCD study is often judged on the stability and level of observations in baseline and treatment phases. For example, if researchers design an intervention to increase a non-reversible behavior (e.g., academic skill), it would be ideal to only recruit participants without the target skill in their repertoire so that observations in the baseline phase reflect such-they would have baseline observations at or near 0. Conversely, an SCD recommendation for behavior-reduction interventions is to seek low rates or non-occurrence of the target behavior in the treatment phase, communicating that the intervention effectively led to the amelioration or extinction of a problem behavior. However, distributions may more closely align with the normality assumption when treatment phase counts are higher than 0, or the percentage variable has a mean closer to \\(50\\%\\). Available evidence indicates that design-comparable effect sizes can tolerate a moderate degree of non-normality for relatively simple model specifications (Chen et al., 2023). However, additional research is needed to determine how much non-normality can be present before there are substantial consequences for the design-comparable effect size. 3.4.3 Homogeneity of Variance In addition to assuming normality, the illustrations of design-comparable effect size estimation provided in this guide all assume that the within-case variation is homogeneous across phases and cases within a study. Although assumptions of homogeneity across cases and phases may be reasonable, there are situations when researchers should not assume homogeneity of variance. Consider again the Ota &amp; DuPaul (2002) study, with the multiple baseline design graphs in Figure 3.1. Results of our visual analysis suggest that the variance differs between the baseline and treatment phases, with less variation in the treatment phase as the percentage of off-task behaviors decreased and approached 0 (i.e., extinction). With count-based variables (e.g., raw counts or counts converted to percentages), variability often depends on the variable mean. Thus, treatments that shift the mean tend to change the variance. If studies have unstructured baselines, substantial variability is common. If there is tight experimental control in the intervention phase (e.g., researchers predict and control for interventionist/peer attention, strong treatment fidelity), we might expect some reduction in variance. With studies like these, where the variance differs between the baseline and treatment phases, we recommend estimating a more complex multilevel model that yields separate variance estimates for the two phases rather than assuming homogeneity across phases. Such models are feasible to estimate using the tools presented in subsequent chapters, but are beyond the scope of the guide. Until future research provides more concrete guidance about the best ways to proceed when encountering between-case heterogeneity, meta-analysts must remain aware of their assumptions and transparent about analytic decisions when reporting methods and results. Upon discovering substantial violations to the normality or homogeneity assumptions, we encourage researchers to consider whether violations to the exchangeability assumption needed for synthesis are also present. For example, imagine that meta-analysts interested in synthesizing the effects of oral narrative interventions select a multiple baseline design study that has all cases reporting baseline observations consistently at or near 0. It is likely that both the normality and homogeneity assumptions are violated for this study. It is also likely that the outcome used in the hypothetical study differs greatly from the outcomes used in the included group design studies. This leads to questions about the exchangeability of the effect sizes. In such circumstances, we advise against trying to force the computation of a design-comparable effect size and the synthesis of SCD and group design studies together. A more appropriate synthesis option may be to meta-analyze the SCD and group design studies separately, allowing for the use of a more appropriate effect size metric for the SCD studies separate from that used for the group design studies. However, we expect that normality and homogeneity of variance assumption violations will often be more modest (or non-existent) than in the above example, so that researchers can continue to entertain the use of the design-comparable effect size. 3.4.4 Appropriate Structural Model The estimation of design-comparable effect sizes requires assumptions about the structural model for the data series collected in the SCD. Ideally, these assumptions are based on content expertise, knowledge of the intervention domain, and understanding of the dependent variable(s) under review. The assumptions also rely on visual analysis and calculation of descriptive statistics from the studies. Regarding baseline trend, given our knowledge and understanding of the behavior(s), context(s), and participants included in the studies, we can assume one of three things: no trend in baseline, a linear trend in baseline, or some form of nonlinear trend. Similarly, we can use the same knowledge to make assumptions about data trends in the treatment phase: no trend, linear trend, or nonlinear trend. Furthermore, we can make assumptions about the parameters defining the baseline trajectory (e.g., level and slope) and the change in trajectory with treatment (e.g., the change in level and change in slope)–they either differ across cases within the study, or we can assume that some of these parameters are the same across cases. Purposeful consideration of our included SCDs is likely to lead to more accurate effect size estimation. If we do not select a structural model consistent with our data, we can expect biased design-comparable effect size estimates. Figure 3.2: Multiple Baseline Across Participants (Rodgers et al., 2020) As an example, we present the study of a writing intervention for post-secondary adults with intellectual and developmental disabilities that targeted the improvement of sentence construction (Rodgers et al., 2021). Figure 3.2 is a graphical depiction of the study design and outcome data. Visual analysis of the baseline phases suggests potential baseline trends in accurate writing sequences. Therefore, it would be appropriate to specify a model that assumes a trend in baseline. For participant Denny, our visual analysis of observed correct writing sequences suggests an increasing baseline trend, represented by the solid line in Figure 3.3 (estimated using ordinary least squares regression). In contrast, if we selected a baseline model assuming no trend, Denny’s projected baseline of no trend is considerably different than the projected baseline assuming a linear trend. (In Figure 3.3, note the difference between the dotted lines representing the baseline trends projected through the treatment phase). The effect estimate would be larger if we did not model the baseline trend in Figure 3.3 because the observed treatment values are further above the projection based on no trend than the projection based on trend. Thus, assumptions about trends have consequences for the effect size estimates. Figure 3.3: Hypothesized and Projected Baselines for Denny (Rodgers et al., 2020) To select an appropriate structural model, we recommend that meta-analysts start by carefully considering the discipline or intervention under study for the research synthesis, including but not limited to the outcome of interest, the participants included in the studies, and what prior research says about what can be expected regarding trends in absence of intervention and data trends in the treatment phase. For example, a review of research on the use of positive behavioral interventions and supports (PBIS) over time show some efficacy in the reduction of both in-school and out-of-school student suspensions for Black students and students with disabilities (see Gage et al., 2020 for a review). Using this as a basis for understanding the nature of the interventions, contexts, and populations, we can reasonably assume future PBIS research would report similar responses to the interventions if the research contexts are similar. However, rather than make such a broad assumption without verifying it, we recommend that meta-analysts visually analyze all data for studies included in the synthesis and consider the degree to which the data from the studies are reasonably consistent with prior expectations. If data from the studies are consistent with trend expectations (e.g., a learning curve associated with the development of a new skill), these assumptions can be used to select among the design-comparable effect size modeling options. Misalignment between a priori assumptions and actual observations across the studies’ included data is likely to compromise the degree of confidence that we place in estimating effect sizes. If substantial inconsistencies exist between researchers’ expectations for and actual observations of study data, results from any single modeling option become more suspect. In these situations, we encourage researchers to estimate the effects for each of the competing sets of trend assumptions to provide information on the sensitivity of the findings to the modeling decisions. 3.5 Modeling Options for Design-comparable Effect Size Estimation After meta-analysts consider the tenability of the exchangeability assumption, we suggest consideration of normality and homogeneity of variance assumptions. If violations to these distributional assumptions are severe, researchers should reconsider whether the SCD and group design study outcomes are similar enough to assume exchangeability and warrant synthesis using the standardized mean difference effect size metric. If outcomes are substantially different across design types, it may be more reasonable to meta-analyze the SCD and group design studies separately and to use a different effect size metric for the SCD studies. When outcomes appear more similar and violations are not too severe, we suggest meta-analysts proceed with the design-comparable effect sizes and note findings. In their decision to proceed with design-comparable effect sizes, researchers should describe the range of characteristics of the SCD as well as the predominant SCD used in the area of synthesis. For example, do most SCD studies in the research area tend to use reversal designs (e.g., ABAB designs) or do studies predominantly use designs of multiple baseline and/or multiple probe across participants? When synthesizing reversal designs with design-comparable effect sizes, researchers are currently limited to models that assume stability (i.e., no trend). For multiple baseline or multiple probe designs, a variety of trend assumptions are feasible such as linear or quadratic. When the synthesis includes predominantly multiple baseline and/or multiple probe designs, researchers should state their expectations about trends given their understanding of the participants, context, and outcome under study. We also recommend they visually inspect the graphs of the data from the primary studies, analyzing them for consistency with the trend expectations. Based on these considerations, it is helpful to determine which of the following sets of trend assumptions are most reasonable for the set of studies to be synthesized: (a) no trends in baseline or treatment phases, (b) no trends in baseline, but trends in treatment phases, or (c) trends in baseline and differential trends in treatment. After clarifying the trend assumptions, researchers next need to clarify assumptions about variability in treatment effect across cases (e.g., Is the treatment effect expected to be the same for each case or are between-participant differences anticipated in the response to intervention?). Again, we rely heavily on logic models for the area of research and visual analyses of primary study data to determine if these data are reasonably consistent with the expectations. Figure 3.4 arranges these considerations into a series of decision rules that researchers can use to select from one of six common models for design-comparable effect sizes. Although there are other possibilities for specifying a design-comparable effect size estimation model, these six models cover a wide range of scenarios that can be estimated with the scdhlm software application (Pustejovsky et al., 2021). In addition, we included models that have received the most attention in the methodological literature, as well as those that have been applied in meta-analyses of single-case data. Figure 3.4: Flow Chart for the Selection of Design-comparable Effect Sizes References Becker, B. J. (1996). The generalizability of empirical research results. In Intellectual talent: Psychometric and social issues (pp. 362–383). Johns Hopkins University Press. Borenstein, M. (2009). Effect sizes for continuous data. In H. M. Cooper, L. V. Hedges, &amp; J. C. Valentine (Eds.), The Handbook of Research Synthesis and Meta-Analysis (pp. 221–235). Russell Sage Foundation. Chen, L.-T., Chen, Y.-K., Yang, T.-R., Chiang, Y.-S., Hsieh, C.-Y., Cheng, C., Ding, Q.-W., Wu, P.-J., &amp; Peng, C.-Y. J. (2023). Examining the normality assumption of a design-comparable effect size in single-case designs. Behavior Research Methods. https://doi.org/10.3758/s13428-022-02035-8 Datchuk, S. M., Wagner, K., &amp; Hier, B. O. (2020). Level and Trend of Writing Sequences: A Review and Meta-Analysis of Writing Interventions for Students With Disabilities. Exceptional Children, 86(2), 174–192. https://doi.org/10.1177/0014402919873311 Gage, N. A., Beahm, L., Kaplan, R., MacSuga-Gage, A. S., &amp; Lee, A. (2020). Using positive behavioral interventions and supports to reduce school suspensions. Beyond Behavior, 29(3), 132–140. https://doi.org/10.1177/1074295620950611 Hedges, L. V. (1981). Distribution theory for Glass’s estimator of effect size and related estimators. Journal of Educational Statistics, 6(2), 107–128. Hedges, L. V., Pustejovsky, J. E., &amp; Shadish, W. R. (2012). A standardized mean difference effect size for single case designs. Research Synthesis Methods, 3, 224–239. https://doi.org/10.1002/jrsm.1052 Hedges, L. V., Pustejovsky, J. E., &amp; Shadish, W. R. (2013). A standardized mean difference effect size for multiple baseline designs across individuals. Research Synthesis Methods. https://doi.org/10.1002/jrsm.1086 Higgins, J. P. T., Thompson, S. G., &amp; Spiegelhalter, D. J. (2009). A re-evaluation of random-effects meta-analysis. Journal of the Royal Statistical Society: Series A (Statistics in Society), 172(1), 137–159. https://doi.org/10.1111/j.1467-985X.2008.00552.x Ota, K. R., &amp; DuPaul, G. J. (2002). Task engagement and mathematics performance in children with attention-deficit hyperactivity disorder: Effects of supplemental computer instruction. School Psychology Quarterly, 17(3), 242–257. https://doi.org/10.1521/scpq.17.3.242.20881 Parker, D. C., Dickey, B. N., Burns, M. K., &amp; McMaster, K. L. (2012). An Application of Brief Experimental Analysis with Early Writing. Journal of Behavioral Education, 21(4), 329–349. https://doi.org/10.1007/s10864-012-9151-3 Pustejovsky, J. E., Chen, M., &amp; Hamilton, B. (2021). Scdhlm: A web-based calculator for between-case standardized mean differences. Pustejovsky, J. E., Hedges, L. V., &amp; Shadish, W. R. (2014). Design-comparable effect sizes in multiple baseline designs: A general modeling framework. Journal of Educational and Behavioral Statistics, 39(5), 368–393. https://doi.org/10.3102/1076998614547577 Rodgers, D. B., Datchuk, S. M., &amp; Rila, A. L. (2021). Effects of a Text-Writing Fluency Intervention for Postsecondary Students with Intellectual and Developmental Disabilities. Exceptionality, 29(4), 310–325. https://doi.org/10.1080/09362835.2020.1850451 Rohatgi, A. (2015). Webplotdigitizer. Zenodo. https://doi.org/10.5281/zenodo.32375 Shadish, W. R., Hedges, L. V., Horner, R. H., &amp; Odom, S. L. (2015). The Role of Between-Case Effect Size in Conducting, Interpreting, and Summarizing Single-Case Research (NCER 2015-002; p. 109). National Center for Education Research, Institute of Education Sciences, U.S. Department of Education. Shadish, W. R., Hedges, L. V., &amp; Pustejovsky, J. E. (2014). Analysis and meta-analysis of single-case designs with a standardized mean difference statistic: A primer and applications. Journal of School Psychology, 52(2), 123–147. https://doi.org/10.1016/j.jsp.2013.11.005 Stotz, K. E., Itoi, M., Konrad, M., &amp; Alber-Morgan, S. R. (2008). Effects of Self-graphing on Written Expression of Fourth Grade Students with High-Incidence Disabilities. Journal of Behavioral Education, 17(2), 172–186. https://doi.org/10.1007/s10864-007-9055-9 Swaminathan, H., Rogers, H. J., &amp; Horner, R. H. (2014). An effect size measure and bayesian analysis of single-case designs. Journal of School Psychology, 52(2), 213–230. https://doi.org/10.1016/j.jsp.2013.12.002 Taylor, J. A., Pigott, T., &amp; Williams, R. (2022). Promoting Knowledge Accumulation About Intervention Effects: Exploring Strategies for Standardizing Statistical Approaches and Effect Size Reporting. Educational Researcher, 51(1), 72–80. https://doi.org/10.3102/0013189X211051319 Van den Noortgate, W., &amp; Onghena, P. (2008). A multilevel meta-analysis of single-subject experimental design studies. Evidence-Based Communication Assessment and Intervention, 2(3), 142–151. https://doi.org/10.1080/17489530802505362 "],["illustrate-D-CES.html", "Chapter 4 Illustration of Design-Comparable Effect Sizes When No Trends Are Assumed 4.1 Selecting a Design-Comparable Effect Size for the Single-Case Studies 4.2 Details of the No Trend Models for Design-Comparable Effect Sizes 4.3 Estimating the Design-Comparable Effect Size for the Single-Case Studies 4.4 Estimating the Design-Comparable Effect Size for the Group Studies 4.5 Analyzing the Effect Sizes", " Chapter 4 Illustration of Design-Comparable Effect Sizes When No Trends Are Assumed This chapter illustrates the computation of design-comparable effect sizes in contexts where we assume no trend in either baseline or treatment phases. We provide step-by-step instructions to demonstrate the selection and estimation of design-comparable effect sizes, using two multiple baseline studies, a replicated ABAB study, and a group design study. We use the scdhlm app to show how to estimate the design-comparable effect sizes for the single-case studies and discuss estimating the effect size for the group study. Then, we illustrate how to combine the effects using a fixed effect meta-analysis. In this chapter, we illustrate the computation of design-comparable effect sizes based on models that do not include time trends (i.e., Models 1 and 2 in Figure 3.4). These models assume that the dependent variable has a stable level throughout the baseline phase for each case and that introduction of treatment leads to an immediate shift in the level of the dependent variable. In this chapter, we pretend we are researchers who want to synthesize evidence from several single-case design (SCD) and group design studies that examine intervention effects on the improvement of math problem solving for students with disabilities. To model the process succinctly, we limit our illustration to four included studies comprised of two multiple baseline designs, a replicated ABAB design, and one group design. In one of the SCDs, Peltier et al. (2020) used a multiple baseline design across three small groups to examine the effect of a mathematics intervention for 12 students struggling with mathematical word problems. In another SCD study, Case et al. (1992) used a multiple baseline design across four students with math word problem solving difficulties to examine the impact of a mathematics intervention. The third SCD study by Lambert et al. (2006) used an ABAB design replicated across nine participants with disruptive behaviors from two classrooms to examine intervention effects on math problem solving. The fourth included study by Hutchinson (1993) used a group design where students with learning disabilities were randomly assigned to intervention and control conditions and students’ math word problem solving accuracy was measured pre- and post-intervention. Because our goal is to aggregate effects across single-case and group design studies, we consider design-comparable effect sizes (see the decision rules in Figure 1.1). We break down this illustration into four stages: (1) selecting a design-comparable effect size for the SCD studies, (2) estimating the design-comparable effect size for the SCD studies using the scdhlm app (Pustejovsky et al., 2021), which is a web-based calculator for design-comparable effect sizes, (3) estimating the effect size for the group study, and (4) synthesizing the effect sizes across the included studies. We concentrate primarily on the first two steps, because well-developed methods for group studies are illustrated in detail elsewhere (e.g., Borenstein et al. (2021); H. Cooper et al. (2019); Hedges (1981); Hedges (2007)). 4.1 Selecting a Design-Comparable Effect Size for the Single-Case Studies We first select an appropriate design-comparable effect size using the decision rules in Figure 1.1. To formulate hypotheses about potential trends in baseline and/or treatment phases (i.e., if and why we expect those trends), we rely on previous experience and existing knowledge of the population, study contexts, and outcome of interest. Based on such, we assume that students in the studies would not improve mathematics skills without intervention. We also expect the interventions to abruptly change the outcomes, so we anticipate an immediate shift in the level of performance from baseline to treatment and no trend in the treatment phase. Finally, because we assume no trends in either baseline or treatment phases, we tentatively consider Model 1 or Model 2 from Figure 1.1. These models are the only options for the ABAB design and are appropriate for multiple baseline designs when not anticipating trends. Ideally, we make the choice between Model 1 and Model 2 based on our conceptual understanding and our a priori expectation that either the treatment effect would not vary from case to case (i.e., Model 1) or would vary from one case to the next (Model 2). Due to the participants’ differential learning histories and existing skills or abilities, we anticipate some variation in the effect across cases. We tentatively select Model 2 as the best fit, as it is most consistent with our understanding of this context. To verify the tenability of our assumptions, we conduct a visual analysis of the SCD study graphs. We present the data extracted from each SCD study, with data from Case et al. (1992) shown in Figure 4.1, Peltier et al. (2020) data in Figure 4.2, and data extracted from Lambert et al. (2006) in Figure 4.3. Prior to examining the data to evaluate its consistency with the Model 2 trend assumptions, we consider whether the data are reasonably aligned with the homogeneity and normality assumptions underlying all design-comparable effect size models. Note that in Figures 4.1 and 4.2, we generally see similar variation from one case to the next, and similar within-case variation across phases. We also fail to see any outliers that would call into question the normality assumption. However, in Figure 4.3, there is a tendency for the treatment phases to have more variability than the baseline phases. Further, in Figure 4.3, all observations for Student A4 have values near zero in the first baseline phase—inconsistent with our homogeneity and normality assumptions. Although it appears that some of the design-comparable effect size assumptions were violated, we proceed because the violations appear relatively minor and our combining effect sizes across single-case and group studies requires that we compute a design-comparable effect size for each study (or drop the study from the synthesis). Next, given the data observed in the primary studies, we consider if a model absent of baseline trends appears reasonable. Across the cases in Figures 4.1-4.3, the typical baseline pattern is one of no trend and is consistent with our expectations. However, there are a few exceptions. In Figure 4.1, Ben’s data appear to have an upward baseline trend with observed values of 2, 2, and 3. This visual trend is ambiguous; it could be an artifact of Ben just happening to solve one more math problem on Day 3 (i.e., chance). Yet, based on our previously stated expectations and an absence of baseline improvement for the other participants, we find it more reasonable to assume that Ben would continue to solve two or three problems per session (i.e., no trend) as opposed to continuing to improve by about one problem per day (i.e., linear trend). The former assumption is also consistent with the decision made by Case et al. (1992) to intervene with Ben. In other words, had the researchers thought Ben’s math problem solving was improving in baseline, there would be less reason for Ben to receive the intervention. We also consider other case examples. In Figure 4.2, Gary’s data appear to have an upward baseline trend and Kyrie’s data appear to have a downward trend in baseline. These trends are inconsistent with each other, and with the typical pattern seen across the Peltier et al. (2020) study cases. As with Ben (see Figure 4.1), we are not confident that the apparent trend would continue. If Gary’s trend were to continue, there would be no need for intervention. If Kyrie’s trend were to continue, we would predict negative percentages correct in the upcoming sessions, which is not possible. Thus, we proceed with the assumption of no baseline trend because the typical pattern for most cases in each study is consistent with our expectations of no trend, and because of the ambiguity surrounding the few possible exceptions. Next, focusing on the treatment phases, we conduct similar analyses. In Figures 4.1 and 4.3, where there is an immediate shift in performance and no trend in the treatment phases, we find patterns that match our hypothesized treatment trend expectations. In Figure 4.2, we again see a typical pattern where the shift in performance is immediate and stable over time. While here there are a few exceptions where cases appear to have a decline in performance toward the end of the intervention phase (e.g., Andy and Ellie), we find it best to select a model consistent with the typical and expected pattern. We proceed with a model that assumes no trend in baseline and treatment phases (i.e., Model 1 or Model 2 from Figure 3.4) because the design-comparable effect sizes assume a common model across cases and estimate an average effect across cases. Figure 4.1: Multiple Baseline Data Extracted from Case et al. (1992) Figure 4.2: Multiple Baseline Data Extracted from Peltier et al. (2020) Figure 4.3: Replicated ABAB Data Extracted from Lambert et al. (2006) 4.2 Details of the No Trend Models for Design-Comparable Effect Sizes To fully differentiate between Model 1 and Model 2, we present the formal specification of each. For both Model 1 and Model 2, we can write the within-case model as: \\[\\begin{equation} \\tag{4.1} Y_{ij} = \\beta_{0j} + \\beta_{1j}Tx_{ij} + e_{ij}, \\end{equation}\\] where \\(Y_ij\\) is the score on the outcome variable \\(Y\\) at measurement occasion \\(i\\) for case \\(j\\), and \\(Tx_{ij}\\) is dummy coded with a value of 0 for baseline observations and a value of 1 for the treatment phase observations. The mean baseline level for case \\(j\\) is \\(\\beta_{0j}\\) (see Figure 4.4 for a visual representation of \\(\\beta_{0j}\\) and \\(\\beta_{1j}\\)). The raw score treatment effect for case \\(j\\) is indexed by \\(\\beta_{1j}\\), which is the difference between the treatment phase outcome mean and the baseline phase mean. The error (\\(e_{ij}\\)) is time- and case-specific and assumed normally distributed and first-order autoregressive with variance \\(\\sigma_e^2\\). Figure 4.4: Illustration of Treatment Effect for Asher (Peltier et al., 2020) For Model 1, the between-case model is: \\[\\begin{equation} \\tag{4.2} \\beta_{0j} = \\gamma_{00} + u_{0j} \\end{equation}\\] \\[\\begin{equation} \\tag{4.3} \\beta_{1j} = \\gamma_{10} \\end{equation}\\] where \\(\\gamma_{00}\\) is the across-case average baseline mean and \\(u_{0j}\\) is a case-specific error, which is the deviation from the overall average for case \\(j\\). We assume the error is normally distributed with variance \\(\\sigma_{u_0}^2\\). We assume the across-case average raw score treatment effect, \\(\\gamma_{10}\\), to be constant for all cases. Thus, there is no error term in the equation for \\(\\beta_{1j}\\). Based on this model, the design-comparable effect size is defined as the average raw score treatment effect (\\(\\gamma_{10}\\)) divided by a SD that is comparable to the SD used to standardize mean differences in group-design studies (Pustejovsky et al., 2014): \\[\\begin{equation} \\tag{4.4} \\delta = \\frac{\\gamma_{10}}{\\sqrt{\\sigma_{u_0}^2 + \\sigma_e^2}} \\end{equation}\\] The Model 2 specification is like Model 1, with the only difference being an error term (\\(u_{1j}\\)) added to Equation (4.3) to account for between-case variation in the treatment effect. More specifically: \\[\\begin{equation} \\tag{4.5} Y_{ij} = \\beta_{0j} + \\beta_{1j}Tx_{ij} + e_{ij}, \\end{equation}\\] \\[\\begin{equation} \\tag{4.6} \\beta_{0j} = \\gamma_{00} + u_{0j} \\end{equation}\\] \\[\\begin{equation} \\tag{4.7} \\beta_{1j} = \\gamma_{10} + u_{1j} \\end{equation}\\] Again, the across-case average baseline mean is \\(\\gamma_{00}\\) and the across-case average raw score treatment effect (i.e., difference in treatment and baseline phase means) is \\(\\gamma_{10}\\). The case-specific errors (\\(u_{0j}\\) and \\(u_{1j}\\)) account for between-case differences in baseline level and response to treatment. They are assumed multivariate normal with covariance \\(\\Sigma_u = \\begin{bmatrix} \\sigma_{u_0}^2 &amp; \\\\ \\sigma_{u_1u_0} &amp; \\sigma_{u_1}^2 \\\\ \\end{bmatrix}\\). We define the design-comparable effect size exactly as in Equation (4.4), because the effect size is scaled by the SD of the outcome in the absence of intervention and not dependent on the addition of \\(u_{1j}\\), which only impacts between-case variance in the treatment phase. Because we tentatively selected Model 2 based on our a priori considerations, we use it to illustrate the estimation of design-comparable effect sizes for this data set. For several reasons, we also contrast the Model 2 results to what we obtain from Model 1. First, contrasting Model 1 and 2 results provides us with an additional way to examine the empirical support for our model selection. For instance, we could report that visual analyses of the model-implied individual trajectories for Model 2 fit the raw data better than the model implied individual trajectories from Model 1. Second, the contrast allows us to examine the sensitivity of our effect size estimates to the model chosen. For instance, this contrast may lead us to rule out between-case effect size variation as having a large impact on the estimated design-comparable effect size. Finally, contrasting allows us to illustrate a method of selecting between models in circumstances where a priori information is not sufficient. 4.3 Estimating the Design-Comparable Effect Size for the Single-Case Studies 4.3.1 Example 1: Multiple Baseline Study by Case et al. (1992) We can estimate design-comparable effect sizes for all models suggested in Figure 4.4, using a web-based calculator for design-comparable effect sizes (scdhlm; Pustejovsky et al. (2021)). The scdhlm app is available at https://jepusto.shinyapps.io/scdhlm/. To use this app, researchers must store their dataset in an Excel file (.xlsx), comma delimited file (.csv), or text file (.txt). In addition, we recommend that users inspect their data to ensure the inclusion of the following variables: case identifier, phase identifier, session number, and the outcome. Although not required, researchers may want to arrange the data columns by order of variable appearance in the app. We show this arrangement for the Case et al. (1992) study in Figure (fig:Excel-Case-1992). There, we demonstrate the following order of column headers with case identifier representing data in the first column, followed by variables in this order (left-to-right): phase identifier, session number, and the outcomes in the fourth column. Researchers have the flexibility to use any labeling scheme that clearly distinguishes between baseline and intervention conditions. For example, for the phase identifier, one can use \\(b\\) or 0 to indicate baseline observations and \\(i\\) or 1 to indicate intervention observations. However, the app requires that numerical values be used for both session number and outcome. Finally, we recommend that users arrange the data first by case (i.e., enter all the rows of data for the first case before any of the rows of data for the second case) and then by session number. Figure 4.5: Illustration of Treatment Effect for Asher (Peltier et al., 2020) After starting the app, we use the Load tab to load the data file, as illustrated in Figure (fig:Load-Case-1992). As mentioned previously, the data file can be a .txt or .csv file that includes one dataset, or an Excel (.xlsx) file that has either one (e.g., a data set for one study) or multiple spreadsheets (one spreadsheet for each of several studies). If using a .xlsx file with multiple spreadsheets, the scdhlm app allows us to select the spreadsheet containing the data for the study of interest from the Load tab. Then, we use the drop-down menus on the right of the screen to indicate the study design (treatment reversal versus Multiple Baseline/Multiple Probe across participants) and which variables in the data set correspond to the case identifier, phase identifier, session, and outcome (see Figure (fig:Load-Case-1992)). Figure 4.6: Between-Case Standardized Mean Difference Estimator (scdhlm, v. 0.6.0) Load Tab After loading our data, we use the Inspect tab to ensure the accurate import of raw data into the app and assigned variable names are accurate (Figure (fig:Inspect-Case-1992)). In addition, we can use the Inspect tab to view a graph of the data (Figure (fig:Graph-Case-1992)). We recommend that researchers compare these data with the graphed data from the original studies to ensure accuracy in uploading the data and specifying the design and variable names on the Load tab. Check the graphed data again for consistency with the (tentatively) selected model for the design-comparable effect size. Figure 4.7: Between-Case Standardized Mean Difference Estimator (scdhlm, v. 0.6.0) Data Tab within Inspect Tab for Case et al. (1992) Figure 4.8: Between-Case Standardized Mean Difference Estimator (scdhlm, v. 0.6.0) Graph Tab Within Inspect Tab for Case et al. (1992) After inspecting the data, we next specify the model for the design-comparable effect size using the Model tab. Figure 4.9 shows our specification for Model 2 (i.e., the model that assumes no trends and an effect that varies across cases). The specification process begins with the selection of trend type for the baseline phase. For this example, under Type of time trend, we select level because we assume no time trends in the baseline phases. Then, we opt to include level as a fixed effect and check the box to enable the model estimation of the average baseline level (i.e., \\(\\gamma_{00}\\) from Equation (4.2). We also include level as a random effect, so that the baseline level can vary from case to case. Focusing on the treatment phase next, we select change in level as the Type of time trend, because we assume that the treatment will only change the level of the outcome (not trend) and include change in level as a fixed effect. As a fixed effect, we can obtain an estimate of the average shift in level between baseline and treatment phases (i.e., \\(\\gamma_{10}\\) from Equation (4.3). Finally, we choose to include change in level as a random effect to allow the change in level (i.e., treatment effect) to vary from case to case. Note the scdhlm app allows us to make different potential assumptions about the correlation structure of the session-level errors. Shown in Figure 4.9 are the default options of auto-regautoregressive and constant variance across phases. These defaults match the model presented in Equation (4.1). At this point, our model for the design-comparable effect size matches Model 2 from Figure 3.4. At the bottom of the screen, the scdhlm app provides a graph of the data with trend lines based on the specified model. We recommend that users inspect this graph to ensure that the trend lines fit the data reasonably well. If the trend lines do not fit the data well, a question about model selection arises. For example, we find the baseline trend line for Abernathy is relatively high compared to their actual baseline observations. However, other model specifications (e.g., Model 1 shown later) did not improve the fit of Abernathy’s baseline trend line. So, we decide to proceed with estimation because most other trend lines look appropriate, and the model is consistent with our a priori expectations. Throughout this process, researchers should maintain a high standard for transparency in decision-making when reporting methods and results. Figure 4.9: Between-Case Standardized Mean Difference Estimator (scdhlm, v. 0.6.0) Model Tab Showing Model 2 Specification for Case et al. (1992) For the Case et al. (1992) data set, the a priori-identified Model 2 provides trajectories that fit the data reasonably well1. Thus, we proceed to the Effect size tab. As shown in Figure 4.10, the estimated design-comparable effect size for this study is 2.57 with a standard error (SE) of 0.45 and \\(95\\%\\) confidence interval (CI) [1.65, 3.50]. Additional information reported on the Effect size tab include estimates of other quantities from the model, information about the model specification, and the assumed time-points used in calculating the design-comparable effect size. The reported degrees of freedom are used in making a small-sample correction to the effect size estimate, analogous to the Hedges’ g correction used with group designs (Hedges, 1981). Larger estimated degrees of freedom imply more precision in estimating the denominator of the design-comparable effect size, making the small-sample correction less consequential. Conversely, smaller degrees of freedom are indicative of imprecise design-comparable effect size denominator estimation, making the small-sample correction more consequential. The Effect size tab, shown in 4.10, also reports autocorrelation, which is the estimate of the correlation between level-1 errors of the model for the same case, differing by one time point (i.e., session) based on a first-order autoregressive model. Given the selected follow-up time, the reported intra-class correlation is an estimate of the between-case variance of the outcome as a proportion of the total variation in the outcome (including both between-case and within-case variance). Larger values of the intra-class correlation indicate that more of the variation in the outcome is between participants. The remaining output information (Study design, Estimation method, Baseline specification, Treatment specification, Initial treatment time, Follow-up time) describe the model specification and assumptions used in the effect size calculations. The scdhlm app includes such to allow for reproducibility of the calculations. Figure 4.10: Between-Case Standardized Mean Difference Estimator (scdhlm, v. 0.6.0) Effect size Tab Showing Model 2 Estimate for Case et al. (1992) If our a priori arguments for selecting Model 2 were weak, or we wanted to consider Model 1, we can easily compute the results of a different model by going back to the Model tab and changing our specification. The only difference is that Model 1 does not include change in level as a random effect for the treatment phase. For illustration purposes, we present Model 1 specification results for this study in Figure 4.11. As before, we keep the level option selected as the Type of time trend for the baseline phase, as both a fixed effect and a random effect. In the treatment phase, we keep change in level as the Type of time trend but select only change in level as a fixed effect. The Model 1 trend lines fit similarly to those from Model 2; the Model 1 design-comparable effect size is 2.59 with an SE of 0.41 and \\(95\\%\\) CI [1.76, 3.42]. This suggests that the effect size estimate is not sensitive to our decision about whether the treatment effect varies across cases. Despite this information, we proceed with the Model 2 estimate because it is consistent with our expectations for the research in this area. Figure 4.11: Between-Case Standardized Mean Difference Estimator (scdhlm, v. 0.6.0) Model Tab Showing Model 1 Specification for Case et al. (1992) 4.3.2 Example 2: Multiple Baseline Study by Peltier et al. (2020) We now have a design-comparable effect size for the first single-case study by Case et al. (1992). Next, we repeat these steps for all other SCD studies in our synthesis. For this illustration, we include a second multiple baseline study (Peltier et al., 2020). For this second study, we ran through the same sequence of steps: 1. load the data; 2. inspect the data in both tabular and graphic form; 3. specify our selected model for the data (i.e., Model 2 for this illustration); and 4. estimate the design-comparable effect size. After performing these steps, we found that the estimated model trajectories for the Model 2 specification fit the Peltier et al. (2020) data well, as shown in Figure 4.12. For the Peltier et al. (2020) study, the design-comparable effect size is 2.95 with an SE of 0.28 and \\(95\\%\\) CI [2.42, 3.53]. We then estimated the effect for Model 1 (i.e., we remove the check next to change in level under include random effect), which is the more restrictive model that does not allow the treatment effect to vary across cases. Model 1 produced a design-comparable effect size estimate of 2.83 with an SE of 0.24 and \\(95\\%\\) CI [2.34, 3.31], like the Model 2 design-comparable effect size. Figure 4.12: Between-Case Standardized Mean Difference Estimator (scdhlm, v. 0.6.0) Model 2 Specification for Peltier et al. (2020) 4.3.3 Example 3: Replicated ABAB Design by Lambert et al. (2006) For the third SCD study, we illustrate use of the scdhlm app using data from a replicated ABAB design by Lambert et al. (2006). As with the previous two SCD studies, we load our spreadsheet containing the study data in the usual manner. However, unlike Case et al. (1992) and Peltier et al. (2020), the Lambert et al. (2006) study does not utilize a multiple baseline design. Therefore, on the right side of the Load tab, using the drop-down menu under Please specify the study design, we must select Treatment Reversal (as opposed to Multiple baseline/Multiple probe across participants). After loading the data, we again use the Inspect tab to visually inspect the data in both tabular and graphic form. Then using the Model tab (shown in Figure 4.13), we continue to specify Model 2 by checking the option under the Baseline phase to include level as both a fixed effect and a random effect and checking under the Treatment phase to include change in level as both a fixed effect and a random effect. Users of the app will note that for reversal designs, there is no drop-down menu from which they can potentially add trends (i.e., level is the only option for baseline specification, and change in level is the only option for treatment phase specification). This makes specification using the Model tab simpler than in our previous examples of multiple baseline studies, although it does also constrain the user’s ability to specify a well-fitting model. On the Model tab, we see that the fitted trajectories fit these data well. After specifying the model, we view the Effect size tab to obtain the design-comparable effect size (see Figure 4.13). For Lambert et al. (2006), the estimated Model 2 design-comparable effect size is 6.37 with an SE of 0.39 and \\(95\\%\\) CI [5.60, 7.14]. Like the previous SCD studies, we also estimate the effect for Model 1, which is the more restrictive model that does not allow the treatment effect to vary across cases. Model 1 produces an estimate of 6.34 with an SE of 0.37 and \\(95\\%\\) CI [5.61, 7.08], a similar result to the Model 2 design-comparable effect size. Figure 4.13: Between-Case Standardized Mean Difference Estimator (scdhlm, v. 0.6.0) Model 2 Specification for Lambert et al. (2006) 4.4 Estimating the Design-Comparable Effect Size for the Group Studies After estimating the design-comparable effect size for each SCD study, we turn our attention to estimating the design-comparable effect size for each group study. Hutchinson (1993) used random assignment of individual students with learning disabilities to either the intervention (\\(n = 12\\)) or control (\\(n = 8\\)) conditions. After intervention, both groups of students completed 25 math word problems selected from the British Columbia Mathematics Achievement Test. Because details for estimating standardized mean difference effect sizes from group studies are readily available from a variety of sources including chapters (Borenstein, 2019), books (e.g., Borenstein et al., 2021) and journal articles (Hedges, 1981; e.g., Hedges, 2007), we do not model the calculation procedures and simply report the results. Using the Hutchinson (1993) group design study summary statistics at posttest, we calculate a standardized mean difference in math word problem solving as 0.71, with an SE of 0.45 based on Hedges’ g to correct for small-sample bias. 4.5 Analyzing the Effect Sizes After we have obtained effect sizes from each single-case and group study, we can proceed with synthesizing the effect sizes. Depending on our synthesis goals, we have a variety of tools and approaches available. We can: (a) create graphical displays (e.g., forest plots) to show the effect size for each study along with their confidence interval, (b) average the effect sizes and create a confidence interval for the overall average effect, (c) estimate the extent of variation in effects across studies, (d) examine the effect sizes for evidence of publication bias, and (e) explore potential moderators of the effect. Since the use of design-comparable effect sizes for the SCD studies produce estimates having the same metric as the commonly used standardized mean difference effect sizes from group studies, researchers can accomplish these goals (e.g., averaging the effect sizes or running a moderator analysis) using methods developed for group studies. Details on these methods are readily available elsewhere (e.g., Borenstein et al., 2021; H. Cooper et al., 2019). We illustrate the averaging of the effect sizes from our studies here using a fixed effect meta-analysis, so that this illustration is consistent with the approach used in What Works Clearinghouse intervention reports (What Works Clearinghouse, 2020b). Table 4.1 reports the effect size estimates, SEs, and fixed effect meta-analysis calculations for our four included studies. In fixed effect meta-analysis, the overall average effect size estimate is a weighted average of the effect size estimates from the individual studies, with weights proportional to the inverse of the sampling variance (squared SE) of each effect size estimate. Further, the SE of the overall effect size is the square root of the inverse of the total weight. In Table 4.1, column C reports the inverse variance weight assigned to each study, with the percentage of the total weight listed in parentheses. For instance, the effect size estimate from Peltier et al. (2020) receives 43.7% of the total weight, while the effect size estimates from Case et al. (1992) and Hutchinson (1993) each receive 16.9% of the total weight. The total inverse variance weight is 29.21. The overall average effect size estimate is 3.28 with an SE of 0.18 and an approximate \\(95\\%\\) CI [2.91, 3.64]. The Q-test for heterogeneity is highly significant, Q(3) = 99.3, p &lt; .0001, indicating that the included effect size estimates are more variable than we would expect due to sampling error alone. In other words, it is unlikely that we would observe such a wide dispersion of effect size estimates if the studies were all estimating the same true effect size parameter. Table 4.1: Fixed Effect Meta-Analysis Calculations for Example Math Invervention Studies Study Effect Size Estimate (A) Standard Error (B) Inverse-variance Weight (%) (C) Case et al. (1992) 2.57 0.45 4.94 (16.9) Peltier et al. (2020) 2.95 0.28 12.76 (43.7) Lambert et al. (2006) 6.37 0.39 6.57 (22.5) Hutchinson (1993) 0.71 0.45 4.94 (16.9) Fixed effect meta-analysis 3.28 0.19 29.21 (100) In fixed effect meta-analysis, the overall average effect size estimate is a summary of the effect size estimates across the included studies, where studies are treated as fixed. Therefore, the SE and CI in fixed effect meta-analysis account for the uncertainty in the process of effect size estimation that occurs in each of the individual studies. However, they do not account for uncertainty in the process of identifying studies for inclusion in the meta-analysis (Konstantopoulos &amp; Hedges, 2019; Rice et al., 2018), nor do they provide a basis for generalization beyond the included studies. When conducting syntheses of larger bodies of literature–and especially of studies with heterogeneous populations, design features, or dependent effect sizes–researchers will often prefer to use random effects models (Hedges &amp; Vevea, 1998) or their further extensions (Pustejovsky &amp; Tipton, 2021; Van den Noortgate et al., 2013). Of the studies we use as illustrative examples in this chapter, the dependent variable of the group study (Hutchinson, 1993) was the broadest. Conversely, the Case et al. (1992) and Peltier et al. (2020) MB studies used more narrowly defined dependent variables. Finally, we note the outcome from the replicated ABAB design (Lambert et al., 2006) included a behavioral component (i.e., it is not purely academic). In a synthesis of many studies, researchers might use moderator analysis (i.e., meta-regression analysis) to explore the extent to which variation in effect size is related to dependent variable characteristics or other study features. Methods for conducting such moderator analysis are described elsewhere (Borenstein et al., 2021, Chapters 19–21; Konstantopoulos &amp; Hedges, 2019). References Borenstein, M. (2019). Effect sizes for continuous data. In H. M. Cooper, L. V. Hedges, &amp; J. C. Valentine (Eds.), The Handbook of Research Synthesis and Meta-Analysis. Russell Sage Foundation. Borenstein, M., Hedges, L. V., Higgins, J. P. T., &amp; Rothstein, H. R. (2021). Introduction to Meta-Analysis. John Wiley &amp; Sons, Ltd. Case, L. P., Harris, K. R., &amp; Graham, S. (1992). Improving the Mathematical Problem-Solving Skills of Students with Learning Disabilities: Self-Regulated Strategy Development. The Journal of Special Education, 26(1), 1–19. https://doi.org/10.1177/002246699202600101 Cooper, H., Hedges, L. V., &amp; Valentine, J. C. (2019). The handbook of research synthesis and meta-analysis. Russell Sage Foundation. Hedges, L. V. (1981). Distribution theory for Glass’s estimator of effect size and related estimators. Journal of Educational Statistics, 6(2), 107–128. Hedges, L. V. (2007). Effect sizes in cluster-randomized designs. Journal of Educational and Behavioral Statistics, 32(4), 341–370. https://doi.org/10.3102/1076998606298043 Hedges, L. V., &amp; Vevea, J. L. (1998). Fixed- and random-effects models in meta-analysis. Psychological Methods, 3(4), 486–504. https://doi.org/10.1037/1082-989X.3.4.486 Hutchinson, N. L. (1993). Effects of Cognitive Strategy Instruction on Algebra Problem Solving of Adolescents with Learning Disabilities. Learning Disability Quarterly, 16(1), 34–63. https://doi.org/10.2307/1511158 Konstantopoulos, S., &amp; Hedges, L. V. (2019). Statistically analyzing effect sizes: Fixed-and random-effects models. In H. Cooper Harris &amp; J. C. Valentine (Eds.), The handbook of research synthesis and meta-analysis (3rd Edition, pp. 246–280). Russell Sage Foundation. Lambert, M. C., Cartledge, G., Heward, W. L., &amp; Lo, Y. (2006). Effects of response cards on disruptive behavior and academic responding during math lessons by fourth-grade urban students. Journal of Positive Behavior Interventions, 8(2), 88. Peltier, C., Sinclair, T. E., Pulos, J. M., &amp; Suk, A. (2020). Effects of Schema-Based Instruction on Immediate, Generalized, and Combined Structured Word Problems. The Journal of Special Education, 54(2), 101–112. https://doi.org/10.1177/0022466919883397 Pustejovsky, J. E., Chen, M., &amp; Hamilton, B. (2021). Scdhlm: A web-based calculator for between-case standardized mean differences. Pustejovsky, J. E., Hedges, L. V., &amp; Shadish, W. R. (2014). Design-comparable effect sizes in multiple baseline designs: A general modeling framework. Journal of Educational and Behavioral Statistics, 39(5), 368–393. https://doi.org/10.3102/1076998614547577 Pustejovsky, J. E., &amp; Tipton, E. (2021). Meta-analysis with robust variance estimation: Expanding the range of working models. Prevention Science. https://doi.org/10.1007/s11121-021-01246-3 Rice, K., Higgins, J. P. T., &amp; Lumley, T. (2018). A re-evaluation of fixed effect(s) meta-analysis. Journal of the Royal Statistical Society Series A: Statistics in Society, 181(1), 205–227. https://doi.org/10.1111/rssa.12275 Van den Noortgate, W., López-López, J. A., Marı́n-Martı́nez, F., &amp; Sánchez-Meca, J. (2013). Three-level meta-analysis of dependent effect sizes. Behavior Research Methods, 45, 576–594. https://doi.org/10.3758/s13428-012-0261-6 What Works Clearinghouse. (2020b). What Works Clearinghouse Standards Handbook (Version 4.1). U.S. Department of Education, Institute of Education Sciences, National Center for Education Evaluation and Regional Assistance. Meta-analysis must specify their criteria for “reasonably well”.↩︎ "],["references.html", "References", " References Barton, E. E., Pustejovsky, J. E., Maggin, D. M., &amp; Reichow, B. (2017). Technology-Aided Instruction and Intervention for Students With ASD: A Meta-Analysis Using Novel Methods of Estimating Effect Sizes for Single-Case Research. Remedial and Special Education, 38(6), 371–386. https://doi.org/10.1177/0741932517729508 Becker, B. J. (1996). The generalizability of empirical research results. In Intellectual talent: Psychometric and social issues (pp. 362–383). Johns Hopkins University Press. Beretvas, S. N., &amp; Chung, H. (2008). A review of meta-analyses of single-subject experimental designs: Methodological issues and practice. Evidence-Based Communication Assessment and Intervention, 2(3), 129–141. https://doi.org/10.1080/17489530802446302 Borenstein, M. (2009). Effect sizes for continuous data. In H. M. Cooper, L. V. Hedges, &amp; J. C. Valentine (Eds.), The Handbook of Research Synthesis and Meta-Analysis (pp. 221–235). Russell Sage Foundation. Borenstein, M. (2019). Effect sizes for continuous data. In H. M. Cooper, L. V. Hedges, &amp; J. C. Valentine (Eds.), The Handbook of Research Synthesis and Meta-Analysis. Russell Sage Foundation. Borenstein, M., Hedges, L. V., Higgins, J. P. T., &amp; Rothstein, H. R. (2021). Introduction to Meta-Analysis. John Wiley &amp; Sons, Ltd. Bowman-Perrott, L., Burke, M. D., Zaini, S., Zhang, N., &amp; Vannest, K. (2016). Promoting Positive Behavior Using the Good Behavior Game: A Meta-Analysis of Single-Case Research. Journal of Positive Behavior Interventions, 18(3), 180–190. https://doi.org/10.1177/1098300715592355 Case, L. P., Harris, K. R., &amp; Graham, S. (1992). Improving the Mathematical Problem-Solving Skills of Students with Learning Disabilities: Self-Regulated Strategy Development. The Journal of Special Education, 26(1), 1–19. https://doi.org/10.1177/002246699202600101 Center, B. A., Skiba, R. J., &amp; Casey, A. (1985). A methodology for the quantitative synthesis of intra-subject design research. The Journal of Special Education, 19(4), 387. Chen, L.-T., Chen, Y.-K., Yang, T.-R., Chiang, Y.-S., Hsieh, C.-Y., Cheng, C., Ding, Q.-W., Wu, P.-J., &amp; Peng, C.-Y. J. (2023). Examining the normality assumption of a design-comparable effect size in single-case designs. Behavior Research Methods. https://doi.org/10.3758/s13428-022-02035-8 Cook, B., Buysse, V., Klingner, J., Landrum, T., McWilliam, R., Tankersley, M., &amp; Test, D. (2014). Council for exceptional children: Standards for evidence-based practices in special education. Teaching Exceptional Children, 46(6), 206. Cooper, H. M. (2010). Research Synthesis and Meta-Analysis (4th ed.). SAGE Publications. Cooper, H., Hedges, L. V., &amp; Valentine, J. C. (2019). The handbook of research synthesis and meta-analysis. Russell Sage Foundation. Council for Exceptional Children: Standards for Evidence-Based Practices in Special Education. (2014). TEACHING Exceptional Children, 46(6), 206–212. https://doi.org/10.1177/0040059914531389 Datchuk, S. M., Wagner, K., &amp; Hier, B. O. (2020). Level and Trend of Writing Sequences: A Review and Meta-Analysis of Writing Interventions for Students With Disabilities. Exceptional Children, 86(2), 174–192. https://doi.org/10.1177/0014402919873311 Gage, N. A., Beahm, L., Kaplan, R., MacSuga-Gage, A. S., &amp; Lee, A. (2020). Using positive behavioral interventions and supports to reduce school suspensions. Beyond Behavior, 29(3), 132–140. https://doi.org/10.1177/1074295620950611 Gingerich, W. J. (1984). Meta-analysis of applied time-series data. The Journal of Applied Behavioral Science, 20(1), 71–79. https://doi.org/10.1177/002188638402000113 Hedges, L. V. (1981). Distribution theory for Glass’s estimator of effect size and related estimators. Journal of Educational Statistics, 6(2), 107–128. Hedges, L. V. (2007). Effect sizes in cluster-randomized designs. Journal of Educational and Behavioral Statistics, 32(4), 341–370. https://doi.org/10.3102/1076998606298043 Hedges, L. V., &amp; Olkin, I. (1985). Statistical Methods for Meta-Analysis. Academic Press. Hedges, L. V., Pustejovsky, J. E., &amp; Shadish, W. R. (2012). A standardized mean difference effect size for single case designs. Research Synthesis Methods, 3, 224–239. https://doi.org/10.1002/jrsm.1052 Hedges, L. V., Pustejovsky, J. E., &amp; Shadish, W. R. (2013). A standardized mean difference effect size for multiple baseline designs across individuals. Research Synthesis Methods. https://doi.org/10.1002/jrsm.1086 Hedges, L. V., &amp; Vevea, J. L. (1998). Fixed- and random-effects models in meta-analysis. Psychological Methods, 3(4), 486–504. https://doi.org/10.1037/1082-989X.3.4.486 Higgins, J. P. T., Thompson, S. G., &amp; Spiegelhalter, D. J. (2009). A re-evaluation of random-effects meta-analysis. Journal of the Royal Statistical Society: Series A (Statistics in Society), 172(1), 137–159. https://doi.org/10.1111/j.1467-985X.2008.00552.x Hutchinson, N. L. (1993). Effects of Cognitive Strategy Instruction on Algebra Problem Solving of Adolescents with Learning Disabilities. Learning Disability Quarterly, 16(1), 34–63. https://doi.org/10.2307/1511158 Jamshidi, L., Heyvaert, M., Declercq, L., Fernández-Castilla, B., Ferron, J. M., Moeyaert, M., Beretvas, S. N., Onghena, P., &amp; Van den Noortgate, W. (2018). Methodological quality of meta-analyses of single-case experimental studies. Research in Developmental Disabilities, 79, 97–115. https://doi.org/10.1016/j.ridd.2017.12.016 Konstantopoulos, S., &amp; Hedges, L. V. (2019). Statistically analyzing effect sizes: Fixed-and random-effects models. In H. Cooper Harris &amp; J. C. Valentine (Eds.), The handbook of research synthesis and meta-analysis (3rd Edition, pp. 246–280). Russell Sage Foundation. Kratochwill, L., Thomas R., &amp; Swoboda, C. M. (2014). Visual analysis of single-case intervention research: Conceptual and methodological issues. In T. R. Kratochwill &amp; J. R. Levin (Eds.), Single-case intervention research: Methodological and statistical advances (pp. 91–125). American Psychological Association. https://doi.org/https://doi.org/10.1037/14376-004 Kratochwill, T. R., Hitchcock, J. H., Horner, R. H., Levin, J. R., Odom, S. L., Rindskopf, D. M., &amp; Shadish, W. R. (2013). Single-Case Intervention Research Design Standards. Remedial and Special Education, 34(1), 26–38. https://doi.org/10.1177/0741932512452794 Kratochwill, T. R., Hitchcock, J., Horner, R. H., Levin, J. R., Odom, S. L., Rindskopf, D. M., &amp; Shadish, W. R. (2010). Single-Case Designs Technical Documentation, Version 1.0 (Pilot) (June; Vol. 0). What Works Clearinghouse. Kratochwill, T. R., Horner, R. H., Levin, J. R., Machalicek, W., Ferron, J., &amp; Johnson, A. (2021). Single-case design standards: An update and proposed upgrades. Journal of School Psychology, 89, 91–105. https://doi.org/10.1016/j.jsp.2021.10.006 Lambert, M. C., Cartledge, G., Heward, W. L., &amp; Lo, Y. (2006). Effects of response cards on disruptive behavior and academic responding during math lessons by fourth-grade urban students. Journal of Positive Behavior Interventions, 8(2), 88. Maggin, D. M., Barton, E., Reichow, B., Lane, K., &amp; Shogren, K. A. (2021). Commentary on the What Works Clearinghouse Standards and Procedures Handbook (v. 4.1) for the Review of Single-Case Research. Remedial and Special Education, 074193252110513. https://doi.org/10.1177/07419325211051317 Maggin, D. M., O’Keeffe, B. V., &amp; Johnson, A. H. (2011). A Quantitative Synthesis of Methodology in the Meta-Analysis of Single-Subject Research for Students with Disabilities: 1985. Exceptionality, 19(2), 109–135. https://doi.org/10.1080/09362835.2011.565725 Maggin, D. M., Pustejovsky, J. E., &amp; Johnson, A. H. A. H. (2017). A meta-analysis of school-based group contingency interventions for students with challenging behavior: An update. Remedial and Special Education, 38(6), 353–370. https://doi.org/10.1177/0741932517716900 Mason, R. A., Davis, H. S., Ayres, K. M., Davis, J. L., &amp; Mason, B. A. (2016). Video Self-Modeling for Individuals with Disabilities: A Best-Evidence, Single Case Meta-Analysis. Journal of Developmental and Physical Disabilities, 28(4), 623–642. https://doi.org/10.1007/s10882-016-9484-2 Odom, S. L., Barton, E. E., Reichow, B., Swaminathan, H., &amp; Pustejovsky, J. E. (2018). Between-case standardized effect size analysis of single case designs: Examination of the two methods. Research in Developmental Disabilities, 79, 88–96. https://doi.org/10.1016/j.ridd.2018.05.009 Ota, K. R., &amp; DuPaul, G. J. (2002). Task engagement and mathematics performance in children with attention-deficit hyperactivity disorder: Effects of supplemental computer instruction. School Psychology Quarterly, 17(3), 242–257. https://doi.org/10.1521/scpq.17.3.242.20881 Parker, D. C., Dickey, B. N., Burns, M. K., &amp; McMaster, K. L. (2012). An Application of Brief Experimental Analysis with Early Writing. Journal of Behavioral Education, 21(4), 329–349. https://doi.org/10.1007/s10864-012-9151-3 Peltier, C., Sinclair, T. E., Pulos, J. M., &amp; Suk, A. (2020). Effects of Schema-Based Instruction on Immediate, Generalized, and Combined Structured Word Problems. The Journal of Special Education, 54(2), 101–112. https://doi.org/10.1177/0022466919883397 Pustejovsky, J. E. (2018). Using response ratios for meta-analyzing single-case designs with behavioral outcomes. Journal of School Psychology, 68, 99–112. https://doi.org/10.1016/j.jsp.2018.02.003 Pustejovsky, J. E., Chen, M., &amp; Hamilton, B. (2021). Scdhlm: A web-based calculator for between-case standardized mean differences. Pustejovsky, J. E., &amp; Ferron, J. (2017). Research Synthesis and Meta-Analysis of Single-Case Designs. In Handbook of Special Education (2nd Edition, p. 63). Routledge. Pustejovsky, J. E., Hedges, L. V., &amp; Shadish, W. R. (2014). Design-comparable effect sizes in multiple baseline designs: A general modeling framework. Journal of Educational and Behavioral Statistics, 39(5), 368–393. https://doi.org/10.3102/1076998614547577 Pustejovsky, J. E., &amp; Tipton, E. (2021). Meta-analysis with robust variance estimation: Expanding the range of working models. Prevention Science. https://doi.org/10.1007/s11121-021-01246-3 Reichow, B., Barton, E. E., &amp; Maggin, D. M. (2018). Development and applications of the single-case design risk of bias tool for evaluating single-case design research study reports. Research in Developmental Disabilities, 79, 53–64. https://doi.org/10.1016/j.ridd.2018.05.008 Rice, K., Higgins, J. P. T., &amp; Lumley, T. (2018). A re-evaluation of fixed effect(s) meta-analysis. Journal of the Royal Statistical Society Series A: Statistics in Society, 181(1), 205–227. https://doi.org/10.1111/rssa.12275 Rodgers, D. B., Datchuk, S. M., &amp; Rila, A. L. (2021). Effects of a Text-Writing Fluency Intervention for Postsecondary Students with Intellectual and Developmental Disabilities. Exceptionality, 29(4), 310–325. https://doi.org/10.1080/09362835.2020.1850451 Rohatgi, A. (2015). Webplotdigitizer. Zenodo. https://doi.org/10.5281/zenodo.32375 Shadish, W. R., Hedges, L. V., Horner, R. H., &amp; Odom, S. L. (2015). The Role of Between-Case Effect Size in Conducting, Interpreting, and Summarizing Single-Case Research (NCER 2015-002; p. 109). National Center for Education Research, Institute of Education Sciences, U.S. Department of Education. Shadish, W. R., Hedges, L. V., &amp; Pustejovsky, J. E. (2014). Analysis and meta-analysis of single-case designs with a standardized mean difference statistic: A primer and applications. Journal of School Psychology, 52(2), 123–147. https://doi.org/10.1016/j.jsp.2013.11.005 Shadish, W. R., &amp; Lecy, J. D. (2015). The meta-analytic big bang. Research Synthesis Methods, 6(3), 246–264. https://doi.org/10.1002/jrsm.1132 Shadish, W. R., &amp; Rindskopf, D. M. (2007). Methods for evidence-based practice: Quantitative synthesis of single-subject designs. New Directions for Evaluation, 113(113), 95–109. https://doi.org/10.1002/ev.217 Stotz, K. E., Itoi, M., Konrad, M., &amp; Alber-Morgan, S. R. (2008). Effects of Self-graphing on Written Expression of Fourth Grade Students with High-Incidence Disabilities. Journal of Behavioral Education, 17(2), 172–186. https://doi.org/10.1007/s10864-007-9055-9 Swaminathan, H., Rogers, H. J., &amp; Horner, R. H. (2014). An effect size measure and bayesian analysis of single-case designs. Journal of School Psychology, 52(2), 213–230. https://doi.org/10.1016/j.jsp.2013.12.002 Tate, R. L., Perdices, M., Rosenkoetter, U., Togher, L., McDonald, S., Shadish, W., Horner, R., Kratochwill, T., Barlow, D. H., Kazdin, A., Sampson, M., Shamseer, L., &amp; Vohra, S. (2016). The Single-Case Reporting Guideline In BEhavioural Interventions (SCRIBE) 2016: Explanation and Elaboration. 22. Taylor, J. A., Pigott, T., &amp; Williams, R. (2022). Promoting Knowledge Accumulation About Intervention Effects: Exploring Strategies for Standardizing Statistical Approaches and Effect Size Reporting. Educational Researcher, 51(1), 72–80. https://doi.org/10.3102/0013189X211051319 Van den Noortgate, W., López-López, J. A., Marı́n-Martı́nez, F., &amp; Sánchez-Meca, J. (2013). Three-level meta-analysis of dependent effect sizes. Behavior Research Methods, 45, 576–594. https://doi.org/10.3758/s13428-012-0261-6 Van den Noortgate, W., &amp; Onghena, P. (2008). A multilevel meta-analysis of single-subject experimental design studies. Evidence-Based Communication Assessment and Intervention, 2(3), 142–151. https://doi.org/10.1080/17489530802505362 What Works Clearinghouse. (2020a). What Works Clearinghouse Procedures and Standards Handbook (Version 5.0). U.S. Department of Education, Institute of Education Sciences, National Center for Education Evaluation and Regional Assistance. What Works Clearinghouse. (2020b). What Works Clearinghouse Standards Handbook (Version 4.1). U.S. Department of Education, Institute of Education Sciences, National Center for Education Evaluation and Regional Assistance. White, O. R. (1987). Some comments concerning \"The quantitative synthesis of single-subject research\". Remedial and Special Education, 8(2), 34–39. https://doi.org/10.1177/074193258700800207 Wood, S. G., Moxley, J. H., Tighe, E. L., &amp; Wagner, R. K. (2018). Does Use of Text-to-Speech and Related Read-Aloud Tools Improve Reading Comprehension for Students With Reading Disabilities? A Meta-Analysis. Journal of Learning Disabilities, 51(1), 73–84. https://doi.org/10.1177/0022219416688170 Zimmerman, K. N., Ledford, J. R., Severini, K. E., Pustejovsky, J. E., Barton, E. E., &amp; Lloyd, B. P. (2018). Single-case synthesis tools I: Comparing tools to evaluate SCD quality and rigor. Research in Developmental Disabilities, 79, 19–32. https://doi.org/10.1016/j.ridd.2018.02.003 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
