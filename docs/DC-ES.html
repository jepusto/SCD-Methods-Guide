<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Introduction to Design-Comparable Effect Sizes | Effect Size Estimation and Synthesis of Single-Case Designs: A Methods Guide</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Introduction to Design-Comparable Effect Sizes | Effect Size Estimation and Synthesis of Single-Case Designs: A Methods Guide" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="github-repo" content="jepusto/SCD-Methods-Guide" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Introduction to Design-Comparable Effect Sizes | Effect Size Estimation and Synthesis of Single-Case Designs: A Methods Guide" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  



<meta name="date" content="2023-11-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intro.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Synthesis of Single-Case Designs</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Authors</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#disclaimer"><i class="fa fa-check"></i>Disclaimer</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#citation"><i class="fa fa-check"></i>Citation</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#preface"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#datasets"><i class="fa fa-check"></i>Datasets</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Approaches for Effect Size Estimation and Synthesis of Single-Case Designs</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#background"><i class="fa fa-check"></i><b>1.1</b> Background</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#purpose-of-the-methods-guide"><i class="fa fa-check"></i><b>1.2</b> Purpose of the Methods Guide</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#study-quality"><i class="fa fa-check"></i><b>1.3</b> Study Quality</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#selecting-an-approach-for-effect-estimation-and-synthesis"><i class="fa fa-check"></i><b>1.4</b> Selecting an Approach for Effect Estimation and Synthesis</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="intro.html"><a href="intro.html#design-comparable-effect-sizes"><i class="fa fa-check"></i><b>1.4.1</b> Design-Comparable Effect Sizes</a></li>
<li class="chapter" data-level="1.4.2" data-path="intro.html"><a href="intro.html#case-specific-effect-sizes"><i class="fa fa-check"></i><b>1.4.2</b> Case-Specific Effect Sizes</a></li>
<li class="chapter" data-level="1.4.3" data-path="intro.html"><a href="intro.html#multilevel-modeling-of-individual-participant-interrupted-time-series-data"><i class="fa fa-check"></i><b>1.4.3</b> Multilevel Modeling of Individual Participant Interrupted Time-Series Data</a></li>
<li class="chapter" data-level="1.4.4" data-path="intro.html"><a href="intro.html#summary-of-options-for-effect-estimation-and-synthesis"><i class="fa fa-check"></i><b>1.4.4</b> Summary of Options for Effect Estimation and Synthesis</a></li>
<li class="chapter" data-level="1.4.5" data-path="intro.html"><a href="intro.html#limitations-in-selecting-an-approach-for-effect-estimation-and-synthesis"><i class="fa fa-check"></i><b>1.4.5</b> Limitations in Selecting an Approach for Effect Estimation and Synthesis</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#structure-of-the-methods-guide"><i class="fa fa-check"></i><b>1.5</b> Structure of the Methods Guide</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="DC-ES.html"><a href="DC-ES.html"><i class="fa fa-check"></i><b>2</b> Introduction to Design-Comparable Effect Sizes</a>
<ul>
<li class="chapter" data-level="2.1" data-path="DC-ES.html"><a href="DC-ES.html#background-1"><i class="fa fa-check"></i><b>2.1</b> Background</a></li>
<li class="chapter" data-level="2.2" data-path="DC-ES.html"><a href="DC-ES.html#when-to-use-design-comparable-effect-sizes"><i class="fa fa-check"></i><b>2.2</b> When to Use Design-Comparable Effect Sizes</a></li>
<li class="chapter" data-level="2.3" data-path="DC-ES.html"><a href="DC-ES.html#a-general-definition-of-design-comparable-effect-sizes"><i class="fa fa-check"></i><b>2.3</b> A General Definition of Design-Comparable Effect Sizes</a></li>
<li class="chapter" data-level="2.4" data-path="DC-ES.html"><a href="DC-ES.html#what-we-assume-with-design-comparable-effect-size-estimation-and-synthesis"><i class="fa fa-check"></i><b>2.4</b> What We Assume with Design-Comparable Effect Size Estimation and Synthesis</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="DC-ES.html"><a href="DC-ES.html#exchangeable-effect-sizes"><i class="fa fa-check"></i><b>2.4.1</b> Exchangeable Effect Sizes</a></li>
<li class="chapter" data-level="2.4.2" data-path="DC-ES.html"><a href="DC-ES.html#es-estimation-homogeneity-of-variance"><i class="fa fa-check"></i><b>2.4.2</b> ES Estimation: Homogeneity of Variance</a></li>
<li class="chapter" data-level="2.4.3" data-path="DC-ES.html"><a href="DC-ES.html#es-estimation-normality"><i class="fa fa-check"></i><b>2.4.3</b> ES Estimation: Normality</a></li>
<li class="chapter" data-level="2.4.4" data-path="DC-ES.html"><a href="DC-ES.html#es-estimation-appropriate-structural-model"><i class="fa fa-check"></i><b>2.4.4</b> ES Estimation: Appropriate Structural Model</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="DC-ES.html"><a href="DC-ES.html#modeling-options-for-design-comparable-effect-size-estimation"><i class="fa fa-check"></i><b>2.5</b> Modeling Options for Design-comparable Effect Size Estimation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Effect Size Estimation and Synthesis of Single-Case Designs: A Methods Guide</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="DC-ES" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Introduction to Design-Comparable Effect Sizes<a href="DC-ES.html#DC-ES" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This chapter provides background on design-comparable effect sizes, describes when to use them, and explains the key assumptions behind their use.
In particular, we highlight both the assumptions for estimation of effect sizes and assumptions for using the effect sizes in meta-analytic synthesis.
We then describe options that are available for estimating design-comparable effect sizes.
These options allow for different assumptions regarding trends in baseline or treatment phases, as well as different assumptions about the variation in the treatment effect across cases.
We close by providing a set of decision rules for selecting among the options for design-comparable effect sizes.</p>
<div id="background-1" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Background<a href="DC-ES.html#background-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Design-comparable effect sizes are effect size (ES) indices for single case studies that are on the same metric as ES indices used in group comparison studies <span class="citation">(<a href="#ref-Hedges2012ABk">Hedges et al., 2012</a>, <a href="#ref-Hedges2012MB">2013</a>; <a href="#ref-Pustejovsky2014design">Pustejovsky et al., 2014</a>; <a href="#ref-Shadish2013d">Shadish et al., 2014</a>; <a href="#ref-Swaminathan2014effect">Swaminathan et al., 2014</a>; <a href="#ref-VandenNoortgate2008multilevel">Van den Noortgate &amp; Onghena, 2008</a>)</span>. These design-comparable ESs are valuable in a variety of synthesis contexts, particularly those that involve both single case designs (SCDs) and group comparison studies. However, these methods also have several limitations. First, because design-comparable ESs rely on an estimate of the variance between participants, they require at least three distinct participants for estimation, which limits their use to SCDs involving multiple participants (such as multiple baselines across participants) or close replications of a design, each of which has a single participant (such as an ABAB design replicated across several students). The methods are also only available for multiple baseline, multiple probe, or treatment reversal designs. Extensions for alternating treatment designs and other types of SCDs have yet to be developed.</p>
<p>A second, conceptual limitation, is that design-comparable ES produce a single, summary ES per outcome, which represents an average effect across participants—just as ES from between-group designs are summaries of average effects. As a result, the design-comparable ES might conceal heterogeneity of effects across the participants in an SCD. Thus, when using such ES in meta-analysis, moderator analyses are limited to study-level characteristics (i.e., no examination of potential moderators that vary across cases in a study or across time points within a case).</p>
<p>A third limitation is that design-comparable ES are based on a hierarchical model that involves specific assumptions about the distribution of outcomes measured in the study. Developing a reasonable model requires care and attention to the plausibility of its assumptions, and is not an automatic process. Moreover, for some types of outcomes, the distributional assumptions of the model may not be appropriate, which creates a further limitation on use of design-comparable ES.</p>
<p>In this chapter, we describe when researchers should select to use design-comparable ES estimates, give a precise definition of the design-comparable standardized mean difference ES, and explain the assumptions made when using this approach for effect estimation and synthesis. We then provide six of the most common modeling options and guidance to assist researchers with the most appropriate selection of an option.</p>
</div>
<div id="when-to-use-design-comparable-effect-sizes" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> When to Use Design-Comparable Effect Sizes<a href="DC-ES.html#when-to-use-design-comparable-effect-sizes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When choosing an effect size (ES) for single case design (SCD) data, the broader purpose for computing ESs needs to be considered first and foremost. In some cases, researchers may want to synthesize results across different types of studies. For example, they may want to estimate the size of the effect based on the data available from all studies in an area (e.g., all studies examining the effects of social skills intervention on the social and academic outcomes of elementary aged students with disabilities). In some areas of educational research, the literature identified for synthesis may include both group and single case experimental studies. To average the effect across studies with different designs, one must pick an ES index that has a comparable interpretation for each of the included designs <span class="citation">(<a href="#ref-Hedges2012ABk">Hedges et al., 2012</a>, <a href="#ref-Hedges2012MB">2013</a>; <a href="#ref-Pustejovsky2014design">Pustejovsky et al., 2014</a>; <a href="#ref-Shadish2013d">Shadish et al., 2014</a>; <a href="#ref-Swaminathan2014effect">Swaminathan et al., 2014</a>)</span>. Researchers developed the design-comparable ES for SCDs for exactly this purpose. It provides an ES on a common metric by answering the question “What would the standardized mean difference ES be if one could have somehow performed a between-group randomized experiment based on the same population of participants, same intervention protocol, and same outcome measures?” Design-comparable ESs can be computed for multiple-baseline designs across 3 or more participants, multiple-probe designs across 3 or more participants, and reversal (e.g. ABAB) designs replicated across 3 or more participants. Ongoing research is likely to increase the designs for which design-comparable ESs can be estimated.</p>
</div>
<div id="a-general-definition-of-design-comparable-effect-sizes" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> A General Definition of Design-Comparable Effect Sizes<a href="DC-ES.html#a-general-definition-of-design-comparable-effect-sizes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To understand the logic of the design comparable ES, it is helpful to first consider how effect sizes are defined in group design studies. In a between-groups randomized experiment comparing an intervention condition (B) to a control condition (A) for a specified population, results are commonly summarized with the standardized mean difference (SMD) ES index. The SMD effect size parameter can be defined as
<span class="math display" id="eq:SMD-group">\[\begin{equation}
\tag{2.1}
\delta = \frac{\mu_T - \mu_C}{\sigma_C},
\end{equation}\]</span>
where <span class="math inline">\(\mu_T\)</span> is the average outcome if the entire population were to receive the intervention, <span class="math inline">\(\mu_C\)</span> is the average outcome if the entire population were to receive the control condition, and <span class="math inline">\(\sigma_C\)</span> is the standard deviation of the outcome if the entire population were to receive the control condition. The effect size may be estimated by substituting sample means and sample standard deviations in place of the corresponding population quantities <span class="citation">(<a href="#ref-Borenstein2009effect">Borenstein, 2009</a>)</span>, or by pooling sample standard deviations across the intervention and control conditions under the assumption that the population variance is equal. Alternately, the mean difference in the numerator of the effect size may be estimated based on a statistical model, such as an analysis of covariance that adjusts for between-group differences on baseline characteristics <span class="citation">(<a href="#ref-taylor2022Promoting">Taylor et al., 2022</a>)</span>. Researchers often apply the Hedges <span class="math inline">\(g\)</span> small-sample correction, which reduces the bias of the effect size estimator that arises from estimating <span class="math inline">\(\sigma_C\)</span> based on a limited number of observations <span class="citation">(<a href="#ref-Hedges1981distribution">Hedges, 1981</a>)</span>.</p>
<p>The design-comparable SMD for SCDs aims to estimate the same quantity as the SMD from a between-groups experiment given in Equation <a href="DC-ES.html#eq:SMD-group">(2.1)</a>, using data from a multiple baseline, multiple probe, or replicated treatment reversal design. Doing so is made more challenging, however, because the data from such SCDs involves repeated measurements taken over time. In order to precisely define a design-comparable SMD, we must therefore be specific about the timing of intervention and outcome assessment. Hypothetically, if we could conduct a between-groups experiment using the same study procedures as the SCD, we would still need to decide when to begin intervention and when to collect outcome data. Suppose that the SCD takes place over times <span class="math inline">\(t=1,...,T\)</span>. In our hypothetical between-group experiment, we start intervention at time <span class="math inline">\(A\)</span>, for <span class="math inline">\(1 \leq A &lt; T\)</span> and we collect outcome data for all participants at time <span class="math inline">\(B\)</span>, for <span class="math inline">\(A &lt; B\)</span>. The SMD from such an experiment would then correspond to
<span class="math display" id="eq:SMD-SCD">\[\begin{equation}
\tag{2.2}
\delta_{AB} = \frac{\mu_B(A) - \mu_B(T)}{\sigma_B(T)},
\end{equation}\]</span>
where <span class="math inline">\(\mu_B(A)\)</span> is the average outcome at follow-up time <span class="math inline">\(B\)</span> if the entire population were to receive the intervention at time <span class="math inline">\(A\)</span>, <span class="math inline">\(\mu_B(T)\)</span> is the average outcome at follow-up time <span class="math inline">\(B\)</span> if the entire population were to receive the intervention at time <span class="math inline">\(T\)</span>, and <span class="math inline">\(\sigma_B(T)\)</span> is the standard deviation of the outcome at follow-up time <span class="math inline">\(B\)</span> if the entire population were to receive the intervention at time <span class="math inline">\(T\)</span>. Note that <span class="math inline">\(\mu_B(T)\)</span> corresponds to the average outcome under the control condition (<span class="math inline">\(\mu_C\)</span> above), because participants would not yet have received intervention as of time <span class="math inline">\(B\)</span>. Similarly <span class="math inline">\(\sigma_B(T)\)</span> is the analogue of <span class="math inline">\(\sigma_C\)</span>, the standard deviation of the outcome under the control condition, because participants would not yet have received the intervention as of time <span class="math inline">\(B\)</span>.</p>
<p><span class="citation">Pustejovsky et al. (<a href="#ref-Pustejovsky2014design">2014</a>)</span> described a strategy for estimating the design-comparable SMD ES <span class="math inline">\(\delta_AB\)</span>, using data from an SCD study. Broadly, the strategy involves specifying a multi-level model for the data, estimating the component quantities <span class="math inline">\(\mu_B(A)\)</span>, <span class="math inline">\(\mu_B(T)\)</span>, and <span class="math inline">\(\sigma_B(T)\)</span> based on the specified model, and applying a small-sample correction analogous to Hedges’ <span class="math inline">\(g\)</span>. This strategy will only work if the SCD study includes data from multiple participants because we need to be able to estimate <span class="math inline">\(\sigma_B(T)\)</span>, the standard deviation of the outcome across the population of participants. The strategy involves a multi-level model for the data because SCDs involve repeated measurements on a set of participants. Thus, the first level of the model describes the pattern of repeated measurements over time, nested within a given participant, and the second level of the model describes how the first-level parameters vary across participants. As a result, the model involves decomposing the standard deviation of the outcome <span class="math inline">\(\sigma_B(T)\)</span> into within-participant variation and between-participant variation. This decomposition is not typically possible in a between-groups randomized experiment (unless researchers collect repeated measures of the outcome for each participant).</p>
</div>
<div id="what-we-assume-with-design-comparable-effect-size-estimation-and-synthesis" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> What We Assume with Design-Comparable Effect Size Estimation and Synthesis<a href="DC-ES.html#what-we-assume-with-design-comparable-effect-size-estimation-and-synthesis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Design-comparable ESs require assumptions for estimation from SCD single case data, as well as further assumptions for inclusion in a synthesis. Regarding synthesis, we assume the effect sizes from all studies are exchangeable, meaning that they are similar (though not necessarily identical) and that their ranking cannot be systematically predicted. Regarding estimation, design-comparable ESs for SCD studies rely on multilevel models with normally distributed error terms. Thus, there are assumptions about the underlying structural model (e.g., whether or not there are trends in phases), as well as assumptions about the error terms (e.g., errors are normally distributed and homoscedastic). We now explain these assumptions in greater detail.</p>
<div id="exchangeable-effect-sizes" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Exchangeable Effect Sizes<a href="DC-ES.html#exchangeable-effect-sizes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The predominant statistical model for synthesizing effect sizes is known as the random effects model, the assumptions of which can be motivated in several different ways. One way is to imagine that the studies included in a synthesis comprise a random sample from a super-population of possible studies on the topic of interest. The model can also be motivated in a Bayesian framework by the assumption of exchangeability, meaning that the effect sizes of studies included in a synthesis are on a common metric that permits judgements of similarity and that their relative magnitude cannot be systematically predicted a priori <span class="citation">(<a href="#ref-higgins2009reevaluation">Higgins et al., 2009</a>)</span>. For brevity, we refer to both the super-population and Bayesian motivations as the exchangeability assumption. Crucially, the exchangeability assumption depends on the effect size metric used for synthesis. In other words, for a given set of studies, the assumption may be reasonable for one effect size metric but not reasonable for another.</p>
<p>Design-comparable ESs for SCDs have thus far been defined for the standardized mean difference metric, which describes the magnitude of an intervention effect in terms of a mean difference, standardized by the population standard deviation of the outcome in the absence of treatment. For the SMD metric, exchangeability is more plausible when the population of participants from one study is similar—or even interchangeable with—the population of participants from another study. If the populations are similar, and if two studies used the exact same operational measure of the dependent variable, then the distribution of outcomes in the control condition should have similar variance. In contrast, suppose that the two studies draw from populations with very different characteristics, so that the distribution of the dependent variable in one study population is much less dispersed than the distribution of the same variable in the other study population. In this scenario, an intervention that produces identical effects on the scale of the dependent variable would nonetheless have quite different effect sizes on the scale of the SMD, making the exchangeability assumption less tenable.</p>
<p>This aspect of the exchangeability assumption can be explored by examining the sampling methods and measurement procedures used in the studies to be synthesized. In particular, when subsets of studies use the same operational measure of the dependent variable, the between-participant variance in those studies can be compared. To illustrate the exploration of the variability between cases, consider the sampling procedures used in the following two studies which examine the effect of interventions on writing performance as measured by correct word sequences. In one study, the sample consisted of three 7-year-old white males identified by their teachers as struggling with writing <span class="citation">(<a href="#ref-parker2012Application">Parker et al., 2012</a>)</span>. In the other study <span class="citation">(<a href="#ref-stotz2008Effects">Stotz et al., 2008</a>)</span>, the sample consisted of three 10-year-old students who exhibited poor writing skills. The first student was an African American male identified with an emotional disturbance, the second student was a white male identified with a specific learning disability, and the third student was a while female identified with a specific learning disability. Presented with this information, we ask ourselves the following questions: Are these samples similar enough to satisfy the exchangeability assumption with the SMD metric? Or might the second sample, which included older students and variability across race, gender, and disability, be so much more variable in writing performance that it is not reasonable to use the SMD metric to judge similarity of effects?</p>
<p>In the first study <span class="citation">(<a href="#ref-parker2012Application">Parker et al., 2012</a>)</span>, the mean number of correct word sequences during baseline for the three participants were 8.29, 15.0, and 10.8. In the second study <span class="citation">(<a href="#ref-stotz2008Effects">Stotz et al., 2008</a>)</span>, the mean baseline levels were 14.6, 29.1, and 22.1. In the first study, the variation between cases, as indexed by the standard deviation (SD) of the three baseline means, is 3.4, whereas in the second study this between case SD is 7.3. Is this difference large enough to distort the design-comparable ES?</p>
<p>To address our questions, we first consider the raw score ES for each study. In Study 1, the shift in the expected number of correct word sequences when moving from baseline to intervention is 10.7 (based on a multilevel model that assumes no trends in baseline or treatment phase, and variation in the effect across cases). However, in Study 2 the raw score ES is 9.6 correct word sequences. Thus, the raw score ES from the first study is about 1.1 times larger than the ES we find in the second study. Next, we consider the design-comparable ES computed based on a model that assumes no trends in either baseline or treatment phases and demonstrates variability in the treatment effect across cases just like the model for the raw score ES. For the first study, the design-comparable ES is 0.962, and in the second study it is 0.827; thus the first study’s ES is about 1.2 times the second’s, similar to what was seen with the raw score ESs.</p>
<p>Hopefully, future research will help to establish how much difference in sampling (and thus variability between cases) that researchers can accommodate without creating notable problems for the synthesis of design-comparable ESs. In addition, we hope future research will provide options and guidance for how to handle consequential levels of heterogeneity. Perhaps, methods will be developed where the SD can be estimated using all cases across studies that were measured on a particular outcome variable, as opposed to estimating the variance within each primary study. Until additional research is conducted, we suggest that meta-analysts be aware that they are assuming that the effect sizes expressed on a given metric are exchangeable across studies, examine the studies with this in mind, and report findings with transparency about what is found.</p>
</div>
<div id="es-estimation-homogeneity-of-variance" class="section level3 hasAnchor" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> ES Estimation: Homogeneity of Variance<a href="DC-ES.html#es-estimation-homogeneity-of-variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The first assumption that comes to mind when thinking about synthesizing ESs is that the effect sizes are exchangeable in the SMD metric. However, that is not the only assumption required for conducting a synthesis of SCDs using design-comparable effect sizes. The estimation of design-comparable ESs assumes that the variation within a case is comparable across phases and comparable from case to case within a study. While often this assumption is reasonable, there are situations when it is not. For example, <span class="citation">Ota &amp; DuPaul (<a href="#ref-ota2002Task">2002</a>)</span> utilized a multiple baseline (MB) design across three participants to examine the effects of math software with a game component on the off-task behaviors of students with ADHD. For the purpose of this methods guide, we extracted the data using Webplot Digitizer <span class="citation">(<a href="#ref-rohatgi2015Webplotdigitizer">Rohatgi, 2015</a>)</span> and present it in Figure <a href="DC-ES.html#fig:Ota-DuPaul-2002">2.1</a>. Results of our visual analysis suggest that the variance differs between the baseline and treatment phases, with treatment phases having less variation as the percentage of off-task behaviors decreased and approached zero. With count-based variables (e.g., raw counts or counts converted to percentages), variability often depends on the mean of the variable. Treatments that shift the mean tend to change the variance. In addition, we have noticed that substantial variability is common to unstructured baselines used in SCD studies. If the intervention phase provides more control (e.g., controlling for interventionist/peer attention, rate of reinforcement), we might expect some reduction in variance. Many single-case intervention studies are prone to this shift in variance across experimental phases. Until future research provides more concrete guidance about the best ways to proceed when uncovering heterogeneity, we recommend that meta-analysts be aware of what they are assuming and transparent about what is found.</p>
<div class="figure"><span style="display:block;" id="fig:Ota-DuPaul-2002"></span>
<img src="images/Ota-DuPaul-2002.png" alt="Multiple baseline design across three participants (Ota \&amp; DuPaul, 2002)" width="75%" />
<p class="caption">
Figure 2.1: Multiple baseline design across three participants (Ota &amp; DuPaul, 2002)
</p>
</div>
<p>In some circumstances, the outcomes chosen for SCD studies may be so unlike the outcomes used in group design research that synthesis across various study designs will not be feasible. In such case, we tend to see extreme violations of the homogeneity assumptions. For example, when all cases in a multiple-baseline study have baselines with consistent values of zero, we advise against trying to force the computation of a design-comparable ES. Only when the assumptions seem more reasonable and violations are more modest (or non-existent), should researchers entertain the use of design-comparable ES.</p>
</div>
<div id="es-estimation-normality" class="section level3 hasAnchor" number="2.4.3">
<h3><span class="header-section-number">2.4.3</span> ES Estimation: Normality<a href="DC-ES.html#es-estimation-normality" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Use of design-comparable ESs is also based on the assumption that the experimental observations are normally distributed around the case-specific trend lines. In some situations, the data may be consistent with this assumption of normality, whereas in other it will not. Examining the baseline phases for the math software intervention study by <span class="citation">Ota &amp; DuPaul (<a href="#ref-ota2002Task">2002</a>)</span> in Figure <a href="DC-ES.html#fig:Ota-DuPaul-2002">2.1</a>, observations appear distributed somewhat normally around the baseline means, with a pooled skewness value near zero (<span class="math inline">\(sk = 0.15\)</span>) and a pooled kurtosis value near zero (<span class="math inline">\(ku = -0.77\)</span>). However, when we shift attention to the treatment phases, we see non-normality because the outcome is a percentage that has remained near the floor of 0% for much of the treatment phase (<span class="math inline">\(sk = 1.86\)</span>; <span class="math inline">\(ku = 4.99\)</span>).</p>
<p>With count-based variables, we often anticipate some departures from normality in addition to some differences in variance. These departures from what is assumed tend to be more pronounced when a count variable has a phase mean that is close to zero, or when a percentage variable has a phase mean close to 0% or close to 100%. However, when the counts are higher, or the percentage variable has a mean closer to 50%, the distributions tend to be more normal. We expect design-comparable ESs to tolerate some non-normality, but we need additional research to determine how much non-normality can be present before there are substantial consequences for the design-comparable ES.</p>
<p>We recommend that researchers consider normality, along with homogeneity, in their preliminary assessments of the primary studies to be included in a meta-analysis. As noted previously, there are some situations where the outcomes chosen for the single case studies may be so unlike the outcomes chosen for the group designs that synthesis across single case and group designs will not be feasible (e.g., SCDs where all cases in a multiple-baseline study have baselines with consistent values of zero). In such circumstances, we advise against trying to force the computation of a design-comparable ES. In other circumstances, design-comparable ESs can be estimated when the assumptions seem more reasonable and violations are more moderate or non-existent. Regardless of the findings, it is important to be transparent about the consistency of the data with the homogeneity and normality assumptions.</p>
</div>
<div id="es-estimation-appropriate-structural-model" class="section level3 hasAnchor" number="2.4.4">
<h3><span class="header-section-number">2.4.4</span> ES Estimation: Appropriate Structural Model<a href="DC-ES.html#es-estimation-appropriate-structural-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Finally, estimation of design-comparable ES requires making assumptions about the structural model for the data series collected in the SCD. These assumptions are ideally based on content expert knowledge of the intervention domain and dependent variables under review, as well as on visualization and calculation of descriptive statistics from the studies. We may assume that there is no trend in baseline, or there is a linear trend in baseline, or there is some form of nonlinear trend. We may assume that there is no trend in the treatment phase, or there is a linear or nonlinear trend in the treatment phase. Furthermore, we may assume that the parameters defining the baseline trajectory (e.g., level and slope) and the parameters defining the change in trajectory with treatment (e.g., the change in level and change in slope) are the same for all cases within a study, or that some of them are different for different cases. If we select a structural model that is inconsistent with our data, we expect biased design-comparable ES estimates.</p>
<div class="figure"><span style="display:block;" id="fig:Rodgers-2020"></span>
<img src="images/Rodgers-2020.png" alt="Multiple Baseline Across Participants (Rodgers et al., 2020)" width="75%" />
<p class="caption">
Figure 2.2: Multiple Baseline Across Participants (Rodgers et al., 2020)
</p>
</div>
<p>As an example, we present the study of a writing intervention for post-secondary adults with intellectual and developmental disabilities that targeted the improvement of sentence construction <span class="citation">(<a href="#ref-rodgers2021Effects">Rodgers et al., 2021</a>)</span>. Figure <a href="DC-ES.html#fig:Rodgers-2020">2.2</a> is a graphical depiction of the outcome data and study design. Visual analysis of the baseline phase suggests potential baseline trends in accurate writing sequences. In our consideration of Denny, our visual analysis of correct writing sequences in baseline suggests the potential of an increasing trend in baseline, represented by the solid line in Figure <a href="DC-ES.html#fig:Rodgers-2020-Denny">2.3</a>, which was estimated using ordinary least squares regression. In contrast, if we select a baseline model with no trends, the baseline projection is considerably different than the projected baseline when a linear trend is assumed (see the difference in the dotted lines through the treatment phase in Figure <a href="DC-ES.html#fig:Rodgers-2020-Denny">2.3</a>). The effect estimate would be larger if we did not model the baseline trend in Figure <a href="DC-ES.html#fig:Rodgers-2020-Denny">2.3</a>, because the observed treatment values are further above the projection based on no trend than the projection based on trend. Thus, whether or not we assume trends will have consequences for the ES estimates.</p>
<div class="figure"><span style="display:block;" id="fig:Rodgers-2020-Denny"></span>
<img src="images/Rodgers-2020-Denny.jpg" alt="Hypothesized and Projected Baselines for Denny (Rodgers et al., 2020)" width="75%" />
<p class="caption">
Figure 2.3: Hypothesized and Projected Baselines for Denny (Rodgers et al., 2020)
</p>
</div>
<p>In thinking about an appropriate structural model, we recommend that meta-analysts start by carefully considering the area of research synthesis, including the outcome of interest, the participants in the studies, and what is expected in this area regarding baseline trends and treatment phase trends. In addition, we recommend that meta-analysts visually analyze the data from all primary studies to be included in the synthesis. In this visual analysis, the meta-analysts should assess the degree to which the data from the studies are reasonably consistent with expectations. If the data from the studies are consistent with the expectations about trends, these expectations can then be used to select among modeling options for design-comparable ESs. The next section details some of the commonly available modeling options for design-comparable ESs.</p>
</div>
</div>
<div id="modeling-options-for-design-comparable-effect-size-estimation" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Modeling Options for Design-comparable Effect Size Estimation<a href="DC-ES.html#modeling-options-for-design-comparable-effect-size-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We suggest that meta-analysts first consider the tenability of the homogeneity and normality assumptions. If violations are severe, researchers should reconsider if the outcomes from the single case studies are similar enough to the outcomes of the group studies to warrant synthesis using the design-comparable ES metric. If outcomes are substantially different across design types, it may be more reasonable to meta-analyze the SCD studies separately from the group studies and to use a different ES metric for the SCD studies. When violations are not too severe and outcomes appear more similar, then we suggest proceeding with the design-comparable ESs, while also noting what was found. If the decision is to proceed with design-comparable ESs, we suggest researchers next determine the predominant design used in the area of synthesis. Do most SCD studies in the research area tend to use reversal designs, such as ABAB designs, or do studies predominantly use MB across participants designs and/or multiple-probe (MP) across participant designs? When synthesizing reversal designs with design-comparable ESs, researchers are currently limited to models that assume there are no trends. For MB or MP designs, a variety of trend assumptions are feasible.</p>
<p>When the synthesis will include predominantly MB and/or MP designs, we suggest researchers clarify their expectations about trends given their understanding of the participants, context, and outcome being studied. We also recommend that the graphs of the data from the primary studies be analyzed visually for consistency with the trend expectations. Based on these considerations, we think it is helpful to determine which of the following sets of trend assumptions is most reasonable for the set of studies to be synthesized: a) no trends in baseline or treatment phases, b) no trends in baseline, but trends in treatment phases, or c) trends in baseline and different trends in treatment. After clarifying the trend assumptions, researchers next need to clarify assumptions about whether the treatment effect varies across the cases (Is the treatment effect expected to be the same for each case? Or is it expected that there will be differences in the response to intervention?) Again, we rely heavily on the expectations in the area of research synthesis, and visual analyses of the primary studies to determine if the data from those studies are reasonably consistent with the expectations. Figure <a href="DC-ES.html#fig:DC-ES-flow-chart">2.4</a> arranges these considerations into a series of decision rules that can be used select one of six common models for design-comparable ESs. Although there are other possibilities for specifying a model for design-comparable ESs, these six models cover a wide range of scenarios, can be estimated with the scdhlm software application <span class="citation">(<a href="#ref-pustejovsky2021scdhlm">Pustejovsky et al., 2021</a>)</span>, and include the models that have received the most attention in the methodological literature, as well as those that have been applied in meta-analyses of single case data.</p>
<div class="figure"><span style="display:block;" id="fig:DC-ES-flow-chart"></span>
<img src="images/DC-ES-flow-chart.png" alt="Flow Chart for the Selection of Design-comparable Effect Sizes" width="100%" />
<p class="caption">
Figure 2.4: Flow Chart for the Selection of Design-comparable Effect Sizes
</p>
</div>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-Borenstein2009effect" class="csl-entry">
Borenstein, M. (2009). Effect sizes for continuous data. In H. M. Cooper, L. V. Hedges, &amp; J. C. Valentine (Eds.), <em>The <span>Handbook</span> of <span>Research Synthesis</span> and <span>Meta-Analysis</span></em> (pp. 221–236). <span>Russell Sage Foundation</span>.
</div>
<div id="ref-Hedges1981distribution" class="csl-entry">
Hedges, L. V. (1981). Distribution theory for <span>Glass</span>’s estimator of effect size and related estimators. <em>Journal of Educational Statistics</em>, <em>6</em>(2), 107–128.
</div>
<div id="ref-Hedges2012ABk" class="csl-entry">
Hedges, L. V., Pustejovsky, J. E., &amp; Shadish, W. R. (2012). A standardized mean difference effect size for single case designs. <em>Research Synthesis Methods</em>, <em>3</em>, 224–239. <a href="https://doi.org/10.1002/jrsm.1052">https://doi.org/10.1002/jrsm.1052</a>
</div>
<div id="ref-Hedges2012MB" class="csl-entry">
Hedges, L. V., Pustejovsky, J. E., &amp; Shadish, W. R. (2013). A standardized mean difference effect size for multiple baseline designs across individuals. <em>Research Synthesis Methods</em>. <a href="https://doi.org/10.1002/jrsm.1086">https://doi.org/10.1002/jrsm.1086</a>
</div>
<div id="ref-higgins2009reevaluation" class="csl-entry">
Higgins, J. P. T., Thompson, S. G., &amp; Spiegelhalter, D. J. (2009). A re-evaluation of random-effects meta-analysis. <em>Journal of the Royal Statistical Society: Series A (Statistics in Society)</em>, <em>172</em>(1), 137–159. <a href="https://doi.org/10.1111/j.1467-985X.2008.00552.x">https://doi.org/10.1111/j.1467-985X.2008.00552.x</a>
</div>
<div id="ref-ota2002Task" class="csl-entry">
Ota, K. R., &amp; DuPaul, G. J. (2002). Task engagement and mathematics performance in children with attention-deficit hyperactivity disorder: <span>Effects</span> of supplemental computer instruction. <em>School Psychology Quarterly</em>, <em>17</em>(3), 242–257. <a href="https://doi.org/10.1521/scpq.17.3.242.20881">https://doi.org/10.1521/scpq.17.3.242.20881</a>
</div>
<div id="ref-parker2012Application" class="csl-entry">
Parker, D. C., Dickey, B. N., Burns, M. K., &amp; McMaster, K. L. (2012). An <span>Application</span> of <span>Brief Experimental Analysis</span> with <span>Early Writing</span>. <em>Journal of Behavioral Education</em>, <em>21</em>(4), 329–349. <a href="https://doi.org/10.1007/s10864-012-9151-3">https://doi.org/10.1007/s10864-012-9151-3</a>
</div>
<div id="ref-pustejovsky2021scdhlm" class="csl-entry">
Pustejovsky, J. E., Chen, M., &amp; Hamilton, B. (2021). <em>Scdhlm: <span>A</span> web-based calculator for between-case standardized mean differences</em>.
</div>
<div id="ref-Pustejovsky2014design" class="csl-entry">
Pustejovsky, J. E., Hedges, L. V., &amp; Shadish, W. R. (2014). Design-comparable effect sizes in multiple baseline designs: <span>A</span> general modeling framework. <em>Journal of Educational and Behavioral Statistics</em>, <em>39</em>(5), 368–393. <a href="https://doi.org/10.3102/1076998614547577">https://doi.org/10.3102/1076998614547577</a>
</div>
<div id="ref-rodgers2021Effects" class="csl-entry">
Rodgers, D. B., Datchuk, S. M., &amp; Rila, A. L. (2021). Effects of a <span>Text-Writing Fluency Intervention</span> for <span>Postsecondary Students</span> with <span>Intellectual</span> and <span>Developmental Disabilities</span>. <em>Exceptionality</em>, <em>29</em>(4), 310–325. <a href="https://doi.org/10.1080/09362835.2020.1850451">https://doi.org/10.1080/09362835.2020.1850451</a>
</div>
<div id="ref-rohatgi2015Webplotdigitizer" class="csl-entry">
Rohatgi, A. (2015). <em>Webplotdigitizer</em>. Zenodo. <a href="https://doi.org/10.5281/zenodo.32375">https://doi.org/10.5281/zenodo.32375</a>
</div>
<div id="ref-Shadish2013d" class="csl-entry">
Shadish, W. R., Hedges, L. V., &amp; Pustejovsky, J. E. (2014). Analysis and meta-analysis of single-case designs with a standardized mean difference statistic: <span>A</span> primer and applications. <em>Journal of School Psychology</em>, <em>52</em>(2), 123–147. <a href="https://doi.org/10.1016/j.jsp.2013.11.005">https://doi.org/10.1016/j.jsp.2013.11.005</a>
</div>
<div id="ref-stotz2008Effects" class="csl-entry">
Stotz, K. E., Itoi, M., Konrad, M., &amp; Alber-Morgan, S. R. (2008). Effects of <span class="nocase">Self-graphing</span> on <span>Written Expression</span> of <span>Fourth Grade Students</span> with <span>High-Incidence Disabilities</span>. <em>Journal of Behavioral Education</em>, <em>17</em>(2), 172–186. <a href="https://doi.org/10.1007/s10864-007-9055-9">https://doi.org/10.1007/s10864-007-9055-9</a>
</div>
<div id="ref-Swaminathan2014effect" class="csl-entry">
Swaminathan, H., Rogers, H. J., &amp; Horner, R. H. (2014). An effect size measure and <span>Bayesian</span> analysis of single-case designs. <em>Journal of School Psychology</em>. <a href="https://doi.org/10.1016/j.jsp.2013.12.002">https://doi.org/10.1016/j.jsp.2013.12.002</a>
</div>
<div id="ref-taylor2022Promoting" class="csl-entry">
Taylor, J. A., Pigott, T., &amp; Williams, R. (2022). Promoting <span>Knowledge Accumulation About Intervention Effects</span>: <span>Exploring Strategies</span> for <span>Standardizing Statistical Approaches</span> and <span>Effect Size Reporting</span>. <em>Educational Researcher</em>, <em>51</em>(1), 72–80. <a href="https://doi.org/10.3102/0013189X211051319">https://doi.org/10.3102/0013189X211051319</a>
</div>
<div id="ref-VandenNoortgate2008multilevel" class="csl-entry">
Van den Noortgate, W., &amp; Onghena, P. (2008). A multilevel meta-analysis of single-subject experimental design studies. <em>Evidence-Based Communication Assessment and Intervention</em>, <em>2</em>(3), 142–151. <a href="https://doi.org/10.1080/17489530802505362">https://doi.org/10.1080/17489530802505362</a>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["SCD-Methods-Guide.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
